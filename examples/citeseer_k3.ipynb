{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycq/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "weight = 20\n",
    "\"\"\"\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer113\"] = {\"in_channels\":2048, \"out_channels\":2048, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer13\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":2, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\"\"\"\n",
    "\n",
    "partitions = 3\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 1019, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers2\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers51\"] = {\"in_channels\": 64, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 64, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers54\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers55\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer21\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer22\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":256, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767, 1019)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/citeseer\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=1019, out_features=512, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (5): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (6): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=64, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.05, inplace=False)\n",
       "  (7): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (8): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -3.123270797729492\n",
      "                , loss1: -73.7976806640625\n",
      "                , loss2: 0.005750814080238342\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: -31.256430053710936\n",
      "                , loss1: -737.826220703125\n",
      "                , loss2: 0.02740100622177124\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 20 epoch, average loss: -31.24250183105469\n",
      "                , loss1: -737.836328125\n",
      "                , loss2: 0.04175727367401123\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 30 epoch, average loss: -31.26105651855469\n",
      "                , loss1: -737.8595703125\n",
      "                , loss2: 0.024184638261795045\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 40 epoch, average loss: -31.2498046875\n",
      "                , loss1: -737.922412109375\n",
      "                , loss2: 0.038107001781463624\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 50 epoch, average loss: -31.269009399414063\n",
      "                , loss1: -737.83916015625\n",
      "                , loss2: 0.01537279486656189\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 60 epoch, average loss: -31.260333251953124\n",
      "                , loss1: -737.909130859375\n",
      "                , loss2: 0.027010011672973632\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 70 epoch, average loss: -31.262176513671875\n",
      "                , loss1: -737.9060546875\n",
      "                , loss2: 0.02504148185253143\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 80 epoch, average loss: -31.259542846679686\n",
      "                , loss1: -737.84755859375\n",
      "                , loss2: 0.025192031264305116\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 90 epoch, average loss: -31.2676513671875\n",
      "                , loss1: -737.952294921875\n",
      "                , loss2: 0.021521595120429993\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 100 epoch, average loss: -31.26942138671875\n",
      "                , loss1: -737.937109375\n",
      "                , loss2: 0.01911267638206482\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 110 epoch, average loss: -31.264315795898437\n",
      "                , loss1: -737.94365234375\n",
      "                , loss2: 0.024497348070144653\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 120 epoch, average loss: -31.274267578125\n",
      "                , loss1: -737.9052734375\n",
      "                , loss2: 0.01291554719209671\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 130 epoch, average loss: -31.248593139648438\n",
      "                , loss1: -737.917578125\n",
      "                , loss2: 0.0391092985868454\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 140 epoch, average loss: -31.261773681640626\n",
      "                , loss1: -738.023583984375\n",
      "                , loss2: 0.030427855253219605\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 150 epoch, average loss: -31.268792724609376\n",
      "                , loss1: -737.93017578125\n",
      "                , loss2: 0.019449979066848755\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 160 epoch, average loss: -31.265692138671874\n",
      "                , loss1: -737.852392578125\n",
      "                , loss2: 0.019249105453491212\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 170 epoch, average loss: -31.254244995117187\n",
      "                , loss1: -737.9939453125\n",
      "                , loss2: 0.03670002222061157\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 180 epoch, average loss: -31.25211181640625\n",
      "                , loss1: -737.925439453125\n",
      "                , loss2: 0.03592357635498047\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 190 epoch, average loss: -31.265350341796875\n",
      "                , loss1: -737.88798828125\n",
      "                , loss2: 0.021099878847599028\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 200 epoch, average loss: -31.2707275390625\n",
      "                , loss1: -737.955908203125\n",
      "                , loss2: 0.018605859577655794\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 210 epoch, average loss: -31.266567993164063\n",
      "                , loss1: -737.916748046875\n",
      "                , loss2: 0.021098464727401733\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 220 epoch, average loss: -31.272415161132812\n",
      "                , loss1: -737.91689453125\n",
      "                , loss2: 0.015259484946727752\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 230 epoch, average loss: -31.25360107421875\n",
      "                , loss1: -737.9615234375\n",
      "                , loss2: 0.03596206903457642\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 240 epoch, average loss: -31.269537353515624\n",
      "                , loss1: -737.900439453125\n",
      "                , loss2: 0.017438077926635744\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 250 epoch, average loss: -31.269732666015624\n",
      "                , loss1: -737.8953125\n",
      "                , loss2: 0.01702888011932373\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 260 epoch, average loss: -31.266693115234375\n",
      "                , loss1: -737.93759765625\n",
      "                , loss2: 0.021860745549201966\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 270 epoch, average loss: -31.268154907226563\n",
      "                , loss1: -737.964453125\n",
      "                , loss2: 0.02153526246547699\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 280 epoch, average loss: -31.26909484863281\n",
      "                , loss1: -737.91640625\n",
      "                , loss2: 0.018560813367366792\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 290 epoch, average loss: -31.250311279296874\n",
      "                , loss1: -738.0474609375\n",
      "                , loss2: 0.042899271845817565\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 300 epoch, average loss: -31.2674072265625\n",
      "                , loss1: -737.953515625\n",
      "                , loss2: 0.021820394694805144\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 310 epoch, average loss: -31.27637634277344\n",
      "                , loss1: -737.918017578125\n",
      "                , loss2: 0.011347521841526032\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 320 epoch, average loss: -31.265130615234376\n",
      "                , loss1: -737.951416015625\n",
      "                , loss2: 0.024008248746395112\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 330 epoch, average loss: -31.275091552734374\n",
      "                , loss1: -737.974462890625\n",
      "                , loss2: 0.015025593340396881\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 340 epoch, average loss: -31.272665405273436\n",
      "                , loss1: -737.95703125\n",
      "                , loss2: 0.016711191833019258\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 350 epoch, average loss: -31.26148681640625\n",
      "                , loss1: -737.922509765625\n",
      "                , loss2: 0.02642672657966614\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 360 epoch, average loss: -31.241799926757814\n",
      "                , loss1: -737.975390625\n",
      "                , loss2: 0.04835625290870667\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 370 epoch, average loss: -31.259298706054686\n",
      "                , loss1: -738.044091796875\n",
      "                , loss2: 0.03376961350440979\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 380 epoch, average loss: -31.267462158203124\n",
      "                , loss1: -737.917529296875\n",
      "                , loss2: 0.020241133868694305\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 390 epoch, average loss: -31.258847045898438\n",
      "                , loss1: -737.941259765625\n",
      "                , loss2: 0.029862868785858154\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 400 epoch, average loss: -31.246917724609375\n",
      "                , loss1: -738.092041015625\n",
      "                , loss2: 0.04818556606769562\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 410 epoch, average loss: -31.2556396484375\n",
      "                , loss1: -738.035546875\n",
      "                , loss2: 0.03706863522529602\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 420 epoch, average loss: -31.2310302734375\n",
      "                , loss1: -737.939453125\n",
      "                , loss2: 0.05760048627853394\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 430 epoch, average loss: -31.270101928710936\n",
      "                , loss1: -737.957666015625\n",
      "                , loss2: 0.01929662376642227\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 440 epoch, average loss: -31.259664916992186\n",
      "                , loss1: -737.8931640625\n",
      "                , loss2: 0.027005308866500856\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 450 epoch, average loss: -31.265167236328125\n",
      "                , loss1: -737.845166015625\n",
      "                , loss2: 0.019465559720993043\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 460 epoch, average loss: -31.274264526367187\n",
      "                , loss1: -737.83603515625\n",
      "                , loss2: 0.00998367667198181\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 470 epoch, average loss: -31.267156982421874\n",
      "                , loss1: -737.959912109375\n",
      "                , loss2: 0.02234109044075012\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 480 epoch, average loss: -31.264193725585937\n",
      "                , loss1: -737.92939453125\n",
      "                , loss2: 0.02400987297296524\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 490 epoch, average loss: -31.266207885742187\n",
      "                , loss1: -738.001904296875\n",
      "                , loss2: 0.025067454576492308\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 500 epoch, average loss: -31.263360595703126\n",
      "                , loss1: -737.945361328125\n",
      "                , loss2: 0.025523114204406738\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 510 epoch, average loss: -31.258203125\n",
      "                , loss1: -737.956982421875\n",
      "                , loss2: 0.031172949075698852\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 520 epoch, average loss: -31.27729187011719\n",
      "                , loss1: -737.933642578125\n",
      "                , loss2: 0.011097650974988937\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 530 epoch, average loss: -31.276141357421874\n",
      "                , loss1: -737.940185546875\n",
      "                , loss2: 0.012519076466560364\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 540 epoch, average loss: -31.266342163085938\n",
      "                , loss1: -737.88994140625\n",
      "                , loss2: 0.020189327001571656\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 550 epoch, average loss: -31.27337646484375\n",
      "                , loss1: -737.966943359375\n",
      "                , loss2: 0.016422148048877715\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 560 epoch, average loss: -31.273947143554686\n",
      "                , loss1: -737.98837890625\n",
      "                , loss2: 0.01675880402326584\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 570 epoch, average loss: -31.27204284667969\n",
      "                , loss1: -737.933935546875\n",
      "                , loss2: 0.01635441929101944\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 580 epoch, average loss: -31.26912841796875\n",
      "                , loss1: -737.952685546875\n",
      "                , loss2: 0.020062460005283354\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 590 epoch, average loss: -31.269403076171876\n",
      "                , loss1: -737.945654296875\n",
      "                , loss2: 0.019492322206497194\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 600 epoch, average loss: -31.269876098632814\n",
      "                , loss1: -738.026953125\n",
      "                , loss2: 0.022463957965373992\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 610 epoch, average loss: -31.268255615234374\n",
      "                , loss1: -737.952783203125\n",
      "                , loss2: 0.020944130420684815\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 620 epoch, average loss: -31.241116333007813\n",
      "                , loss1: -738.029296875\n",
      "                , loss2: 0.05132206678390503\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 630 epoch, average loss: -31.264675903320313\n",
      "                , loss1: -738.0251953125\n",
      "                , loss2: 0.02758834362030029\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 640 epoch, average loss: -31.26317138671875\n",
      "                , loss1: -737.9880859375\n",
      "                , loss2: 0.027520224452018738\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 650 epoch, average loss: -31.27083740234375\n",
      "                , loss1: -737.94521484375\n",
      "                , loss2: 0.01803997904062271\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 660 epoch, average loss: -31.265948486328124\n",
      "                , loss1: -737.94482421875\n",
      "                , loss2: 0.02291777729988098\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 670 epoch, average loss: -31.25578918457031\n",
      "                , loss1: -737.91484375\n",
      "                , loss2: 0.03180028796195984\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 680 epoch, average loss: -31.26648864746094\n",
      "                , loss1: -738.00224609375\n",
      "                , loss2: 0.024807758629322052\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 690 epoch, average loss: -31.258895874023438\n",
      "                , loss1: -738.020703125\n",
      "                , loss2: 0.033177778124809265\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 700 epoch, average loss: -31.270806884765626\n",
      "                , loss1: -737.917236328125\n",
      "                , loss2: 0.016885480284690856\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 710 epoch, average loss: -31.256298828125\n",
      "                , loss1: -737.9712890625\n",
      "                , loss2: 0.03368265926837921\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 720 epoch, average loss: -31.268170166015626\n",
      "                , loss1: -738.052783203125\n",
      "                , loss2: 0.025269532203674318\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 730 epoch, average loss: -31.26805725097656\n",
      "                , loss1: -737.982421875\n",
      "                , loss2: 0.022394254803657532\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 740 epoch, average loss: -31.255047607421876\n",
      "                , loss1: -737.87236328125\n",
      "                , loss2: 0.0307387113571167\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 750 epoch, average loss: -31.25704345703125\n",
      "                , loss1: -737.931201171875\n",
      "                , loss2: 0.03123776614665985\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 760 epoch, average loss: -31.25638427734375\n",
      "                , loss1: -738.012890625\n",
      "                , loss2: 0.03536205291748047\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 770 epoch, average loss: -31.265228271484375\n",
      "                , loss1: -738.03935546875\n",
      "                , loss2: 0.02763805687427521\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 780 epoch, average loss: -31.276153564453125\n",
      "                , loss1: -737.906640625\n",
      "                , loss2: 0.011085206270217895\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 790 epoch, average loss: -31.254135131835938\n",
      "                , loss1: -737.879638671875\n",
      "                , loss2: 0.03196071982383728\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 800 epoch, average loss: -31.266656494140626\n",
      "                , loss1: -738.025341796875\n",
      "                , loss2: 0.02561882734298706\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 810 epoch, average loss: -31.277542114257812\n",
      "                , loss1: -737.952197265625\n",
      "                , loss2: 0.011629854887723922\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 820 epoch, average loss: -31.27166748046875\n",
      "                , loss1: -737.932421875\n",
      "                , loss2: 0.016667725145816804\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 830 epoch, average loss: -31.269027709960938\n",
      "                , loss1: -737.9724609375\n",
      "                , loss2: 0.02100543528795242\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 840 epoch, average loss: -31.2648681640625\n",
      "                , loss1: -738.03720703125\n",
      "                , loss2: 0.027906712889671326\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 850 epoch, average loss: -31.266940307617187\n",
      "                , loss1: -738.00576171875\n",
      "                , loss2: 0.024506841599941254\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 860 epoch, average loss: -31.255487060546876\n",
      "                , loss1: -737.9201171875\n",
      "                , loss2: 0.032321178913116456\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 870 epoch, average loss: -31.252215576171874\n",
      "                , loss1: -738.028076171875\n",
      "                , loss2: 0.04017448723316193\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 880 epoch, average loss: -31.266738891601562\n",
      "                , loss1: -737.985791015625\n",
      "                , loss2: 0.023860262334346773\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 890 epoch, average loss: -31.277276611328126\n",
      "                , loss1: -737.90234375\n",
      "                , loss2: 0.00978822261095047\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 900 epoch, average loss: -31.263522338867187\n",
      "                , loss1: -737.958447265625\n",
      "                , loss2: 0.025915852189064024\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 910 epoch, average loss: -31.265716552734375\n",
      "                , loss1: -738.03671875\n",
      "                , loss2: 0.027041444182395936\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 920 epoch, average loss: -31.263442993164062\n",
      "                , loss1: -737.98359375\n",
      "                , loss2: 0.027060174942016603\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 930 epoch, average loss: -31.2724365234375\n",
      "                , loss1: -737.984521484375\n",
      "                , loss2: 0.018105635046958925\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 940 epoch, average loss: -31.271026611328125\n",
      "                , loss1: -738.033642578125\n",
      "                , loss2: 0.021600228548049927\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 950 epoch, average loss: -31.21915283203125\n",
      "                , loss1: -738.025634765625\n",
      "                , loss2: 0.07313617467880248\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 960 epoch, average loss: -31.2429443359375\n",
      "                , loss1: -738.058056640625\n",
      "                , loss2: 0.05071737170219422\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 970 epoch, average loss: -31.263204956054686\n",
      "                , loss1: -737.94306640625\n",
      "                , loss2: 0.02558012306690216\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 980 epoch, average loss: -31.27091064453125\n",
      "                , loss1: -737.905126953125\n",
      "                , loss2: 0.016265317797660828\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 990 epoch, average loss: -31.266790771484374\n",
      "                , loss1: -737.941015625\n",
      "                , loss2: 0.02190830558538437\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1000 epoch, average loss: -31.2746337890625\n",
      "                , loss1: -737.997119140625\n",
      "                , loss2: 0.016441431641578675\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1010 epoch, average loss: -31.27043762207031\n",
      "                , loss1: -737.931103515625\n",
      "                , loss2: 0.017839303612709044\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1020 epoch, average loss: -31.254367065429687\n",
      "                , loss1: -737.96533203125\n",
      "                , loss2: 0.03536019623279572\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1030 epoch, average loss: -31.271664428710938\n",
      "                , loss1: -738.039306640625\n",
      "                , loss2: 0.02119927853345871\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1040 epoch, average loss: -31.271542358398438\n",
      "                , loss1: -737.97880859375\n",
      "                , loss2: 0.01876152157783508\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1050 epoch, average loss: -31.261346435546876\n",
      "                , loss1: -737.95849609375\n",
      "                , loss2: 0.0280904620885849\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1060 epoch, average loss: -31.276910400390626\n",
      "                , loss1: -737.93017578125\n",
      "                , loss2: 0.011332502961158753\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1070 epoch, average loss: -31.274896240234376\n",
      "                , loss1: -737.940625\n",
      "                , loss2: 0.013787573575973511\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1080 epoch, average loss: -31.269671630859374\n",
      "                , loss1: -737.928369140625\n",
      "                , loss2: 0.018488764762878418\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1090 epoch, average loss: -31.270687866210938\n",
      "                , loss1: -738.020556640625\n",
      "                , loss2: 0.02138240784406662\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1100 epoch, average loss: -31.247787475585938\n",
      "                , loss1: -737.91318359375\n",
      "                , loss2: 0.03972859084606171\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1110 epoch, average loss: -31.26842041015625\n",
      "                , loss1: -738.01884765625\n",
      "                , loss2: 0.023575958609580994\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1120 epoch, average loss: -31.258621215820312\n",
      "                , loss1: -738.011279296875\n",
      "                , loss2: 0.03305544257164002\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1130 epoch, average loss: -31.27099609375\n",
      "                , loss1: -737.889794921875\n",
      "                , loss2: 0.01552819162607193\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1140 epoch, average loss: -31.260574340820312\n",
      "                , loss1: -737.95078125\n",
      "                , loss2: 0.02853483855724335\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1150 epoch, average loss: -31.27222900390625\n",
      "                , loss1: -737.998486328125\n",
      "                , loss2: 0.01890503764152527\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1160 epoch, average loss: -31.262905883789063\n",
      "                , loss1: -737.914208984375\n",
      "                , loss2: 0.024652965366840363\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1170 epoch, average loss: -31.272735595703125\n",
      "                , loss1: -737.977587890625\n",
      "                , loss2: 0.017518411576747894\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1180 epoch, average loss: -31.264837646484374\n",
      "                , loss1: -738.026318359375\n",
      "                , loss2: 0.02747562825679779\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1190 epoch, average loss: -31.253643798828126\n",
      "                , loss1: -737.989013671875\n",
      "                , loss2: 0.03709031343460083\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1200 epoch, average loss: -31.269821166992188\n",
      "                , loss1: -737.983203125\n",
      "                , loss2: 0.020664730668067934\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1210 epoch, average loss: -31.278173828125\n",
      "                , loss1: -737.96875\n",
      "                , loss2: 0.011698894202709198\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1220 epoch, average loss: -31.272979736328125\n",
      "                , loss1: -738.016796875\n",
      "                , loss2: 0.018934275209903716\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1230 epoch, average loss: -31.266903686523438\n",
      "                , loss1: -737.951611328125\n",
      "                , loss2: 0.022239911556243896\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1240 epoch, average loss: -31.280014038085938\n",
      "                , loss1: -738.00810546875\n",
      "                , loss2: 0.011531049013137817\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1250 epoch, average loss: -31.269631958007814\n",
      "                , loss1: -737.98447265625\n",
      "                , loss2: 0.02091074138879776\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1260 epoch, average loss: -31.268817138671874\n",
      "                , loss1: -738.035400390625\n",
      "                , loss2: 0.023883891105651856\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1270 epoch, average loss: -31.273529052734375\n",
      "                , loss1: -737.953759765625\n",
      "                , loss2: 0.015708306431770326\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1280 epoch, average loss: -31.264425659179686\n",
      "                , loss1: -737.959130859375\n",
      "                , loss2: 0.02504193186759949\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1290 epoch, average loss: -31.265924072265626\n",
      "                , loss1: -738.062109375\n",
      "                , loss2: 0.02790856659412384\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1300 epoch, average loss: -31.264828491210938\n",
      "                , loss1: -738.118603515625\n",
      "                , loss2: 0.031402099132537845\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1310 epoch, average loss: -31.270504760742188\n",
      "                , loss1: -737.9806640625\n",
      "                , loss2: 0.0198741540312767\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1320 epoch, average loss: -31.2699951171875\n",
      "                , loss1: -738.00888671875\n",
      "                , loss2: 0.021582280099391938\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1330 epoch, average loss: -31.263970947265626\n",
      "                , loss1: -738.0654296875\n",
      "                , loss2: 0.03000100255012512\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1340 epoch, average loss: -31.261959838867188\n",
      "                , loss1: -737.982177734375\n",
      "                , loss2: 0.028483286499977112\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1350 epoch, average loss: -31.2782470703125\n",
      "                , loss1: -737.992236328125\n",
      "                , loss2: 0.012624353170394897\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1360 epoch, average loss: -31.265359497070314\n",
      "                , loss1: -737.99287109375\n",
      "                , loss2: 0.025537973642349242\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1370 epoch, average loss: -31.2820068359375\n",
      "                , loss1: -738.00703125\n",
      "                , loss2: 0.009491422772407531\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1380 epoch, average loss: -31.27147216796875\n",
      "                , loss1: -738.01943359375\n",
      "                , loss2: 0.020547986030578613\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1390 epoch, average loss: -31.274917602539062\n",
      "                , loss1: -737.966259765625\n",
      "                , loss2: 0.014849384129047394\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1400 epoch, average loss: -31.266864013671874\n",
      "                , loss1: -738.05244140625\n",
      "                , loss2: 0.026559367775917053\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1410 epoch, average loss: -31.257763671875\n",
      "                , loss1: -737.94794921875\n",
      "                , loss2: 0.031227341294288634\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1420 epoch, average loss: -31.275830078125\n",
      "                , loss1: -738.030224609375\n",
      "                , loss2: 0.0166526660323143\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1430 epoch, average loss: -31.272848510742186\n",
      "                , loss1: -738.04296875\n",
      "                , loss2: 0.020171038806438446\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n",
      "in 1440 epoch, average loss: -31.262872314453126\n",
      "                , loss1: -737.978173828125\n",
      "                , loss2: 0.027400672435760498\n",
      "                , weight: 0.042400000005475114\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0436\u001b[39m:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.0019\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=3e-5, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(10000):\n",
    "    if hgnn_trainer.weight > 0.0436:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.0019\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([340., 337., 342.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004906771344455349"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(torch.tensor([53.,56,53,54,56,55]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
