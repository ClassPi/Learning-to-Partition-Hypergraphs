{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycq/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "weight = 65\n",
    "\"\"\"\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer113\"] = {\"in_channels\":2048, \"out_channels\":2048, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer13\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":2, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\"\"\"\n",
    "\n",
    "partitions = 6\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 1019, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers2\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers51\"] = {\"in_channels\": 64, \"out_channels\": 32, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 64, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 32, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers54\"] = {\"in_channels\": 256, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers55\"] = {\"in_channels\": 512, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":32, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":64, \"out_channels\":128, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer21\"] = {\"in_channels\":128, \"out_channels\":256, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer22\"] = {\"in_channels\":256, \"out_channels\":512, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer23\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer24\"] = {\"in_channels\":64, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":6, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767, 1019)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/citeseer\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=1019, out_features=512, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (5): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.05, inplace=False)\n",
       "  (7): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (8): ReLU()\n",
       "  (9): Dropout(p=0.05, inplace=False)\n",
       "  (10): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (11): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): ReLU()\n",
       "  (13): Dropout(p=0.05, inplace=False)\n",
       "  (14): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (15): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -28.438430786132812\n",
      "                , loss1: -1.2083264350891114\n",
      "                , loss2: 50.10157775878906\n",
      "                , weight: 64.999\n",
      "=================================\n",
      "in 10 epoch, average loss: -563.48310546875\n",
      "                , loss1: -12.681715393066407\n",
      "                , loss2: 260.7440185546875\n",
      "                , weight: 64.98899999999995\n",
      "=================================\n",
      "in 20 epoch, average loss: -1590.22890625\n",
      "                , loss1: -25.072515869140624\n",
      "                , loss2: 39.053759765625\n",
      "                , weight: 64.9789999999999\n",
      "=================================\n",
      "in 30 epoch, average loss: -2837.98125\n",
      "                , loss1: -43.99250793457031\n",
      "                , loss2: 20.355488586425782\n",
      "                , weight: 64.96899999999985\n",
      "=================================\n",
      "in 40 epoch, average loss: -3621.750390625\n",
      "                , loss1: -55.974786376953126\n",
      "                , loss2: 14.559417724609375\n",
      "                , weight: 64.9589999999998\n",
      "=================================\n",
      "in 50 epoch, average loss: -4455.207421875\n",
      "                , loss1: -69.30055541992188\n",
      "                , loss2: 46.09632263183594\n",
      "                , weight: 64.94899999999976\n",
      "=================================\n",
      "in 60 epoch, average loss: -5272.101171875\n",
      "                , loss1: -82.47576904296875\n",
      "                , loss2: 84.15200805664062\n",
      "                , weight: 64.93899999999971\n",
      "=================================\n",
      "in 70 epoch, average loss: -6057.9\n",
      "                , loss1: -95.57131958007812\n",
      "                , loss2: 147.86861572265624\n",
      "                , weight: 64.92899999999966\n",
      "=================================\n",
      "in 80 epoch, average loss: -6892.89609375\n",
      "                , loss1: -108.4575927734375\n",
      "                , loss2: 148.5415771484375\n",
      "                , weight: 64.91899999999961\n",
      "=================================\n",
      "in 90 epoch, average loss: -7689.1546875\n",
      "                , loss1: -121.64180908203124\n",
      "                , loss2: 207.027685546875\n",
      "                , weight: 64.90899999999957\n",
      "=================================\n",
      "in 100 epoch, average loss: -8564.1734375\n",
      "                , loss1: -135.08572998046876\n",
      "                , loss2: 203.3554443359375\n",
      "                , weight: 64.89899999999952\n",
      "=================================\n",
      "in 110 epoch, average loss: -9385.875\n",
      "                , loss1: -147.557666015625\n",
      "                , loss2: 189.64615478515626\n",
      "                , weight: 64.88899999999947\n",
      "=================================\n",
      "in 120 epoch, average loss: -9976.1453125\n",
      "                , loss1: -156.193701171875\n",
      "                , loss2: 158.24716796875\n",
      "                , weight: 64.87899999999942\n",
      "=================================\n",
      "in 130 epoch, average loss: -10573.88359375\n",
      "                , loss1: -166.1890380859375\n",
      "                , loss2: 207.3714111328125\n",
      "                , weight: 64.86899999999937\n",
      "=================================\n",
      "in 140 epoch, average loss: -11148.5671875\n",
      "                , loss1: -175.4968017578125\n",
      "                , loss2: 234.76201171875\n",
      "                , weight: 64.85899999999933\n",
      "=================================\n",
      "in 150 epoch, average loss: -11543.7203125\n",
      "                , loss1: -180.926708984375\n",
      "                , loss2: 190.00535888671874\n",
      "                , weight: 64.84899999999928\n",
      "=================================\n",
      "in 160 epoch, average loss: -11934.759375\n",
      "                , loss1: -186.43299560546876\n",
      "                , loss2: 154.20648193359375\n",
      "                , weight: 64.83899999999923\n",
      "=================================\n",
      "in 170 epoch, average loss: -12202.05859375\n",
      "                , loss1: -190.1247314453125\n",
      "                , loss2: 124.38983154296875\n",
      "                , weight: 64.82899999999918\n",
      "=================================\n",
      "in 180 epoch, average loss: -12403.82109375\n",
      "                , loss1: -192.9732421875\n",
      "                , loss2: 105.3782958984375\n",
      "                , weight: 64.81899999999914\n",
      "=================================\n",
      "in 190 epoch, average loss: -12580.121875\n",
      "                , loss1: -195.4798095703125\n",
      "                , loss2: 89.60709228515626\n",
      "                , weight: 64.80899999999909\n",
      "=================================\n",
      "in 200 epoch, average loss: -12784.4265625\n",
      "                , loss1: -198.4818359375\n",
      "                , loss2: 77.88738403320312\n",
      "                , weight: 64.79899999999904\n",
      "=================================\n",
      "in 210 epoch, average loss: -13096.79609375\n",
      "                , loss1: -204.19664306640624\n",
      "                , loss2: 133.8124755859375\n",
      "                , weight: 64.78899999999899\n",
      "=================================\n",
      "in 220 epoch, average loss: -13546.99375\n",
      "                , loss1: -211.5579833984375\n",
      "                , loss2: 158.4706787109375\n",
      "                , weight: 64.77899999999894\n",
      "=================================\n",
      "in 230 epoch, average loss: -13728.28125\n",
      "                , loss1: -213.8482421875\n",
      "                , loss2: 123.41539306640625\n",
      "                , weight: 64.7689999999989\n",
      "=================================\n",
      "in 240 epoch, average loss: -13883.0859375\n",
      "                , loss1: -215.887109375\n",
      "                , loss2: 98.51773681640626\n",
      "                , weight: 64.75899999999885\n",
      "=================================\n",
      "in 250 epoch, average loss: -13964.7046875\n",
      "                , loss1: -216.874462890625\n",
      "                , loss2: 78.67457885742188\n",
      "                , weight: 64.7489999999988\n",
      "=================================\n",
      "in 260 epoch, average loss: -14074.1546875\n",
      "                , loss1: -218.3385498046875\n",
      "                , loss2: 61.847467041015626\n",
      "                , weight: 64.73899999999875\n",
      "=================================\n",
      "in 270 epoch, average loss: -14144.0109375\n",
      "                , loss1: -219.2948486328125\n",
      "                , loss2: 51.7132080078125\n",
      "                , weight: 64.7289999999987\n",
      "=================================\n",
      "in 280 epoch, average loss: -14216.7125\n",
      "                , loss1: -220.3204345703125\n",
      "                , loss2: 43.197845458984375\n",
      "                , weight: 64.71899999999866\n",
      "=================================\n",
      "in 290 epoch, average loss: -14304.9984375\n",
      "                , loss1: -221.614697265625\n",
      "                , loss2: 36.46442565917969\n",
      "                , weight: 64.70899999999861\n",
      "=================================\n",
      "in 300 epoch, average loss: -14366.640625\n",
      "                , loss1: -222.56982421875\n",
      "                , loss2: 34.40428466796875\n",
      "                , weight: 64.69899999999856\n",
      "=================================\n",
      "in 310 epoch, average loss: -14564.6140625\n",
      "                , loss1: -225.9462158203125\n",
      "                , loss2: 52.63594360351563\n",
      "                , weight: 64.68899999999852\n",
      "=================================\n",
      "in 320 epoch, average loss: -14631.409375\n",
      "                , loss1: -226.922216796875\n",
      "                , loss2: 46.713250732421876\n",
      "                , weight: 64.67899999999847\n",
      "=================================\n",
      "in 330 epoch, average loss: -14744.4\n",
      "                , loss1: -228.7411376953125\n",
      "                , loss2: 49.08854675292969\n",
      "                , weight: 64.66899999999842\n",
      "=================================\n",
      "in 340 epoch, average loss: -14913.825\n",
      "                , loss1: -231.752587890625\n",
      "                , loss2: 72.10555419921874\n",
      "                , weight: 64.65899999999837\n",
      "=================================\n",
      "in 350 epoch, average loss: -15197.0140625\n",
      "                , loss1: -236.7947021484375\n",
      "                , loss2: 112.5849853515625\n",
      "                , weight: 64.64899999999832\n",
      "=================================\n",
      "in 360 epoch, average loss: -15793.265625\n",
      "                , loss1: -246.8685546875\n",
      "                , loss2: 165.1704833984375\n",
      "                , weight: 64.63899999999828\n",
      "=================================\n",
      "in 370 epoch, average loss: -16541.490625\n",
      "                , loss1: -259.88076171875\n",
      "                , loss2: 255.5001953125\n",
      "                , weight: 64.62899999999823\n",
      "=================================\n",
      "in 380 epoch, average loss: -17588.0890625\n",
      "                , loss1: -278.0508544921875\n",
      "                , loss2: 380.5132080078125\n",
      "                , weight: 64.61899999999818\n",
      "=================================\n",
      "in 390 epoch, average loss: -18711.8078125\n",
      "                , loss1: -297.12080078125\n",
      "                , loss2: 486.188232421875\n",
      "                , weight: 64.60899999999813\n",
      "=================================\n",
      "in 400 epoch, average loss: -19745.409375\n",
      "                , loss1: -314.0767333984375\n",
      "                , loss2: 545.03662109375\n",
      "                , weight: 64.59899999999809\n",
      "=================================\n",
      "in 410 epoch, average loss: -20306.50625\n",
      "                , loss1: -320.99609375\n",
      "                , loss2: 427.75078125\n",
      "                , weight: 64.58899999999804\n",
      "=================================\n",
      "in 420 epoch, average loss: -20765.6578125\n",
      "                , loss1: -326.7390869140625\n",
      "                , loss2: 336.2913330078125\n",
      "                , weight: 64.57899999999799\n",
      "=================================\n",
      "in 430 epoch, average loss: -21074.625\n",
      "                , loss1: -330.3715087890625\n",
      "                , loss2: 258.61796875\n",
      "                , weight: 64.56899999999794\n",
      "=================================\n",
      "in 440 epoch, average loss: -21291.709375\n",
      "                , loss1: -333.00546875\n",
      "                , loss2: 208.285498046875\n",
      "                , weight: 64.5589999999979\n",
      "=================================\n",
      "in 450 epoch, average loss: -21540.2\n",
      "                , loss1: -337.6205078125\n",
      "                , loss2: 254.380517578125\n",
      "                , weight: 64.54899999999785\n",
      "=================================\n",
      "in 460 epoch, average loss: -21805.428125\n",
      "                , loss1: -341.175830078125\n",
      "                , loss2: 215.2499755859375\n",
      "                , weight: 64.5389999999978\n",
      "=================================\n",
      "in 470 epoch, average loss: -21920.371875\n",
      "                , loss1: -342.38994140625\n",
      "                , loss2: 175.2481201171875\n",
      "                , weight: 64.52899999999775\n",
      "=================================\n",
      "in 480 epoch, average loss: -22009.559375\n",
      "                , loss1: -343.472119140625\n",
      "                , loss2: 152.4629638671875\n",
      "                , weight: 64.5189999999977\n",
      "=================================\n",
      "in 490 epoch, average loss: -22124.2\n",
      "                , loss1: -344.85810546875\n",
      "                , loss2: 123.80277099609376\n",
      "                , weight: 64.50899999999766\n",
      "=================================\n",
      "in 500 epoch, average loss: -22195.765625\n",
      "                , loss1: -345.696142578125\n",
      "                , loss2: 102.84271240234375\n",
      "                , weight: 64.49899999999761\n",
      "=================================\n",
      "in 510 epoch, average loss: -22302.7890625\n",
      "                , loss1: -347.4706298828125\n",
      "                , loss2: 106.806494140625\n",
      "                , weight: 64.48899999999756\n",
      "=================================\n",
      "in 520 epoch, average loss: -22346.465625\n",
      "                , loss1: -347.977197265625\n",
      "                , loss2: 92.32103271484375\n",
      "                , weight: 64.47899999999751\n",
      "=================================\n",
      "in 530 epoch, average loss: -22403.5640625\n",
      "                , loss1: -348.7364013671875\n",
      "                , loss2: 80.69075927734374\n",
      "                , weight: 64.46899999999746\n",
      "=================================\n",
      "in 540 epoch, average loss: -22449.4953125\n",
      "                , loss1: -349.291650390625\n",
      "                , loss2: 67.0672119140625\n",
      "                , weight: 64.45899999999742\n",
      "=================================\n",
      "in 550 epoch, average loss: -22479.9859375\n",
      "                , loss1: -349.733447265625\n",
      "                , loss2: 61.55804443359375\n",
      "                , weight: 64.44899999999737\n",
      "=================================\n",
      "in 560 epoch, average loss: -22530.809375\n",
      "                , loss1: -350.4956787109375\n",
      "                , loss2: 56.35905151367187\n",
      "                , weight: 64.43899999999732\n",
      "=================================\n",
      "in 570 epoch, average loss: -22538.96875\n",
      "                , loss1: -350.584716796875\n",
      "                , loss2: 50.4316650390625\n",
      "                , weight: 64.42899999999727\n",
      "=================================\n",
      "in 580 epoch, average loss: -22567.9484375\n",
      "                , loss1: -350.9765869140625\n",
      "                , loss2: 43.1896728515625\n",
      "                , weight: 64.41899999999723\n",
      "=================================\n",
      "in 590 epoch, average loss: -22621.6234375\n",
      "                , loss1: -351.7957763671875\n",
      "                , loss2: 38.775143432617185\n",
      "                , weight: 64.40899999999718\n",
      "=================================\n",
      "in 600 epoch, average loss: -22635.74375\n",
      "                , loss1: -352.0113525390625\n",
      "                , loss2: 35.01981201171875\n",
      "                , weight: 64.39899999999713\n",
      "=================================\n",
      "in 610 epoch, average loss: -22641.0609375\n",
      "                , loss1: -352.1100341796875\n",
      "                , loss2: 32.53342895507812\n",
      "                , weight: 64.38899999999708\n",
      "=================================\n",
      "in 620 epoch, average loss: -22675.64375\n",
      "                , loss1: -352.616259765625\n",
      "                , loss2: 27.023321533203124\n",
      "                , weight: 64.37899999999703\n",
      "=================================\n",
      "in 630 epoch, average loss: -22690.978125\n",
      "                , loss1: -352.9070556640625\n",
      "                , loss2: 26.88585510253906\n",
      "                , weight: 64.36899999999699\n",
      "=================================\n",
      "in 640 epoch, average loss: -22890.6046875\n",
      "                , loss1: -359.0101806640625\n",
      "                , loss2: 216.5327880859375\n",
      "                , weight: 64.35899999999694\n",
      "=================================\n",
      "in 650 epoch, average loss: -23434.309375\n",
      "                , loss1: -371.7433837890625\n",
      "                , loss2: 488.67421875\n",
      "                , weight: 64.34899999999689\n",
      "=================================\n",
      "in 660 epoch, average loss: -23604.559375\n",
      "                , loss1: -373.833447265625\n",
      "                , loss2: 449.1923828125\n",
      "                , weight: 64.33899999999684\n",
      "=================================\n",
      "in 670 epoch, average loss: -23682.615625\n",
      "                , loss1: -374.743115234375\n",
      "                , loss2: 425.91689453125\n",
      "                , weight: 64.3289999999968\n",
      "=================================\n",
      "in 680 epoch, average loss: -23734.571875\n",
      "                , loss1: -376.48056640625\n",
      "                , loss2: 481.9744140625\n",
      "                , weight: 64.31899999999675\n",
      "=================================\n",
      "in 690 epoch, average loss: -23805.475\n",
      "                , loss1: -377.7156005859375\n",
      "                , loss2: 486.737548828125\n",
      "                , weight: 64.3089999999967\n",
      "=================================\n",
      "in 700 epoch, average loss: -23869.1140625\n",
      "                , loss1: -378.238037109375\n",
      "                , loss2: 452.917822265625\n",
      "                , weight: 64.29899999999665\n",
      "=================================\n",
      "in 710 epoch, average loss: -23890.5953125\n",
      "                , loss1: -378.381005859375\n",
      "                , loss2: 436.8421875\n",
      "                , weight: 64.2889999999966\n",
      "=================================\n",
      "in 720 epoch, average loss: -23924.875\n",
      "                , loss1: -378.644775390625\n",
      "                , loss2: 415.7357421875\n",
      "                , weight: 64.27899999999656\n",
      "=================================\n",
      "in 730 epoch, average loss: -23952.2609375\n",
      "                , loss1: -378.9845703125\n",
      "                , loss2: 406.4058837890625\n",
      "                , weight: 64.26899999999651\n",
      "=================================\n",
      "in 740 epoch, average loss: -23972.5359375\n",
      "                , loss1: -379.5739990234375\n",
      "                , loss2: 420.216015625\n",
      "                , weight: 64.25899999999646\n",
      "=================================\n",
      "in 750 epoch, average loss: -23986.2390625\n",
      "                , loss1: -379.710791015625\n",
      "                , loss2: 411.50859375\n",
      "                , weight: 64.24899999999641\n",
      "=================================\n",
      "in 760 epoch, average loss: -24004.825\n",
      "                , loss1: -380.0150390625\n",
      "                , loss2: 408.6682373046875\n",
      "                , weight: 64.23899999999637\n",
      "=================================\n",
      "in 770 epoch, average loss: -24009.51875\n",
      "                , loss1: -379.8540771484375\n",
      "                , loss2: 389.8377197265625\n",
      "                , weight: 64.22899999999632\n",
      "=================================\n",
      "in 780 epoch, average loss: -24034.790625\n",
      "                , loss1: -380.501953125\n",
      "                , loss2: 402.37744140625\n",
      "                , weight: 64.21899999999627\n",
      "=================================\n",
      "in 790 epoch, average loss: -24060.6078125\n",
      "                , loss1: -380.7403076171875\n",
      "                , loss2: 388.057177734375\n",
      "                , weight: 64.20899999999622\n",
      "=================================\n",
      "in 800 epoch, average loss: -24072.15625\n",
      "                , loss1: -380.905810546875\n",
      "                , loss2: 383.3311279296875\n",
      "                , weight: 64.19899999999618\n",
      "=================================\n",
      "in 810 epoch, average loss: -24077.4515625\n",
      "                , loss1: -380.995166015625\n",
      "                , loss2: 379.9630615234375\n",
      "                , weight: 64.18899999999613\n",
      "=================================\n",
      "in 820 epoch, average loss: -24092.503125\n",
      "                , loss1: -381.1959228515625\n",
      "                , loss2: 373.9833984375\n",
      "                , weight: 64.17899999999608\n",
      "=================================\n",
      "in 830 epoch, average loss: -24101.7546875\n",
      "                , loss1: -381.366455078125\n",
      "                , loss2: 371.8656982421875\n",
      "                , weight: 64.16899999999603\n",
      "=================================\n",
      "in 840 epoch, average loss: -24111.115625\n",
      "                , loss1: -381.6050048828125\n",
      "                , loss2: 373.9983154296875\n",
      "                , weight: 64.15899999999598\n",
      "=================================\n",
      "in 850 epoch, average loss: -24123.55\n",
      "                , loss1: -381.47666015625\n",
      "                , loss2: 349.5105224609375\n",
      "                , weight: 64.14899999999594\n",
      "=================================\n",
      "in 860 epoch, average loss: -24131.040625\n",
      "                , loss1: -381.3171630859375\n",
      "                , loss2: 327.976806640625\n",
      "                , weight: 64.13899999999589\n",
      "=================================\n",
      "in 870 epoch, average loss: -24121.0125\n",
      "                , loss1: -381.880712890625\n",
      "                , loss2: 370.33291015625\n",
      "                , weight: 64.12899999999584\n",
      "=================================\n",
      "in 880 epoch, average loss: -24141.353125\n",
      "                , loss1: -382.009814453125\n",
      "                , loss2: 354.4497314453125\n",
      "                , weight: 64.1189999999958\n",
      "=================================\n",
      "in 890 epoch, average loss: -24147.3265625\n",
      "                , loss1: -382.148828125\n",
      "                , loss2: 353.56787109375\n",
      "                , weight: 64.10899999999575\n",
      "=================================\n",
      "in 900 epoch, average loss: -24157.853125\n",
      "                , loss1: -382.278076171875\n",
      "                , loss2: 347.5069091796875\n",
      "                , weight: 64.0989999999957\n",
      "=================================\n",
      "in 910 epoch, average loss: -24165.371875\n",
      "                , loss1: -382.464697265625\n",
      "                , loss2: 348.1287109375\n",
      "                , weight: 64.08899999999565\n",
      "=================================\n",
      "in 920 epoch, average loss: -24168.9625\n",
      "                , loss1: -382.4970458984375\n",
      "                , loss2: 342.787744140625\n",
      "                , weight: 64.0789999999956\n",
      "=================================\n",
      "in 930 epoch, average loss: -24178.0921875\n",
      "                , loss1: -382.4715576171875\n",
      "                , loss2: 328.20439453125\n",
      "                , weight: 64.06899999999555\n",
      "=================================\n",
      "in 940 epoch, average loss: -24167.175\n",
      "                , loss1: -381.8830078125\n",
      "                , loss2: 297.5858642578125\n",
      "                , weight: 64.0589999999955\n",
      "=================================\n",
      "in 950 epoch, average loss: -24166.3203125\n",
      "                , loss1: -381.86962890625\n",
      "                , loss2: 293.765771484375\n",
      "                , weight: 64.04899999999546\n",
      "=================================\n",
      "in 960 epoch, average loss: -24152.465625\n",
      "                , loss1: -382.339306640625\n",
      "                , loss2: 333.878857421875\n",
      "                , weight: 64.03899999999541\n",
      "=================================\n",
      "in 970 epoch, average loss: -24160.4953125\n",
      "                , loss1: -382.602880859375\n",
      "                , loss2: 338.904541015625\n",
      "                , weight: 64.02899999999536\n",
      "=================================\n",
      "in 980 epoch, average loss: -24170.121875\n",
      "                , loss1: -382.937353515625\n",
      "                , loss2: 346.870458984375\n",
      "                , weight: 64.01899999999532\n",
      "=================================\n",
      "in 990 epoch, average loss: -24182.8734375\n",
      "                , loss1: -382.944970703125\n",
      "                , loss2: 330.7737060546875\n",
      "                , weight: 64.00899999999527\n",
      "=================================\n",
      "in 1000 epoch, average loss: -24192.4109375\n",
      "                , loss1: -383.1452392578125\n",
      "                , loss2: 330.2239990234375\n",
      "                , weight: 63.998999999995235\n",
      "=================================\n",
      "in 1010 epoch, average loss: -24195.5125\n",
      "                , loss1: -383.2613037109375\n",
      "                , loss2: 330.719384765625\n",
      "                , weight: 63.98899999999526\n",
      "=================================\n",
      "in 1020 epoch, average loss: -24259.2359375\n",
      "                , loss1: -384.2212890625\n",
      "                , loss2: 324.5886962890625\n",
      "                , weight: 63.97899999999528\n",
      "=================================\n",
      "in 1030 epoch, average loss: -24264.475\n",
      "                , loss1: -384.37255859375\n",
      "                , loss2: 325.1846435546875\n",
      "                , weight: 63.968999999995305\n",
      "=================================\n",
      "in 1040 epoch, average loss: -24303.509375\n",
      "                , loss1: -385.044775390625\n",
      "                , loss2: 325.302197265625\n",
      "                , weight: 63.95899999999533\n",
      "=================================\n",
      "in 1050 epoch, average loss: -24316.321875\n",
      "                , loss1: -385.294970703125\n",
      "                , loss2: 324.63759765625\n",
      "                , weight: 63.94899999999535\n",
      "=================================\n",
      "in 1060 epoch, average loss: -24317.2875\n",
      "                , loss1: -385.312744140625\n",
      "                , loss2: 320.957763671875\n",
      "                , weight: 63.938999999995374\n",
      "=================================\n",
      "in 1070 epoch, average loss: -24330.0484375\n",
      "                , loss1: -385.5632568359375\n",
      "                , loss2: 320.3627197265625\n",
      "                , weight: 63.9289999999954\n",
      "=================================\n",
      "in 1080 epoch, average loss: -24332.8609375\n",
      "                , loss1: -385.630224609375\n",
      "                , loss2: 317.9731689453125\n",
      "                , weight: 63.91899999999542\n",
      "=================================\n",
      "in 1090 epoch, average loss: -24331.7875\n",
      "                , loss1: -385.7178955078125\n",
      "                , loss2: 320.792919921875\n",
      "                , weight: 63.908999999995444\n",
      "=================================\n",
      "in 1100 epoch, average loss: -24339.965625\n",
      "                , loss1: -385.87978515625\n",
      "                , loss2: 319.0997802734375\n",
      "                , weight: 63.89899999999547\n",
      "=================================\n",
      "in 1110 epoch, average loss: -24336.453125\n",
      "                , loss1: -385.8989990234375\n",
      "                , loss2: 319.9877197265625\n",
      "                , weight: 63.88899999999549\n",
      "=================================\n",
      "in 1120 epoch, average loss: -24354.6125\n",
      "                , loss1: -386.2966552734375\n",
      "                , loss2: 323.3700439453125\n",
      "                , weight: 63.878999999995514\n",
      "=================================\n",
      "in 1130 epoch, average loss: -24381.9359375\n",
      "                , loss1: -386.8907470703125\n",
      "                , loss2: 330.12802734375\n",
      "                , weight: 63.86899999999554\n",
      "=================================\n",
      "in 1140 epoch, average loss: -24710.55\n",
      "                , loss1: -393.0623779296875\n",
      "                , loss2: 391.78056640625\n",
      "                , weight: 63.85899999999556\n",
      "=================================\n",
      "in 1150 epoch, average loss: -25950.115625\n",
      "                , loss1: -414.7015625\n",
      "                , loss2: 530.01025390625\n",
      "                , weight: 63.848999999995584\n",
      "=================================\n",
      "in 1160 epoch, average loss: -26943.71875\n",
      "                , loss1: -430.32509765625\n",
      "                , loss2: 529.73173828125\n",
      "                , weight: 63.83899999999561\n",
      "=================================\n",
      "in 1170 epoch, average loss: -27526.30625\n",
      "                , loss1: -439.920458984375\n",
      "                , loss2: 555.3509765625\n",
      "                , weight: 63.82899999999563\n",
      "=================================\n",
      "in 1180 epoch, average loss: -27911.86875\n",
      "                , loss1: -445.97392578125\n",
      "                , loss2: 551.744677734375\n",
      "                , weight: 63.818999999995654\n",
      "=================================\n",
      "in 1190 epoch, average loss: -28285.4375\n",
      "                , loss1: -451.383642578125\n",
      "                , loss2: 518.932275390625\n",
      "                , weight: 63.80899999999568\n",
      "=================================\n",
      "in 1200 epoch, average loss: -28501.825\n",
      "                , loss1: -454.6802734375\n",
      "                , loss2: 508.36591796875\n",
      "                , weight: 63.7989999999957\n",
      "=================================\n",
      "in 1210 epoch, average loss: -28681.26875\n",
      "                , loss1: -457.418115234375\n",
      "                , loss2: 499.033642578125\n",
      "                , weight: 63.788999999995724\n",
      "=================================\n",
      "in 1220 epoch, average loss: -28831.1625\n",
      "                , loss1: -459.81865234375\n",
      "                , loss2: 497.679443359375\n",
      "                , weight: 63.77899999999575\n",
      "=================================\n",
      "in 1230 epoch, average loss: -29046.4375\n",
      "                , loss1: -463.814697265625\n",
      "                , loss2: 532.64443359375\n",
      "                , weight: 63.76899999999577\n",
      "=================================\n",
      "in 1240 epoch, average loss: -29396.3875\n",
      "                , loss1: -470.3171875\n",
      "                , loss2: 592.678759765625\n",
      "                , weight: 63.758999999995794\n",
      "=================================\n",
      "in 1250 epoch, average loss: -29562.43125\n",
      "                , loss1: -472.4578125\n",
      "                , loss2: 558.40556640625\n",
      "                , weight: 63.74899999999582\n",
      "=================================\n",
      "in 1260 epoch, average loss: -29685.721875\n",
      "                , loss1: -474.19189453125\n",
      "                , loss2: 540.92548828125\n",
      "                , weight: 63.73899999999584\n",
      "=================================\n",
      "in 1270 epoch, average loss: -29796.034375\n",
      "                , loss1: -475.376416015625\n",
      "                , loss2: 501.367431640625\n",
      "                , weight: 63.728999999995864\n",
      "=================================\n",
      "in 1280 epoch, average loss: -29875.71875\n",
      "                , loss1: -476.28759765625\n",
      "                , loss2: 474.9966796875\n",
      "                , weight: 63.71899999999589\n",
      "=================================\n",
      "in 1290 epoch, average loss: -29977.265625\n",
      "                , loss1: -477.727587890625\n",
      "                , loss2: 460.431884765625\n",
      "                , weight: 63.70899999999591\n",
      "=================================\n",
      "in 1300 epoch, average loss: -29998.60625\n",
      "                , loss1: -477.706494140625\n",
      "                , loss2: 432.970263671875\n",
      "                , weight: 63.698999999995934\n",
      "=================================\n",
      "in 1310 epoch, average loss: -30038.36875\n",
      "                , loss1: -478.33447265625\n",
      "                , loss2: 428.425927734375\n",
      "                , weight: 63.68899999999596\n",
      "=================================\n",
      "in 1320 epoch, average loss: -30084.309375\n",
      "                , loss1: -479.360498046875\n",
      "                , loss2: 443.0455078125\n",
      "                , weight: 63.67899999999598\n",
      "=================================\n",
      "in 1330 epoch, average loss: -30115.734375\n",
      "                , loss1: -479.80712890625\n",
      "                , loss2: 435.26328125\n",
      "                , weight: 63.668999999996004\n",
      "=================================\n",
      "in 1340 epoch, average loss: -30151.2125\n",
      "                , loss1: -480.47529296875\n",
      "                , loss2: 437.5212890625\n",
      "                , weight: 63.65899999999603\n",
      "=================================\n",
      "in 1350 epoch, average loss: -30175.671875\n",
      "                , loss1: -480.777734375\n",
      "                , loss2: 427.512060546875\n",
      "                , weight: 63.64899999999605\n",
      "=================================\n",
      "in 1360 epoch, average loss: -30190.228125\n",
      "                , loss1: -481.051123046875\n",
      "                , loss2: 425.549560546875\n",
      "                , weight: 63.638999999996074\n",
      "=================================\n",
      "in 1370 epoch, average loss: -30210.98125\n",
      "                , loss1: -481.418896484375\n",
      "                , loss2: 423.39013671875\n",
      "                , weight: 63.6289999999961\n",
      "=================================\n",
      "in 1380 epoch, average loss: -30190.24375\n",
      "                , loss1: -481.346826171875\n",
      "                , loss2: 434.725732421875\n",
      "                , weight: 63.61899999999612\n",
      "=================================\n",
      "in 1390 epoch, average loss: -30189.896875\n",
      "                , loss1: -481.81181640625\n",
      "                , loss2: 459.836474609375\n",
      "                , weight: 63.60899999999614\n",
      "=================================\n",
      "in 1400 epoch, average loss: -30217.0375\n",
      "                , loss1: -482.04833984375\n",
      "                , loss2: 442.920703125\n",
      "                , weight: 63.59899999999617\n",
      "=================================\n",
      "in 1410 epoch, average loss: -30249.65\n",
      "                , loss1: -482.2740234375\n",
      "                , loss2: 419.846533203125\n",
      "                , weight: 63.58899999999619\n",
      "=================================\n",
      "in 1420 epoch, average loss: -30261.06875\n",
      "                , loss1: -482.58271484375\n",
      "                , loss2: 423.2294921875\n",
      "                , weight: 63.57899999999621\n",
      "=================================\n",
      "in 1430 epoch, average loss: -30271.734375\n",
      "                , loss1: -482.97900390625\n",
      "                , loss2: 432.9298828125\n",
      "                , weight: 63.56899999999624\n",
      "=================================\n",
      "in 1440 epoch, average loss: -30292.809375\n",
      "                , loss1: -483.17314453125\n",
      "                , loss2: 419.36796875\n",
      "                , weight: 63.55899999999626\n",
      "=================================\n",
      "in 1450 epoch, average loss: -30298.16875\n",
      "                , loss1: -483.328466796875\n",
      "                , loss2: 419.0494140625\n",
      "                , weight: 63.54899999999628\n",
      "=================================\n",
      "in 1460 epoch, average loss: -30304.3125\n",
      "                , loss1: -483.5955078125\n",
      "                , loss2: 425.036767578125\n",
      "                , weight: 63.53899999999631\n",
      "=================================\n",
      "in 1470 epoch, average loss: -30293.434375\n",
      "                , loss1: -483.409326171875\n",
      "                , loss2: 419.244140625\n",
      "                , weight: 63.52899999999633\n",
      "=================================\n",
      "in 1480 epoch, average loss: -30313.796875\n",
      "                , loss1: -483.836669921875\n",
      "                , loss2: 421.20107421875\n",
      "                , weight: 63.51899999999635\n",
      "=================================\n",
      "in 1490 epoch, average loss: -30318.0625\n",
      "                , loss1: -483.936767578125\n",
      "                , loss2: 418.45732421875\n",
      "                , weight: 63.50899999999638\n",
      "=================================\n",
      "in 1500 epoch, average loss: -30326.915625\n",
      "                , loss1: -484.13984375\n",
      "                , loss2: 417.6609375\n",
      "                , weight: 63.4989999999964\n",
      "=================================\n",
      "in 1510 epoch, average loss: -30322.721875\n",
      "                , loss1: -484.144921875\n",
      "                , loss2: 417.3341796875\n",
      "                , weight: 63.48899999999642\n",
      "=================================\n",
      "in 1520 epoch, average loss: -30326.38125\n",
      "                , loss1: -484.3685546875\n",
      "                , loss2: 423.031103515625\n",
      "                , weight: 63.47899999999645\n",
      "=================================\n",
      "in 1530 epoch, average loss: -30313.1\n",
      "                , loss1: -484.1458984375\n",
      "                , loss2: 417.33525390625\n",
      "                , weight: 63.46899999999647\n",
      "=================================\n",
      "in 1540 epoch, average loss: -30335.075\n",
      "                , loss1: -484.620458984375\n",
      "                , loss2: 420.63330078125\n",
      "                , weight: 63.45899999999649\n",
      "=================================\n",
      "in 1550 epoch, average loss: -30315.50625\n",
      "                , loss1: -484.3951171875\n",
      "                , loss2: 421.0587890625\n",
      "                , weight: 63.448999999996516\n",
      "=================================\n",
      "in 1560 epoch, average loss: -30338.253125\n",
      "                , loss1: -484.71865234375\n",
      "                , loss2: 413.99287109375\n",
      "                , weight: 63.43899999999654\n",
      "=================================\n",
      "in 1570 epoch, average loss: -30333.3\n",
      "                , loss1: -484.85146484375\n",
      "                , loss2: 422.524365234375\n",
      "                , weight: 63.42899999999656\n",
      "=================================\n",
      "in 1580 epoch, average loss: -30340.35625\n",
      "                , loss1: -485.000927734375\n",
      "                , loss2: 420.1005859375\n",
      "                , weight: 63.418999999996586\n",
      "=================================\n",
      "in 1590 epoch, average loss: -30335.215625\n",
      "                , loss1: -485.042138671875\n",
      "                , loss2: 423.007568359375\n",
      "                , weight: 63.40899999999661\n",
      "=================================\n",
      "in 1600 epoch, average loss: -30348.56875\n",
      "                , loss1: -485.24384765625\n",
      "                , loss2: 417.588916015625\n",
      "                , weight: 63.39899999999663\n",
      "=================================\n",
      "in 1610 epoch, average loss: -30356.2\n",
      "                , loss1: -485.4939453125\n",
      "                , loss2: 420.954296875\n",
      "                , weight: 63.388999999996656\n",
      "=================================\n",
      "in 1620 epoch, average loss: -30385.1125\n",
      "                , loss1: -486.4755859375\n",
      "                , loss2: 449.4109375\n",
      "                , weight: 63.37899999999668\n",
      "=================================\n",
      "in 1630 epoch, average loss: -30832.96875\n",
      "                , loss1: -494.280810546875\n",
      "                , loss2: 491.327099609375\n",
      "                , weight: 63.3689999999967\n",
      "=================================\n",
      "in 1640 epoch, average loss: -32122.5125\n",
      "                , loss1: -515.052197265625\n",
      "                , loss2: 512.9736328125\n",
      "                , weight: 63.358999999996726\n",
      "=================================\n",
      "in 1650 epoch, average loss: -35151.2875\n",
      "                , loss1: -563.5158203125\n",
      "                , loss2: 549.356982421875\n",
      "                , weight: 63.34899999999675\n",
      "=================================\n",
      "in 1660 epoch, average loss: -37743.3375\n",
      "                , loss1: -606.357568359375\n",
      "                , loss2: 665.449072265625\n",
      "                , weight: 63.33899999999677\n",
      "=================================\n",
      "in 1670 epoch, average loss: -39444.40625\n",
      "                , loss1: -633.918115234375\n",
      "                , loss2: 703.8353515625\n",
      "                , weight: 63.328999999996796\n",
      "=================================\n",
      "in 1680 epoch, average loss: -39956.6375\n",
      "                , loss1: -641.2677734375\n",
      "                , loss2: 650.675537109375\n",
      "                , weight: 63.31899999999682\n",
      "=================================\n",
      "in 1690 epoch, average loss: -40272.68125\n",
      "                , loss1: -644.940185546875\n",
      "                , loss2: 560.736181640625\n",
      "                , weight: 63.30899999999684\n",
      "=================================\n",
      "in 1700 epoch, average loss: -40624.0375\n",
      "                , loss1: -649.19462890625\n",
      "                , loss2: 472.25146484375\n",
      "                , weight: 63.298999999996866\n",
      "=================================\n",
      "in 1710 epoch, average loss: -40774.621875\n",
      "                , loss1: -652.11181640625\n",
      "                , loss2: 499.82265625\n",
      "                , weight: 63.28899999999689\n",
      "=================================\n",
      "in 1720 epoch, average loss: -40874.703125\n",
      "                , loss1: -653.346875\n",
      "                , loss2: 471.375146484375\n",
      "                , weight: 63.27899999999691\n",
      "=================================\n",
      "in 1730 epoch, average loss: -40981.971875\n",
      "                , loss1: -655.042822265625\n",
      "                , loss2: 464.87626953125\n",
      "                , weight: 63.268999999996936\n",
      "=================================\n",
      "in 1740 epoch, average loss: -41074.375\n",
      "                , loss1: -656.695556640625\n",
      "                , loss2: 470.48115234375\n",
      "                , weight: 63.25899999999696\n",
      "=================================\n",
      "in 1750 epoch, average loss: -41078.059375\n",
      "                , loss1: -656.656201171875\n",
      "                , loss2: 457.744970703125\n",
      "                , weight: 63.24899999999698\n",
      "=================================\n",
      "in 1760 epoch, average loss: -41127.4125\n",
      "                , loss1: -657.338525390625\n",
      "                , loss2: 444.972216796875\n",
      "                , weight: 63.238999999997006\n",
      "=================================\n",
      "in 1770 epoch, average loss: -41171.215625\n",
      "                , loss1: -658.22236328125\n",
      "                , loss2: 450.48984375\n",
      "                , weight: 63.22899999999703\n",
      "=================================\n",
      "in 1780 epoch, average loss: -41203.528125\n",
      "                , loss1: -658.6623046875\n",
      "                , loss2: 439.4029296875\n",
      "                , weight: 63.21899999999705\n",
      "=================================\n",
      "in 1790 epoch, average loss: -41228.5375\n",
      "                , loss1: -659.261328125\n",
      "                , loss2: 445.676513671875\n",
      "                , weight: 63.208999999997076\n",
      "=================================\n",
      "in 1800 epoch, average loss: -41253.93125\n",
      "                , loss1: -659.7462890625\n",
      "                , loss2: 444.3443359375\n",
      "                , weight: 63.1989999999971\n",
      "=================================\n",
      "in 1810 epoch, average loss: -41269.015625\n",
      "                , loss1: -659.95810546875\n",
      "                , loss2: 436.04892578125\n",
      "                , weight: 63.18899999999712\n",
      "=================================\n",
      "in 1820 epoch, average loss: -41280.921875\n",
      "                , loss1: -660.202001953125\n",
      "                , loss2: 432.952783203125\n",
      "                , weight: 63.178999999997146\n",
      "=================================\n",
      "in 1830 epoch, average loss: -41261.421875\n",
      "                , loss1: -659.97607421875\n",
      "                , loss2: 431.578857421875\n",
      "                , weight: 63.16899999999717\n",
      "=================================\n",
      "in 1840 epoch, average loss: -41316.73125\n",
      "                , loss1: -660.886083984375\n",
      "                , loss2: 427.145751953125\n",
      "                , weight: 63.15899999999719\n",
      "=================================\n",
      "in 1850 epoch, average loss: -41322.75\n",
      "                , loss1: -661.1666015625\n",
      "                , loss2: 432.230517578125\n",
      "                , weight: 63.148999999997216\n",
      "=================================\n",
      "in 1860 epoch, average loss: -41359.11875\n",
      "                , loss1: -661.713671875\n",
      "                , loss2: 423.79794921875\n",
      "                , weight: 63.13899999999724\n",
      "=================================\n",
      "in 1870 epoch, average loss: -41324.7875\n",
      "                , loss1: -661.3642578125\n",
      "                , loss2: 429.45419921875\n",
      "                , weight: 63.12899999999726\n",
      "=================================\n",
      "in 1880 epoch, average loss: -41343.69375\n",
      "                , loss1: -661.792236328125\n",
      "                , loss2: 430.9453125\n",
      "                , weight: 63.118999999997285\n",
      "=================================\n",
      "in 1890 epoch, average loss: -41346.740625\n",
      "                , loss1: -661.7837890625\n",
      "                , loss2: 420.75673828125\n",
      "                , weight: 63.10899999999731\n",
      "=================================\n",
      "in 1900 epoch, average loss: -41368.153125\n",
      "                , loss1: -662.153564453125\n",
      "                , loss2: 416.0548828125\n",
      "                , weight: 63.09899999999733\n",
      "=================================\n",
      "in 1910 epoch, average loss: -41362.528125\n",
      "                , loss1: -662.360986328125\n",
      "                , loss2: 428.14130859375\n",
      "                , weight: 63.088999999997355\n",
      "=================================\n",
      "in 1920 epoch, average loss: -41317.915625\n",
      "                , loss1: -661.95556640625\n",
      "                , loss2: 440.556396484375\n",
      "                , weight: 63.07899999999738\n",
      "=================================\n",
      "in 1930 epoch, average loss: -41326.9375\n",
      "                , loss1: -662.22353515625\n",
      "                , loss2: 441.81796875\n",
      "                , weight: 63.0689999999974\n",
      "=================================\n",
      "in 1940 epoch, average loss: -41345.384375\n",
      "                , loss1: -662.49482421875\n",
      "                , loss2: 433.85654296875\n",
      "                , weight: 63.058999999997425\n",
      "=================================\n",
      "in 1950 epoch, average loss: -41347.928125\n",
      "                , loss1: -662.50205078125\n",
      "                , loss2: 425.14423828125\n",
      "                , weight: 63.04899999999745\n",
      "=================================\n",
      "in 1960 epoch, average loss: -41355.83125\n",
      "                , loss1: -662.655615234375\n",
      "                , loss2: 420.30107421875\n",
      "                , weight: 63.03899999999747\n",
      "=================================\n",
      "in 1970 epoch, average loss: -41356.64375\n",
      "                , loss1: -662.946728515625\n",
      "                , loss2: 431.20654296875\n",
      "                , weight: 63.028999999997495\n",
      "=================================\n",
      "in 1980 epoch, average loss: -41366.2625\n",
      "                , loss1: -663.10029296875\n",
      "                , loss2: 424.640283203125\n",
      "                , weight: 63.01899999999752\n",
      "=================================\n",
      "in 1990 epoch, average loss: -41359.34375\n",
      "                , loss1: -662.963720703125\n",
      "                , loss2: 416.318359375\n",
      "                , weight: 63.00899999999754\n",
      "=================================\n",
      "in 2000 epoch, average loss: -41363.959375\n",
      "                , loss1: -663.1568359375\n",
      "                , loss2: 417.24296875\n",
      "                , weight: 62.998999999997565\n",
      "=================================\n",
      "in 2010 epoch, average loss: -41362.91875\n",
      "                , loss1: -663.2056640625\n",
      "                , loss2: 414.72841796875\n",
      "                , weight: 62.98899999999759\n",
      "=================================\n",
      "in 2020 epoch, average loss: -41348.4375\n",
      "                , loss1: -663.36279296875\n",
      "                , loss2: 432.4708984375\n",
      "                , weight: 62.97899999999761\n",
      "=================================\n",
      "in 2030 epoch, average loss: -41315.65\n",
      "                , loss1: -662.61376953125\n",
      "                , loss2: 411.45625\n",
      "                , weight: 62.968999999997635\n",
      "=================================\n",
      "in 2040 epoch, average loss: -41328.965625\n",
      "                , loss1: -662.78994140625\n",
      "                , loss2: 402.60361328125\n",
      "                , weight: 62.95899999999766\n",
      "=================================\n",
      "in 2050 epoch, average loss: -41304.5875\n",
      "                , loss1: -663.016015625\n",
      "                , loss2: 434.58994140625\n",
      "                , weight: 62.94899999999768\n",
      "=================================\n",
      "in 2060 epoch, average loss: -41340.084375\n",
      "                , loss1: -663.333935546875\n",
      "                , loss2: 412.475830078125\n",
      "                , weight: 62.938999999997705\n",
      "=================================\n",
      "in 2070 epoch, average loss: -41346.7\n",
      "                , loss1: -663.694921875\n",
      "                , loss2: 421.947216796875\n",
      "                , weight: 62.92899999999773\n",
      "=================================\n",
      "in 2080 epoch, average loss: -41315.190625\n",
      "                , loss1: -663.095166015625\n",
      "                , loss2: 409.0788330078125\n",
      "                , weight: 62.91899999999775\n",
      "=================================\n",
      "in 2090 epoch, average loss: -41344.959375\n",
      "                , loss1: -663.90126953125\n",
      "                , loss2: 423.39296875\n",
      "                , weight: 62.908999999997775\n",
      "=================================\n",
      "in 2100 epoch, average loss: -41326.6625\n",
      "                , loss1: -663.81328125\n",
      "                , loss2: 429.5166015625\n",
      "                , weight: 62.8989999999978\n",
      "=================================\n",
      "in 2110 epoch, average loss: -41351.484375\n",
      "                , loss1: -664.11328125\n",
      "                , loss2: 416.9232421875\n",
      "                , weight: 62.88899999999782\n",
      "=================================\n",
      "in 2120 epoch, average loss: -41335.353125\n",
      "                , loss1: -663.771875\n",
      "                , loss2: 404.9446533203125\n",
      "                , weight: 62.878999999997845\n",
      "=================================\n",
      "in 2130 epoch, average loss: -41309.321875\n",
      "                , loss1: -663.7984375\n",
      "                , loss2: 426.011669921875\n",
      "                , weight: 62.86899999999787\n",
      "=================================\n",
      "in 2140 epoch, average loss: -41307.665625\n",
      "                , loss1: -663.82509765625\n",
      "                , loss2: 422.708203125\n",
      "                , weight: 62.85899999999789\n",
      "=================================\n",
      "in 2150 epoch, average loss: -41316.171875\n",
      "                , loss1: -664.1380859375\n",
      "                , loss2: 427.23037109375\n",
      "                , weight: 62.848999999997915\n",
      "=================================\n",
      "in 2160 epoch, average loss: -41308.30625\n",
      "                , loss1: -663.98515625\n",
      "                , loss2: 418.84990234375\n",
      "                , weight: 62.83899999999794\n",
      "=================================\n",
      "in 2170 epoch, average loss: -41301.68125\n",
      "                , loss1: -664.155517578125\n",
      "                , loss2: 429.537646484375\n",
      "                , weight: 62.82899999999796\n",
      "=================================\n",
      "in 2180 epoch, average loss: -41296.115625\n",
      "                , loss1: -663.9888671875\n",
      "                , loss2: 417.99033203125\n",
      "                , weight: 62.818999999997985\n",
      "=================================\n",
      "in 2190 epoch, average loss: -41277.753125\n",
      "                , loss1: -664.07568359375\n",
      "                , loss2: 435.1658203125\n",
      "                , weight: 62.80899999999801\n",
      "=================================\n",
      "in 2200 epoch, average loss: -41291.81875\n",
      "                , loss1: -664.1\n",
      "                , loss2: 415.986181640625\n",
      "                , weight: 62.79899999999803\n",
      "=================================\n",
      "in 2210 epoch, average loss: -41290.2125\n",
      "                , loss1: -664.189892578125\n",
      "                , loss2: 416.595654296875\n",
      "                , weight: 62.788999999998055\n",
      "=================================\n",
      "in 2220 epoch, average loss: -41281.278125\n",
      "                , loss1: -664.091748046875\n",
      "                , loss2: 412.730859375\n",
      "                , weight: 62.77899999999808\n",
      "=================================\n",
      "in 2230 epoch, average loss: -41248.2875\n",
      "                , loss1: -664.076171875\n",
      "                , loss2: 438.1017578125\n",
      "                , weight: 62.7689999999981\n",
      "=================================\n",
      "in 2240 epoch, average loss: -41147.765625\n",
      "                , loss1: -660.04453125\n",
      "                , loss2: 278.94013671875\n",
      "                , weight: 62.758999999998125\n",
      "=================================\n",
      "in 2250 epoch, average loss: -41439.490625\n",
      "                , loss1: -662.32177734375\n",
      "                , loss2: 123.51214599609375\n",
      "                , weight: 62.74899999999815\n",
      "=================================\n",
      "in 2260 epoch, average loss: -41443.81875\n",
      "                , loss1: -661.697998046875\n",
      "                , loss2: 73.42281494140624\n",
      "                , weight: 62.73899999999817\n",
      "=================================\n",
      "in 2270 epoch, average loss: -41526.125\n",
      "                , loss1: -663.79736328125\n",
      "                , loss2: 116.2032470703125\n",
      "                , weight: 62.728999999998194\n",
      "=================================\n",
      "in 2280 epoch, average loss: -41585.728125\n",
      "                , loss1: -664.10439453125\n",
      "                , loss2: 69.22255859375\n",
      "                , weight: 62.71899999999822\n",
      "=================================\n",
      "in 2290 epoch, average loss: -41581.803125\n",
      "                , loss1: -664.154833984375\n",
      "                , loss2: 69.67421264648438\n",
      "                , weight: 62.70899999999824\n",
      "=================================\n",
      "in 2300 epoch, average loss: -41598.490625\n",
      "                , loss1: -664.466748046875\n",
      "                , loss2: 65.90430908203125\n",
      "                , weight: 62.698999999998264\n",
      "=================================\n",
      "in 2310 epoch, average loss: -41596.05625\n",
      "                , loss1: -664.4412109375\n",
      "                , loss2: 60.08546142578125\n",
      "                , weight: 62.68899999999829\n",
      "=================================\n",
      "in 2320 epoch, average loss: -41600.26875\n",
      "                , loss1: -664.54423828125\n",
      "                , loss2: 55.68798828125\n",
      "                , weight: 62.67899999999831\n",
      "=================================\n",
      "in 2330 epoch, average loss: -41600.303125\n",
      "                , loss1: -664.635595703125\n",
      "                , loss2: 54.734613037109376\n",
      "                , weight: 62.668999999998334\n",
      "=================================\n",
      "in 2340 epoch, average loss: -41594.815625\n",
      "                , loss1: -664.647607421875\n",
      "                , loss2: 54.3296142578125\n",
      "                , weight: 62.65899999999836\n",
      "=================================\n",
      "in 2350 epoch, average loss: -41585.896875\n",
      "                , loss1: -664.586181640625\n",
      "                , loss2: 52.75440673828125\n",
      "                , weight: 62.64899999999838\n",
      "=================================\n",
      "in 2360 epoch, average loss: -41584.521875\n",
      "                , loss1: -664.6755859375\n",
      "                , loss2: 53.08387451171875\n",
      "                , weight: 62.638999999998404\n",
      "=================================\n",
      "in 2370 epoch, average loss: -41582.559375\n",
      "                , loss1: -664.741015625\n",
      "                , loss2: 52.5037841796875\n",
      "                , weight: 62.62899999999843\n",
      "=================================\n",
      "in 2380 epoch, average loss: -41575.875\n",
      "                , loss1: -664.71650390625\n",
      "                , loss2: 51.007571411132815\n",
      "                , weight: 62.61899999999845\n",
      "=================================\n",
      "in 2390 epoch, average loss: -41568.375\n",
      "                , loss1: -664.6951171875\n",
      "                , loss2: 50.50889587402344\n",
      "                , weight: 62.608999999998474\n",
      "=================================\n",
      "in 2400 epoch, average loss: -41564.9\n",
      "                , loss1: -664.8302734375\n",
      "                , loss2: 55.802520751953125\n",
      "                , weight: 62.5989999999985\n",
      "=================================\n",
      "in 2410 epoch, average loss: -41561.825\n",
      "                , loss1: -664.7474609375\n",
      "                , loss2: 47.04403991699219\n",
      "                , weight: 62.58899999999852\n",
      "=================================\n",
      "in 2420 epoch, average loss: -41556.09375\n",
      "                , loss1: -664.795654296875\n",
      "                , loss2: 49.14659423828125\n",
      "                , weight: 62.578999999998544\n",
      "=================================\n",
      "in 2430 epoch, average loss: -41555.5125\n",
      "                , loss1: -664.89052734375\n",
      "                , loss2: 49.010195922851565\n",
      "                , weight: 62.56899999999857\n",
      "=================================\n",
      "in 2440 epoch, average loss: -41546.753125\n",
      "                , loss1: -664.81650390625\n",
      "                , loss2: 46.48770446777344\n",
      "                , weight: 62.55899999999859\n",
      "=================================\n",
      "in 2450 epoch, average loss: -41537.940625\n",
      "                , loss1: -664.80302734375\n",
      "                , loss2: 47.813845825195315\n",
      "                , weight: 62.548999999998614\n",
      "=================================\n",
      "in 2460 epoch, average loss: -41535.63125\n",
      "                , loss1: -664.88115234375\n",
      "                , loss2: 48.36610717773438\n",
      "                , weight: 62.53899999999864\n",
      "=================================\n",
      "in 2470 epoch, average loss: -41535.1375\n",
      "                , loss1: -664.961767578125\n",
      "                , loss2: 47.243896484375\n",
      "                , weight: 62.52899999999866\n",
      "=================================\n",
      "in 2480 epoch, average loss: -41534.1625\n",
      "                , loss1: -665.045947265625\n",
      "                , loss2: 46.83289794921875\n",
      "                , weight: 62.518999999998684\n",
      "=================================\n",
      "in 2490 epoch, average loss: -41519.909375\n",
      "                , loss1: -664.91201171875\n",
      "                , loss2: 46.065982055664065\n",
      "                , weight: 62.50899999999871\n",
      "=================================\n",
      "in 2500 epoch, average loss: -41520.1875\n",
      "                , loss1: -665.02509765625\n",
      "                , loss2: 46.21405029296875\n",
      "                , weight: 62.49899999999873\n",
      "=================================\n",
      "in 2510 epoch, average loss: -41497.8375\n",
      "                , loss1: -664.85654296875\n",
      "                , loss2: 51.37662353515625\n",
      "                , weight: 62.488999999998754\n",
      "=================================\n",
      "in 2520 epoch, average loss: -41491.93125\n",
      "                , loss1: -664.9212890625\n",
      "                , loss2: 54.679681396484376\n",
      "                , weight: 62.47899999999878\n",
      "=================================\n",
      "in 2530 epoch, average loss: -41501.278125\n",
      "                , loss1: -665.13486328125\n",
      "                , loss2: 52.027105712890624\n",
      "                , weight: 62.4689999999988\n",
      "=================================\n",
      "in 2540 epoch, average loss: -41487.484375\n",
      "                , loss1: -665.0578125\n",
      "                , loss2: 54.35731201171875\n",
      "                , weight: 62.458999999998824\n",
      "=================================\n",
      "in 2550 epoch, average loss: -41477.075\n",
      "                , loss1: -664.964599609375\n",
      "                , loss2: 52.285345458984374\n",
      "                , weight: 62.44899999999885\n",
      "=================================\n",
      "in 2560 epoch, average loss: -41463.928125\n",
      "                , loss1: -665.06064453125\n",
      "                , loss2: 64.7900390625\n",
      "                , weight: 62.43899999999887\n",
      "=================================\n",
      "in 2570 epoch, average loss: -41465.775\n",
      "                , loss1: -665.230078125\n",
      "                , loss2: 66.86547241210937\n",
      "                , weight: 62.428999999998894\n",
      "=================================\n",
      "in 2580 epoch, average loss: -41455.6\n",
      "                , loss1: -665.2021484375\n",
      "                , loss2: 68.64674682617188\n",
      "                , weight: 62.41899999999892\n",
      "=================================\n",
      "in 2590 epoch, average loss: -41456.11875\n",
      "                , loss1: -665.26982421875\n",
      "                , loss2: 65.70087280273438\n",
      "                , weight: 62.40899999999894\n",
      "=================================\n",
      "in 2600 epoch, average loss: -41453.896875\n",
      "                , loss1: -665.253662109375\n",
      "                , loss2: 60.264794921875\n",
      "                , weight: 62.39899999999896\n",
      "=================================\n",
      "in 2610 epoch, average loss: -41443.0375\n",
      "                , loss1: -664.99296875\n",
      "                , loss2: 48.20257568359375\n",
      "                , weight: 62.38899999999899\n",
      "=================================\n",
      "in 2620 epoch, average loss: -41443.25625\n",
      "                , loss1: -665.095166015625\n",
      "                , loss2: 47.7091796875\n",
      "                , weight: 62.37899999999901\n",
      "=================================\n",
      "in 2630 epoch, average loss: -41446.003125\n",
      "                , loss1: -665.216064453125\n",
      "                , loss2: 45.848696899414065\n",
      "                , weight: 62.36899999999903\n",
      "=================================\n",
      "in 2640 epoch, average loss: -41426.034375\n",
      "                , loss1: -665.076416015625\n",
      "                , loss2: 50.461831665039064\n",
      "                , weight: 62.35899999999906\n",
      "=================================\n",
      "in 2650 epoch, average loss: -41435.29375\n",
      "                , loss1: -665.28154296875\n",
      "                , loss2: 47.339907836914065\n",
      "                , weight: 62.34899999999908\n",
      "=================================\n",
      "in 2660 epoch, average loss: -41426.284375\n",
      "                , loss1: -665.2228515625\n",
      "                , loss2: 46.03514099121094\n",
      "                , weight: 62.3389999999991\n",
      "=================================\n",
      "in 2670 epoch, average loss: -41423.759375\n",
      "                , loss1: -665.276953125\n",
      "                , loss2: 45.28341064453125\n",
      "                , weight: 62.32899999999913\n",
      "=================================\n",
      "in 2680 epoch, average loss: -41418.965625\n",
      "                , loss1: -665.315625\n",
      "                , loss2: 45.83160400390625\n",
      "                , weight: 62.31899999999915\n",
      "=================================\n",
      "in 2690 epoch, average loss: -41412.03125\n",
      "                , loss1: -665.294921875\n",
      "                , loss2: 44.821624755859375\n",
      "                , weight: 62.30899999999917\n",
      "=================================\n",
      "in 2700 epoch, average loss: -41402.64375\n",
      "                , loss1: -665.24599609375\n",
      "                , loss2: 44.5092529296875\n",
      "                , weight: 62.2989999999992\n",
      "=================================\n",
      "in 2710 epoch, average loss: -41397.15625\n",
      "                , loss1: -665.26708984375\n",
      "                , loss2: 44.66139526367188\n",
      "                , weight: 62.28899999999922\n",
      "=================================\n",
      "in 2720 epoch, average loss: -41392.903125\n",
      "                , loss1: -665.3205078125\n",
      "                , loss2: 45.58824157714844\n",
      "                , weight: 62.27899999999924\n",
      "=================================\n",
      "in 2730 epoch, average loss: -41388.40625\n",
      "                , loss1: -665.3421875\n",
      "                , loss2: 44.778436279296876\n",
      "                , weight: 62.26899999999927\n",
      "=================================\n",
      "in 2740 epoch, average loss: -41380.840625\n",
      "                , loss1: -665.32568359375\n",
      "                , loss2: 44.658929443359376\n",
      "                , weight: 62.25899999999929\n",
      "=================================\n",
      "in 2750 epoch, average loss: -41373.190625\n",
      "                , loss1: -665.31171875\n",
      "                , loss2: 44.79114685058594\n",
      "                , weight: 62.24899999999931\n",
      "=================================\n",
      "in 2760 epoch, average loss: -41370.940625\n",
      "                , loss1: -665.377783203125\n",
      "                , loss2: 44.50605773925781\n",
      "                , weight: 62.238999999999336\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0036\u001b[39m:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=3e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(20000):\n",
    "    if hgnn_trainer.weight > 0.0036:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.001\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([163., 182., 155., 164., 171., 184.], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02845927379784102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(torch.tensor([53.,56,53,54,56,55]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
