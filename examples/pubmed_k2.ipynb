{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行前请安装dhg: `pip install git+https://github.com/iMoonLab/DeepHypergraph.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycq/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import dhg\n",
    "from dhg import Hypergraph\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP\n",
    "from hgp.loss import loss_bs_matrix\n",
    "from hgp.utils import from_pickle_to_hypergraph\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 2\n",
    "\n",
    "\"\"\"\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.4}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.1}\n",
    "\"\"\"\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.25}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 128, \"out_channels\": 105, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 105, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 128, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 256, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 512, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers54\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":956, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":3824, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer21\"] = {\"in_channels\":956, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer34\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":956, \"out_channels\":2, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "        1.计算与顶点``vₙ``处于不同partition的顶点在超边``eₖ``上的数量``ne_k``.  \n",
    "        2.计算与顶点``vₙ``是否处于该超边``eₖ``上.  \n",
    "        3.若在,则说明``vₙ``所在的边为 **cut** , 记录该边的损失.  \n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "\n",
    "    #loss = 50 * loss_1 + loss_2\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 3824)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = from_pickle_to_hypergraph(\"../data/pubmed\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "X = torch.eye(n=G.num_v)\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "optim = optim.Adam(hgnn_trainer.parameters(), lr=3e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.layers.0.theta.weight Parameter containing:\n",
      "tensor([[-0.0060, -0.0104, -0.0046,  ..., -0.0094, -0.0057, -0.0024],\n",
      "        [ 0.0072,  0.0096,  0.0046,  ..., -0.0078, -0.0047,  0.0104],\n",
      "        [-0.0057,  0.0120,  0.0112,  ..., -0.0066,  0.0050, -0.0148],\n",
      "        ...,\n",
      "        [ 0.0075, -0.0036,  0.0147,  ...,  0.0120,  0.0119, -0.0023],\n",
      "        [-0.0062,  0.0061, -0.0021,  ..., -0.0094,  0.0152, -0.0124],\n",
      "        [-0.0060, -0.0071, -0.0025,  ...,  0.0083, -0.0013,  0.0051]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.0.theta.bias Parameter containing:\n",
      "tensor([ 0.0133,  0.0080, -0.0002,  ..., -0.0112, -0.0021, -0.0030],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.1.theta.weight Parameter containing:\n",
      "tensor([[-0.0209, -0.0173, -0.0123,  ...,  0.0158, -0.0184, -0.0166],\n",
      "        [ 0.0017, -0.0180,  0.0122,  ..., -0.0162,  0.0025,  0.0160],\n",
      "        [-0.0001,  0.0062,  0.0209,  ...,  0.0208, -0.0081, -0.0062],\n",
      "        ...,\n",
      "        [-0.0165, -0.0215, -0.0199,  ...,  0.0018,  0.0206,  0.0023],\n",
      "        [-0.0132,  0.0050, -0.0171,  ...,  0.0218, -0.0009, -0.0083],\n",
      "        [ 0.0126, -0.0066, -0.0055,  ...,  0.0162,  0.0170,  0.0088]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.1.theta.bias Parameter containing:\n",
      "tensor([ 0.0057,  0.0145, -0.0008,  ...,  0.0003,  0.0208, -0.0029],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.2.theta.weight Parameter containing:\n",
      "tensor([[-0.0034, -0.0269, -0.0102,  ..., -0.0234, -0.0151,  0.0018],\n",
      "        [-0.0207, -0.0016,  0.0063,  ...,  0.0070,  0.0269, -0.0073],\n",
      "        [-0.0168, -0.0299, -0.0142,  ..., -0.0235, -0.0223, -0.0194],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0281,  0.0257,  ...,  0.0145,  0.0014,  0.0118],\n",
      "        [-0.0170,  0.0064,  0.0135,  ..., -0.0066,  0.0161,  0.0300],\n",
      "        [ 0.0302, -0.0308, -0.0301,  ..., -0.0108,  0.0079,  0.0274]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.2.theta.bias Parameter containing:\n",
      "tensor([-0.0102, -0.0149,  0.0229,  ..., -0.0266, -0.0160, -0.0193],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.3.theta.weight Parameter containing:\n",
      "tensor([[ 0.0176,  0.0091, -0.0127,  ...,  0.0264,  0.0068, -0.0112],\n",
      "        [ 0.0146, -0.0128,  0.0167,  ..., -0.0028, -0.0194, -0.0166],\n",
      "        [ 0.0171, -0.0267,  0.0144,  ...,  0.0077,  0.0230,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0125,  0.0101,  ...,  0.0269,  0.0231, -0.0116],\n",
      "        [ 0.0208,  0.0133, -0.0245,  ..., -0.0199,  0.0124,  0.0242],\n",
      "        [-0.0173, -0.0193,  0.0023,  ...,  0.0192,  0.0058,  0.0289]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.3.theta.bias Parameter containing:\n",
      "tensor([-1.1458e-02, -1.4241e-02,  2.2317e-02,  1.3440e-02,  1.5417e-03,\n",
      "        -3.1170e-02, -2.0065e-02, -3.7748e-03,  4.1510e-03,  2.8883e-02,\n",
      "        -2.5341e-02, -2.8878e-02, -1.7410e-02, -1.4688e-02,  9.9517e-03,\n",
      "        -3.0574e-02, -3.2816e-03,  2.2091e-04,  2.0797e-02, -1.5272e-02,\n",
      "         2.1737e-02,  2.3970e-02,  1.5324e-02,  9.3401e-03, -1.8703e-03,\n",
      "        -1.5350e-02, -4.2147e-03, -2.7348e-02, -8.4820e-03,  4.3890e-03,\n",
      "        -2.4786e-03,  1.9107e-03,  2.1963e-02, -1.2188e-02, -7.8558e-03,\n",
      "         2.6663e-02, -2.2735e-03, -1.8624e-02, -1.4241e-02,  1.7391e-02,\n",
      "         1.2966e-02, -2.8931e-02,  2.7360e-02,  5.0903e-03,  2.6425e-02,\n",
      "        -3.0142e-02, -2.8605e-03, -2.9613e-02,  1.4554e-02, -2.6984e-02,\n",
      "         1.0987e-02,  2.0780e-02,  1.7913e-03,  1.6131e-02,  2.1869e-02,\n",
      "        -2.7969e-02,  1.6813e-02, -2.6133e-02, -1.6672e-02, -2.5758e-02,\n",
      "        -9.6787e-03,  2.9910e-02, -2.0605e-02,  2.2022e-02,  1.7897e-02,\n",
      "         4.1945e-03, -1.9698e-02,  1.0611e-02, -2.6223e-02,  3.0934e-02,\n",
      "         1.6534e-02, -2.9761e-02,  2.4839e-02, -6.5465e-03, -2.7648e-02,\n",
      "         1.1831e-02, -2.7641e-02, -2.0674e-02,  1.1417e-02, -2.1362e-04,\n",
      "         9.3728e-03,  1.6153e-02,  5.5121e-03, -4.1735e-03,  2.5222e-02,\n",
      "        -1.2549e-03, -7.3679e-03, -2.4787e-02,  6.8509e-03,  2.5682e-02,\n",
      "         1.6349e-02, -2.4697e-02,  1.6198e-02, -1.2061e-02, -2.0687e-02,\n",
      "        -2.2873e-02,  2.7201e-02,  1.7154e-02,  2.1198e-02,  6.9521e-04,\n",
      "         6.4106e-03, -5.6751e-03,  1.7329e-02, -2.7687e-02, -2.7948e-02,\n",
      "         1.1935e-02, -3.1040e-02, -4.9915e-03,  6.3269e-03, -1.9979e-02,\n",
      "        -2.4697e-02, -2.5020e-02, -1.1909e-02,  9.7686e-03,  3.0550e-02,\n",
      "         1.8913e-03,  6.0541e-03, -7.7135e-04,  1.2995e-02,  1.1484e-02,\n",
      "        -2.6276e-02,  2.7340e-02, -2.6988e-03,  2.3288e-02, -1.1549e-02,\n",
      "        -6.7674e-04,  2.4647e-02,  3.0349e-02,  3.7468e-03,  8.9838e-03,\n",
      "         4.6170e-03, -2.5206e-02, -3.0477e-02,  2.9383e-02,  3.8113e-03,\n",
      "        -8.9512e-03, -3.3474e-03,  3.2397e-03, -2.8912e-02,  2.1766e-02,\n",
      "        -3.4144e-04,  1.4379e-02, -1.0335e-02, -1.1319e-02, -1.7842e-02,\n",
      "        -2.0248e-02,  4.7790e-03, -1.7308e-02,  1.1874e-02,  8.9143e-03,\n",
      "         6.5243e-03, -2.6860e-02,  2.3871e-02, -3.0469e-02, -4.8643e-03,\n",
      "        -6.3214e-03, -2.2601e-02, -1.4156e-02, -1.3632e-02, -2.7900e-02,\n",
      "        -2.4772e-02,  2.7750e-02, -3.0948e-02, -2.8945e-02, -5.6161e-03,\n",
      "        -3.0424e-02, -2.0994e-02,  5.3273e-03,  2.1167e-02,  6.9609e-03,\n",
      "        -1.7002e-02,  1.8645e-02, -2.4583e-02, -1.4303e-02,  2.5339e-02,\n",
      "        -1.0669e-02, -1.0401e-02, -1.0485e-02, -4.0944e-03,  2.5129e-02,\n",
      "        -2.6763e-02,  2.4200e-03, -1.4455e-02,  1.5458e-02, -1.7780e-02,\n",
      "        -9.6816e-03, -1.8823e-02,  2.0940e-02,  2.0395e-02, -1.9176e-02,\n",
      "         1.4173e-02, -2.7705e-02,  1.1191e-02,  2.9256e-02,  3.0074e-02,\n",
      "         1.7195e-02, -1.9680e-03,  2.3060e-03, -1.1118e-03,  2.7306e-02,\n",
      "         1.7468e-02,  1.4638e-02,  5.7431e-03,  2.0062e-02, -2.6176e-02,\n",
      "         6.9253e-03,  5.8877e-03, -3.0505e-02,  2.4824e-02,  2.4277e-02,\n",
      "        -1.5090e-02,  4.2897e-03,  1.5134e-02,  1.6275e-02, -2.1532e-02,\n",
      "        -2.1736e-02,  1.9110e-02,  2.3519e-02,  2.6036e-02, -2.9509e-02,\n",
      "         3.8350e-03, -1.0984e-02,  2.8549e-03, -7.7222e-03, -1.8913e-02,\n",
      "        -8.2276e-03, -2.8238e-02, -1.9536e-02,  1.6898e-02, -4.8422e-03,\n",
      "         6.7385e-03,  1.5914e-02, -8.1227e-03, -1.5663e-02, -2.3957e-02,\n",
      "         1.6690e-02,  4.5503e-03,  2.7365e-02, -1.7328e-03, -2.6510e-02,\n",
      "         1.6903e-02, -2.0861e-02, -2.9507e-02,  2.3323e-02, -2.1237e-02,\n",
      "        -1.4003e-02, -2.2135e-02,  7.8699e-03, -4.3861e-03, -7.6774e-03,\n",
      "        -2.8775e-02,  2.5397e-02, -9.6936e-03,  1.9567e-02, -2.1085e-02,\n",
      "        -4.5825e-03,  1.1229e-02,  2.5747e-02,  2.9957e-02, -6.0231e-03,\n",
      "         1.5033e-02,  9.0942e-03,  1.1919e-02, -4.4857e-04,  2.1653e-02,\n",
      "         2.3639e-02,  4.2553e-03,  3.4302e-04,  1.6342e-02,  3.3956e-03,\n",
      "        -2.7162e-02, -1.0220e-02, -8.3023e-03,  3.3836e-03,  1.2602e-02,\n",
      "        -2.4072e-02,  2.6611e-02,  1.5334e-02, -1.7974e-02,  1.6246e-02,\n",
      "        -3.5714e-03, -2.6457e-02,  3.0075e-02,  2.2879e-04,  4.8959e-04,\n",
      "         6.8791e-03,  1.5742e-02,  2.7341e-02,  1.5100e-02,  3.9371e-03,\n",
      "         1.7919e-02,  2.0614e-02,  2.0134e-02,  1.4390e-02, -9.0318e-03,\n",
      "         5.6362e-03,  2.5239e-02, -1.2614e-02, -1.3171e-03, -1.8966e-03,\n",
      "         1.9791e-02,  2.2463e-02, -8.5596e-04, -1.5755e-02,  3.1021e-02,\n",
      "        -2.9870e-02,  1.7779e-02, -2.4372e-02, -2.5442e-02,  2.0786e-02,\n",
      "         9.0802e-03, -2.3928e-02,  2.7962e-02, -2.6052e-03,  9.9001e-03,\n",
      "         7.4824e-03,  3.1049e-02,  1.1804e-02, -2.1459e-02, -1.6192e-02,\n",
      "        -2.7440e-02, -6.2696e-03, -1.6745e-03,  2.1726e-02, -1.4430e-02,\n",
      "        -4.4568e-03, -4.4665e-03,  2.3425e-02, -2.7716e-02,  7.5261e-03,\n",
      "        -1.9322e-02,  2.5108e-02,  2.9299e-03, -2.8918e-02,  9.6092e-03,\n",
      "         1.4983e-02,  1.4762e-02, -1.0502e-02,  1.1684e-02, -2.6870e-02,\n",
      "        -9.3658e-03, -1.6366e-04,  6.5636e-03, -2.9605e-02,  2.8489e-03,\n",
      "        -1.5144e-02, -8.8982e-03, -8.5537e-03, -2.3999e-02,  1.2454e-02,\n",
      "        -4.3894e-03, -8.6454e-04, -1.5962e-02,  9.2085e-03, -3.1233e-02,\n",
      "        -2.5836e-03,  2.9667e-02,  1.5130e-02,  2.1000e-02, -4.9310e-03,\n",
      "         2.9750e-02,  9.3822e-03,  2.2788e-02,  4.2220e-03,  1.8395e-03,\n",
      "        -9.7316e-03,  1.8010e-02, -2.3575e-03,  1.7941e-02,  2.7183e-02,\n",
      "         5.9143e-03,  4.2992e-03, -1.8298e-02, -2.1400e-02, -3.3587e-03,\n",
      "         2.2390e-02,  2.4379e-02,  9.4310e-03, -1.4065e-02,  2.5793e-02,\n",
      "        -2.8900e-04, -2.0816e-02,  1.8682e-02, -5.0960e-04,  2.2145e-02,\n",
      "        -6.8087e-03,  1.1862e-02, -4.0241e-03,  2.1937e-02,  1.8786e-02,\n",
      "         2.4952e-02, -1.3941e-02, -2.9530e-02, -2.1990e-02, -2.0664e-02,\n",
      "         2.7350e-02, -2.7974e-03, -2.8156e-02,  1.3740e-02, -1.7332e-02,\n",
      "        -2.6967e-02, -5.5988e-03, -1.7181e-02, -2.1747e-02, -2.4006e-02,\n",
      "        -2.7931e-02,  2.8056e-03,  1.7326e-03,  3.0929e-02, -1.0027e-02,\n",
      "         2.9781e-02,  8.7579e-03,  1.1291e-02,  1.1121e-02, -1.2174e-02,\n",
      "         2.8579e-03, -2.1712e-02, -2.9341e-02, -2.5384e-02,  1.3305e-02,\n",
      "         5.6470e-03,  3.1007e-02, -1.5908e-02, -2.7434e-03,  2.7932e-02,\n",
      "        -2.5685e-02,  1.7612e-02, -2.8393e-02,  1.7124e-02, -2.9967e-02,\n",
      "        -4.6488e-03,  3.4156e-03, -6.1755e-04,  2.0740e-02, -7.8365e-03,\n",
      "        -1.9875e-02,  2.8194e-02,  1.5768e-02,  2.8784e-02, -9.7841e-04,\n",
      "         1.1955e-03,  3.0607e-02,  2.7443e-02,  1.5288e-02, -2.9473e-02,\n",
      "        -2.1157e-02, -6.5512e-03, -4.0340e-03, -1.6556e-02,  9.9522e-04,\n",
      "        -1.4758e-02,  1.6328e-02, -2.0642e-02, -2.6017e-02,  2.6850e-02,\n",
      "         1.3676e-02, -1.2368e-03, -1.1931e-02,  2.3868e-02,  7.2879e-03,\n",
      "         1.9053e-02,  2.4874e-02, -2.7919e-02, -2.2570e-02, -1.1434e-02,\n",
      "        -2.3622e-02,  3.0191e-02, -6.4354e-03,  3.1084e-02, -1.3062e-02,\n",
      "         3.0921e-02, -1.2765e-02, -7.7519e-03,  1.7757e-02, -8.9191e-03,\n",
      "         1.5419e-02, -6.4536e-03, -8.7647e-03,  7.5637e-03, -1.8785e-02,\n",
      "        -1.4656e-02,  1.7555e-02,  7.3371e-03,  2.6789e-02,  2.9768e-03,\n",
      "        -1.7001e-02, -1.3130e-02, -2.8160e-02,  9.3133e-03, -2.7401e-02,\n",
      "         5.8818e-03,  5.8880e-03, -5.5559e-05, -9.9376e-03, -9.0906e-03,\n",
      "         2.1462e-02,  7.2309e-03,  1.3913e-02, -2.8200e-02,  3.5068e-03,\n",
      "         1.0866e-03, -2.7067e-02, -1.1943e-02,  1.1742e-03, -1.7967e-02,\n",
      "         8.0520e-03,  2.3419e-02,  1.0883e-02, -6.2250e-03,  1.9022e-02,\n",
      "         2.5614e-02, -1.5155e-03], device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.4.theta.weight Parameter containing:\n",
      "tensor([[-0.0167, -0.0136,  0.0111,  ..., -0.0048, -0.0109,  0.0094],\n",
      "        [-0.0071, -0.0283,  0.0385,  ...,  0.0256, -0.0168,  0.0047],\n",
      "        [ 0.0217, -0.0062,  0.0180,  ..., -0.0152, -0.0335, -0.0148],\n",
      "        ...,\n",
      "        [ 0.0390, -0.0198, -0.0287,  ...,  0.0098, -0.0187, -0.0017],\n",
      "        [ 0.0298, -0.0290, -0.0382,  ...,  0.0440,  0.0346, -0.0315],\n",
      "        [ 0.0227,  0.0326, -0.0329,  ..., -0.0329, -0.0135, -0.0098]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.4.theta.bias Parameter containing:\n",
      "tensor([-1.8899e-02, -4.5523e-03,  3.6538e-02,  1.0058e-02,  4.6906e-03,\n",
      "        -3.6231e-02, -1.4537e-02,  3.3146e-02, -1.9182e-02, -2.1168e-02,\n",
      "        -3.9131e-02, -4.0616e-02,  2.2576e-02, -1.0447e-02, -4.4698e-04,\n",
      "         3.2345e-02,  2.5799e-02, -3.9777e-02,  2.3270e-02, -4.3400e-02,\n",
      "        -2.2946e-02, -2.3740e-02, -1.6095e-02, -4.2884e-02,  1.1140e-02,\n",
      "         1.0667e-02,  2.8735e-02,  3.3947e-02,  3.6032e-02,  1.3317e-02,\n",
      "        -1.8834e-02, -3.1887e-02,  7.1018e-03,  4.4059e-02,  2.6299e-02,\n",
      "        -3.3132e-03, -3.7503e-02,  3.6971e-02, -1.4087e-02, -3.2535e-02,\n",
      "        -1.2584e-02,  1.2686e-02, -2.2386e-03, -9.6444e-04, -5.2459e-03,\n",
      "        -2.8404e-02,  3.8200e-02, -4.0599e-02, -3.2722e-02,  3.0637e-02,\n",
      "         9.5097e-03, -2.4103e-02, -2.9383e-02,  4.9240e-03, -4.0213e-02,\n",
      "        -2.8555e-02,  2.4639e-02,  2.5878e-02, -9.7287e-03,  3.8013e-02,\n",
      "         2.8743e-02, -1.1214e-02,  2.2479e-02,  1.1048e-03, -2.7727e-02,\n",
      "         1.4492e-02,  3.3563e-02,  1.0649e-02,  1.0301e-02, -4.2470e-02,\n",
      "         3.2558e-02, -1.2865e-02,  5.6715e-03, -1.2562e-03, -3.2243e-02,\n",
      "        -2.4103e-02, -2.4635e-02, -3.6596e-02, -5.7317e-03,  4.0066e-03,\n",
      "         2.3441e-02, -1.9777e-02,  3.8453e-02, -3.6606e-02, -5.8552e-03,\n",
      "         2.7743e-02, -3.0149e-02, -1.1857e-02, -4.2579e-02,  2.2996e-02,\n",
      "        -1.5429e-02,  2.9617e-02,  3.8876e-02,  1.7686e-02, -7.4020e-03,\n",
      "         2.0867e-02,  6.6641e-03,  1.5598e-02,  3.5413e-02,  3.9929e-02,\n",
      "         1.7118e-02, -7.6996e-03, -2.5636e-02, -3.1498e-02, -2.9911e-03,\n",
      "         8.5387e-04, -2.6447e-02,  3.5216e-02, -3.7487e-02, -1.1911e-02,\n",
      "        -2.0861e-02,  5.0413e-03, -3.8038e-02, -4.0048e-02, -3.8556e-02,\n",
      "        -3.5333e-02,  2.0940e-02,  3.1805e-02, -2.0782e-02,  4.3133e-02,\n",
      "        -3.1785e-02,  2.8631e-02,  3.6397e-02, -1.0428e-02,  3.8116e-02,\n",
      "         2.6531e-02, -1.7996e-02,  3.5320e-02, -2.3991e-02,  2.0018e-02,\n",
      "         3.4361e-02, -4.7207e-03,  3.9481e-02, -2.5191e-02,  8.2820e-03,\n",
      "        -1.5870e-02,  9.9953e-03, -5.2942e-05, -3.0569e-02,  1.0984e-02,\n",
      "        -5.2824e-03, -2.2710e-02, -3.4944e-03,  1.4711e-02, -3.9700e-02,\n",
      "        -4.0119e-02, -2.3039e-02,  7.4966e-03, -1.1803e-02,  1.4135e-02,\n",
      "        -3.6809e-02, -1.8617e-02, -3.5665e-02, -5.8370e-03, -2.3935e-02,\n",
      "        -6.8991e-04, -2.6790e-02, -1.6212e-02,  1.9940e-02, -3.6005e-02,\n",
      "         1.5425e-02,  3.2280e-03,  3.4487e-02, -3.9247e-02, -3.3740e-02,\n",
      "         2.0713e-02,  1.0120e-02,  8.7699e-03, -1.4085e-02,  2.4072e-02,\n",
      "        -2.7706e-02, -3.7825e-02,  1.8034e-02, -3.5723e-02, -7.8398e-03,\n",
      "         2.6770e-02,  3.2275e-03,  3.2743e-02,  3.7619e-02,  6.5700e-03,\n",
      "        -3.6114e-03,  1.7329e-02,  2.6490e-02, -2.5323e-02,  6.5512e-03,\n",
      "         2.7767e-02, -1.7166e-02,  4.2275e-02,  2.6561e-02,  9.0444e-03,\n",
      "         2.2236e-02, -3.0729e-02, -2.7545e-02, -1.6151e-02, -9.7815e-03,\n",
      "        -6.9516e-03,  3.6952e-02,  2.7035e-02,  2.3881e-02, -1.5346e-02,\n",
      "        -2.9517e-02, -4.0365e-02, -6.2265e-03, -1.4488e-02, -4.3354e-02,\n",
      "        -8.7948e-03, -1.5472e-02, -4.0745e-03,  3.5549e-02, -1.2873e-02,\n",
      "         3.9775e-02,  1.2963e-02,  3.1036e-03, -2.3237e-03, -4.3516e-02,\n",
      "        -6.8418e-04, -2.0257e-02, -3.2512e-02,  1.2815e-02,  2.8950e-02,\n",
      "        -3.0875e-02,  3.4677e-02,  3.8670e-02,  3.2420e-02,  1.0743e-02,\n",
      "         1.6043e-02,  3.8024e-02,  2.9038e-02,  3.5870e-02, -3.0751e-02,\n",
      "        -4.2886e-03, -1.0067e-02,  1.1052e-02, -1.0223e-02, -1.8955e-02,\n",
      "        -8.8132e-03, -4.4936e-03,  4.1700e-02, -4.2610e-02,  1.8594e-02,\n",
      "         1.1021e-02, -2.4684e-02,  1.8000e-02,  2.1790e-02, -3.0300e-02,\n",
      "         1.9374e-02, -6.6049e-04, -2.8990e-02, -7.6901e-03, -3.7272e-02,\n",
      "        -2.2073e-02, -7.3959e-03, -1.3456e-02, -4.1887e-02,  3.3662e-02,\n",
      "        -4.2047e-02], device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.5.theta.weight Parameter containing:\n",
      "tensor([[-0.0295,  0.0386,  0.0335,  ...,  0.0084,  0.0594, -0.0075],\n",
      "        [-0.0553, -0.0144,  0.0300,  ...,  0.0080,  0.0372,  0.0516],\n",
      "        [ 0.0543,  0.0617, -0.0563,  ...,  0.0267,  0.0382, -0.0343],\n",
      "        ...,\n",
      "        [-0.0361, -0.0110, -0.0524,  ..., -0.0502, -0.0518,  0.0449],\n",
      "        [ 0.0171, -0.0306, -0.0521,  ..., -0.0362, -0.0331, -0.0324],\n",
      "        [ 0.0618, -0.0568, -0.0042,  ..., -0.0124,  0.0609,  0.0525]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.5.theta.bias Parameter containing:\n",
      "tensor([ 0.0600,  0.0407, -0.0109, -0.0083,  0.0541,  0.0587, -0.0250,  0.0119,\n",
      "        -0.0617, -0.0369,  0.0125, -0.0052,  0.0350, -0.0425, -0.0523, -0.0470,\n",
      "         0.0033, -0.0202, -0.0599,  0.0524, -0.0361,  0.0408, -0.0133,  0.0375,\n",
      "         0.0430,  0.0268, -0.0448, -0.0171, -0.0539,  0.0516, -0.0030,  0.0066,\n",
      "        -0.0562, -0.0041,  0.0254, -0.0352, -0.0258, -0.0608,  0.0005, -0.0518,\n",
      "        -0.0592,  0.0263, -0.0066,  0.0145, -0.0517, -0.0520,  0.0095, -0.0122,\n",
      "         0.0551, -0.0235,  0.0186,  0.0042,  0.0194, -0.0307, -0.0296,  0.0462,\n",
      "         0.0057,  0.0164, -0.0264,  0.0521, -0.0225, -0.0111,  0.0431,  0.0298,\n",
      "         0.0200, -0.0087,  0.0086,  0.0225, -0.0058,  0.0079, -0.0407,  0.0126,\n",
      "         0.0309, -0.0184,  0.0052,  0.0238,  0.0404, -0.0042,  0.0119, -0.0599,\n",
      "         0.0460,  0.0063,  0.0353, -0.0422,  0.0579,  0.0223,  0.0470, -0.0480,\n",
      "         0.0363, -0.0102, -0.0139, -0.0103,  0.0478,  0.0135,  0.0212,  0.0137,\n",
      "         0.0327,  0.0237, -0.0578, -0.0105,  0.0333,  0.0234,  0.0278, -0.0210,\n",
      "         0.0606,  0.0487,  0.0462, -0.0619, -0.0146, -0.0146, -0.0320,  0.0072,\n",
      "         0.0015,  0.0525,  0.0172, -0.0333, -0.0014, -0.0401,  0.0474,  0.0190,\n",
      "         0.0622, -0.0495,  0.0370, -0.0105, -0.0220,  0.0546,  0.0409, -0.0157],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.6.theta.weight Parameter containing:\n",
      "tensor([[-0.0749, -0.0540, -0.0150,  ..., -0.0589, -0.0110,  0.0547],\n",
      "        [ 0.0553, -0.0623,  0.0338,  ..., -0.0318,  0.0655, -0.0276],\n",
      "        [-0.0763, -0.0531,  0.0161,  ..., -0.0731,  0.0862, -0.0010],\n",
      "        ...,\n",
      "        [ 0.0708,  0.0183,  0.0442,  ..., -0.0476,  0.0554, -0.0884],\n",
      "        [-0.0036,  0.0364, -0.0554,  ..., -0.0169, -0.0561,  0.0654],\n",
      "        [ 0.0394, -0.0867,  0.0308,  ...,  0.0157, -0.0130, -0.0468]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.6.theta.bias Parameter containing:\n",
      "tensor([ 0.0689,  0.0362,  0.0743,  0.0697, -0.0842,  0.0584,  0.0332, -0.0636,\n",
      "         0.0208, -0.0575, -0.0788, -0.0552, -0.0249,  0.0236, -0.0272, -0.0092,\n",
      "         0.0201, -0.0125,  0.0074, -0.0172,  0.0486,  0.0213, -0.0018,  0.0369,\n",
      "         0.0757, -0.0027, -0.0800,  0.0255, -0.0166,  0.0293,  0.0138, -0.0390,\n",
      "        -0.0710, -0.0245, -0.0466, -0.0049, -0.0477, -0.0157,  0.0745,  0.0142,\n",
      "        -0.0164,  0.0219, -0.0372,  0.0416, -0.0406, -0.0520,  0.0734,  0.0792,\n",
      "        -0.0213, -0.0544,  0.0241, -0.0842, -0.0133,  0.0343,  0.0635,  0.0585,\n",
      "         0.0854,  0.0725,  0.0138,  0.0501,  0.0828,  0.0399, -0.0603,  0.0171,\n",
      "         0.0810, -0.0495, -0.0690,  0.0197, -0.0513, -0.0297, -0.0848, -0.0780,\n",
      "        -0.0547,  0.0296,  0.0417, -0.0540,  0.0057,  0.0792,  0.0867,  0.0122,\n",
      "         0.0874,  0.0116,  0.0252, -0.0133,  0.0771, -0.0264,  0.0012, -0.0335,\n",
      "        -0.0201,  0.0211, -0.0005, -0.0781, -0.0511, -0.0559,  0.0638, -0.0701,\n",
      "        -0.0171,  0.0242,  0.0731, -0.0779,  0.0438,  0.0629,  0.0642,  0.0860,\n",
      "        -0.0551], device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.7.theta.weight Parameter containing:\n",
      "tensor([[ 0.0928,  0.0020, -0.0369,  ..., -0.0152, -0.0716, -0.0086],\n",
      "        [-0.0705,  0.0749, -0.0279,  ..., -0.0133,  0.0789, -0.0047],\n",
      "        [-0.0922,  0.0410, -0.0077,  ...,  0.0512,  0.0790, -0.0635],\n",
      "        ...,\n",
      "        [-0.0831, -0.0323, -0.0469,  ...,  0.0344, -0.0019,  0.0299],\n",
      "        [ 0.0867,  0.0908, -0.0754,  ..., -0.0178, -0.0313, -0.0213],\n",
      "        [ 0.0344, -0.0394, -0.0084,  ..., -0.0474, -0.0397, -0.0326]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.7.theta.bias Parameter containing:\n",
      "tensor([ 0.0131,  0.0500, -0.0147, -0.0189,  0.0182, -0.0476,  0.0484,  0.0042,\n",
      "         0.0714, -0.0180, -0.0950,  0.0165,  0.0827,  0.0620, -0.0399,  0.0284,\n",
      "         0.0661, -0.0588,  0.0605,  0.0574,  0.0811,  0.0710,  0.0555, -0.0556,\n",
      "        -0.0072,  0.0905, -0.0056,  0.0112, -0.0906,  0.0747,  0.0007,  0.0196,\n",
      "        -0.0859,  0.0851,  0.0822,  0.0297, -0.0681,  0.0072,  0.0429, -0.0576,\n",
      "         0.0100, -0.0403,  0.0538, -0.0678,  0.0880,  0.0535, -0.0598, -0.0410,\n",
      "         0.0746,  0.0316,  0.0918,  0.0418,  0.0265, -0.0571, -0.0111,  0.0583,\n",
      "        -0.0721, -0.0956, -0.0794,  0.0410,  0.0190, -0.0673,  0.0094, -0.0595,\n",
      "        -0.0210, -0.0562, -0.0956, -0.0255, -0.0729,  0.0460,  0.0358,  0.0851,\n",
      "        -0.0365, -0.0223, -0.0223,  0.0467,  0.0466, -0.0242,  0.0095, -0.0657,\n",
      "         0.0835,  0.0726,  0.0117, -0.0440,  0.0913, -0.0365, -0.0824, -0.0806,\n",
      "         0.0383, -0.0946, -0.0912, -0.0832, -0.0543, -0.0028,  0.0972, -0.0191,\n",
      "         0.0574,  0.0887, -0.0834, -0.0172, -0.0104, -0.0815,  0.0795,  0.0326,\n",
      "         0.0068,  0.0003, -0.0954, -0.0761, -0.0336, -0.0381, -0.0185,  0.0382,\n",
      "        -0.0928, -0.0463,  0.0652, -0.0649, -0.0940,  0.0680,  0.0093, -0.0922,\n",
      "        -0.0612, -0.0808,  0.0306, -0.0674,  0.0052,  0.0078,  0.0836, -0.0338],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.8.theta.weight Parameter containing:\n",
      "tensor([[-0.0396, -0.0037, -0.0070,  ...,  0.0526,  0.0771,  0.0494],\n",
      "        [ 0.0527, -0.0407, -0.0363,  ...,  0.0852, -0.0106, -0.0762],\n",
      "        [-0.0216,  0.0732, -0.0448,  ..., -0.0651,  0.0863,  0.0599],\n",
      "        ...,\n",
      "        [-0.0559, -0.0154, -0.0450,  ...,  0.0828, -0.0102, -0.0661],\n",
      "        [ 0.0676, -0.0522,  0.0466,  ...,  0.0661, -0.0121, -0.0255],\n",
      "        [-0.0422, -0.0467,  0.0256,  ...,  0.0721, -0.0525, -0.0019]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.8.theta.bias Parameter containing:\n",
      "tensor([ 0.0520,  0.0220, -0.0780,  0.0096, -0.0697, -0.0398, -0.0429,  0.0651,\n",
      "        -0.0004, -0.0223,  0.0884,  0.0026, -0.0806, -0.0280, -0.0658,  0.0212,\n",
      "        -0.0613, -0.0581, -0.0337, -0.0736,  0.0050,  0.0091,  0.0780,  0.0632,\n",
      "        -0.0561,  0.0070, -0.0482,  0.0521, -0.0226,  0.0843, -0.0713, -0.0428,\n",
      "        -0.0697, -0.0231, -0.0413,  0.0802,  0.0042, -0.0394,  0.0373, -0.0382,\n",
      "         0.0711,  0.0186,  0.0157, -0.0228, -0.0065,  0.0186,  0.0645,  0.0871,\n",
      "         0.0239,  0.0579,  0.0069, -0.0299, -0.0478,  0.0116, -0.0261, -0.0845,\n",
      "        -0.0469,  0.0266, -0.0646, -0.0304, -0.0627,  0.0314, -0.0879, -0.0381,\n",
      "        -0.0093,  0.0499,  0.0692, -0.0039, -0.0232,  0.0300, -0.0231,  0.0685,\n",
      "         0.0607,  0.0155,  0.0510, -0.0618, -0.0814,  0.0046, -0.0291,  0.0484,\n",
      "         0.0670, -0.0369,  0.0201, -0.0439,  0.0432,  0.0398,  0.0391, -0.0217,\n",
      "        -0.0227,  0.0611, -0.0193, -0.0369, -0.0713,  0.0491, -0.0580,  0.0809,\n",
      "         0.0883,  0.0614, -0.0720,  0.0205, -0.0373,  0.0031,  0.0136, -0.0095,\n",
      "        -0.0814,  0.0553,  0.0253,  0.0680,  0.0783,  0.0134, -0.0534,  0.0250,\n",
      "        -0.0120, -0.0770, -0.0719, -0.0184,  0.0681, -0.0147, -0.0695, -0.0666,\n",
      "         0.0683, -0.0338,  0.0396, -0.0314, -0.0058,  0.0745, -0.0236,  0.0227,\n",
      "         0.0590,  0.0012, -0.0846, -0.0663, -0.0077, -0.0393,  0.0770,  0.0532,\n",
      "        -0.0626,  0.0516, -0.0859, -0.0437, -0.0489,  0.0205,  0.0614, -0.0547,\n",
      "         0.0076,  0.0800, -0.0314,  0.0229,  0.0504, -0.0219,  0.0505,  0.0306,\n",
      "        -0.0238,  0.0531, -0.0201,  0.0646, -0.0141, -0.0400, -0.0400,  0.0435,\n",
      "         0.0420, -0.0580, -0.0470, -0.0804,  0.0170,  0.0338,  0.0086, -0.0623,\n",
      "        -0.0568,  0.0268, -0.0504,  0.0055, -0.0852, -0.0473, -0.0874,  0.0436,\n",
      "        -0.0253, -0.0658,  0.0194,  0.0045,  0.0835, -0.0486, -0.0244, -0.0742,\n",
      "        -0.0208, -0.0384, -0.0676,  0.0727,  0.0522,  0.0384,  0.0309, -0.0614,\n",
      "         0.0547,  0.0114, -0.0663, -0.0719, -0.0135,  0.0068,  0.0120, -0.0300,\n",
      "         0.0565, -0.0238, -0.0070,  0.0786, -0.0239,  0.0733,  0.0013, -0.0352,\n",
      "        -0.0880,  0.0556,  0.0488,  0.0608, -0.0105,  0.0049, -0.0228, -0.0698,\n",
      "        -0.0047, -0.0126, -0.0182, -0.0462, -0.0876, -0.0785,  0.0266, -0.0657,\n",
      "         0.0448, -0.0758, -0.0768,  0.0335, -0.0593,  0.0208,  0.0781, -0.0534,\n",
      "        -0.0461,  0.0489, -0.0174, -0.0825, -0.0520, -0.0300, -0.0689, -0.0493,\n",
      "         0.0587,  0.0224,  0.0111, -0.0424, -0.0055, -0.0262,  0.0712, -0.0611,\n",
      "        -0.0190,  0.0296, -0.0786,  0.0853,  0.0160, -0.0444,  0.0295, -0.0312],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.9.theta.weight Parameter containing:\n",
      "tensor([[-0.0553, -0.0354,  0.0500,  ...,  0.0149,  0.0453,  0.0437],\n",
      "        [-0.0501,  0.0054, -0.0569,  ..., -0.0625,  0.0609, -0.0614],\n",
      "        [ 0.0314,  0.0016, -0.0365,  ..., -0.0391, -0.0548, -0.0606],\n",
      "        ...,\n",
      "        [-0.0336, -0.0519,  0.0407,  ..., -0.0376,  0.0261,  0.0336],\n",
      "        [ 0.0376, -0.0350,  0.0617,  ...,  0.0419, -0.0114, -0.0576],\n",
      "        [ 0.0316,  0.0023,  0.0615,  ...,  0.0261, -0.0616,  0.0170]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.9.theta.bias Parameter containing:\n",
      "tensor([ 2.4167e-02,  5.9429e-02,  2.1422e-02, -3.4385e-02, -1.7956e-02,\n",
      "        -7.1291e-03,  7.8388e-03,  3.4449e-02, -6.7648e-03,  2.4664e-02,\n",
      "        -1.2310e-02,  4.0094e-02,  4.1003e-02,  2.4691e-02, -2.1266e-02,\n",
      "        -4.0939e-02,  2.1809e-03, -5.8669e-02, -7.2094e-03,  4.4465e-02,\n",
      "        -3.8528e-02,  6.1520e-02, -5.3268e-02, -4.7026e-02, -1.4696e-03,\n",
      "         3.1900e-02,  2.5967e-02, -5.5294e-02, -4.1159e-03,  3.7832e-02,\n",
      "        -2.3451e-03, -5.2957e-02, -5.8480e-05, -3.6905e-02, -5.1253e-02,\n",
      "         3.1628e-02,  4.7333e-02,  5.4226e-02,  3.7553e-03,  3.0626e-02,\n",
      "         4.2820e-02, -2.1010e-02, -1.6510e-02,  3.5425e-03, -6.1143e-02,\n",
      "         4.6365e-02,  6.2337e-02,  8.0515e-03, -2.3486e-02,  1.3331e-02,\n",
      "        -4.6455e-02,  6.1318e-02, -1.4196e-02, -2.5928e-02, -4.8328e-02,\n",
      "        -3.0509e-02,  1.3332e-02, -1.7929e-02,  2.3473e-02, -6.0013e-02,\n",
      "        -4.0693e-02, -5.6732e-02, -6.1280e-02, -1.1937e-02, -5.5913e-02,\n",
      "         8.9160e-03,  5.5939e-02, -2.4710e-02,  5.4698e-02, -4.1371e-02,\n",
      "         5.2951e-02, -1.5590e-03, -6.9725e-03,  4.0610e-02,  2.8242e-02,\n",
      "         1.5666e-02,  5.3475e-02, -3.5919e-02,  1.0655e-02,  4.9336e-02,\n",
      "         3.0016e-02,  1.7782e-02,  1.3743e-02, -1.1565e-02, -4.8303e-02,\n",
      "         2.4955e-03,  2.6215e-02, -4.2114e-03, -1.8876e-02,  3.8331e-02,\n",
      "        -4.0403e-03,  1.6283e-02, -5.0021e-02, -1.1981e-02,  5.0830e-02,\n",
      "         1.5669e-02,  5.7491e-02,  2.8443e-02,  3.7431e-02,  2.5540e-03,\n",
      "         4.9724e-02, -5.2011e-02, -1.7549e-03,  3.6864e-02,  2.2795e-02,\n",
      "         4.6567e-02,  1.8697e-02,  5.9632e-02, -3.6865e-02,  1.2288e-02,\n",
      "        -1.4358e-02,  5.7949e-02,  3.8889e-02, -1.0387e-02,  1.2539e-02,\n",
      "         3.5145e-02, -5.5361e-03,  4.7966e-02,  4.2130e-02, -2.8874e-02,\n",
      "         2.9731e-02,  4.2591e-02, -3.7498e-02, -3.9667e-02, -5.4028e-02,\n",
      "         2.1147e-02, -2.9546e-02, -3.1889e-02,  5.3495e-02, -2.0480e-02,\n",
      "         3.0576e-02,  1.9053e-02,  9.1853e-03, -2.2445e-02, -8.3795e-03,\n",
      "        -6.2390e-03,  5.0125e-02,  1.5358e-02, -3.9387e-02, -4.5663e-02,\n",
      "         1.9291e-02,  3.7244e-04,  3.8157e-02, -3.0622e-03,  4.1820e-02,\n",
      "        -5.6741e-02, -5.3656e-02, -2.8385e-03, -4.8795e-02, -6.0755e-02,\n",
      "        -2.9580e-02, -2.4950e-02, -1.9329e-02, -9.3091e-04, -3.2242e-02,\n",
      "        -1.5374e-02,  4.7507e-02,  1.0615e-02, -4.7838e-02, -3.8467e-02,\n",
      "         5.2451e-02,  4.5847e-02, -4.5636e-03,  1.2776e-02,  3.0278e-03,\n",
      "         8.4177e-03, -8.4665e-03,  4.5455e-02, -5.1876e-02, -3.4553e-02,\n",
      "        -2.2814e-02,  5.1531e-03, -3.4292e-02,  6.1643e-02, -5.0957e-02,\n",
      "         3.3052e-02, -4.1732e-03,  6.0940e-02,  5.7678e-02,  2.3266e-02,\n",
      "        -4.6977e-02,  3.9805e-02,  5.4673e-03, -1.7296e-02, -5.0343e-02,\n",
      "         2.0070e-02, -3.5435e-03, -4.2321e-02,  4.0624e-02, -1.4721e-02,\n",
      "        -1.2629e-02,  2.0142e-02, -1.1182e-02, -2.6629e-03,  5.3516e-02,\n",
      "         1.9395e-02, -4.3046e-02,  2.7982e-02,  4.0745e-03, -4.0208e-02,\n",
      "         5.1267e-02,  1.5300e-02,  2.1514e-02, -3.1558e-02, -5.5944e-02,\n",
      "        -2.2405e-02,  3.5556e-02, -5.4584e-02, -3.5580e-02, -2.5606e-02,\n",
      "         7.9670e-03,  1.8470e-02, -2.0782e-02, -7.8446e-03, -5.5311e-02,\n",
      "         4.3540e-02,  2.8256e-02,  2.4699e-02, -2.4638e-02, -2.8670e-02,\n",
      "        -3.8731e-02, -2.1059e-03, -7.6627e-03,  8.9193e-04, -6.8290e-04,\n",
      "        -5.0874e-02,  3.2226e-02, -2.3009e-02, -4.0890e-02, -4.8742e-02,\n",
      "        -5.1836e-02, -2.0082e-02, -4.0364e-02, -2.2278e-02,  4.1281e-02,\n",
      "        -2.1909e-02, -4.4986e-02,  3.1100e-02,  6.2779e-03,  7.6254e-03,\n",
      "        -4.4393e-02, -1.1071e-02, -2.1432e-02, -3.3020e-02,  5.2533e-02,\n",
      "         1.7789e-03,  2.7140e-02, -1.1815e-02,  5.3029e-02, -4.1660e-02,\n",
      "         5.0586e-02, -4.7824e-02, -1.0195e-02, -4.3447e-02,  2.4109e-02,\n",
      "        -5.7402e-02, -4.1975e-02, -3.6914e-02, -4.2831e-02,  6.2258e-02,\n",
      "        -4.1771e-02, -5.9822e-02, -5.8916e-03,  5.3645e-02,  2.0595e-02,\n",
      "        -3.8503e-02,  1.9427e-02, -2.4760e-02, -1.7109e-03,  3.9397e-02,\n",
      "         2.8713e-02, -2.2940e-02, -6.1049e-02,  5.3394e-02, -1.7960e-03,\n",
      "        -4.3358e-02,  4.8216e-02,  2.9554e-02,  4.5547e-03,  5.2386e-02,\n",
      "         4.2714e-03,  4.3685e-02,  5.8264e-02, -1.2398e-02, -5.1866e-02,\n",
      "         3.3146e-02,  5.4324e-02,  1.1034e-02,  1.7040e-02, -4.8760e-02,\n",
      "         2.2959e-02, -4.4157e-02,  3.3522e-02, -5.6216e-02,  1.8557e-02,\n",
      "        -1.1009e-02,  1.7173e-03, -4.2547e-02, -4.3211e-02,  4.7153e-02,\n",
      "        -4.5407e-02,  2.0280e-02, -5.9629e-02, -2.7059e-02,  3.2014e-02,\n",
      "        -1.7888e-02,  4.2581e-02, -5.9903e-02, -2.4322e-02,  2.1698e-02,\n",
      "         1.0464e-02,  4.3730e-02,  1.8026e-02, -3.6976e-02, -4.9780e-03,\n",
      "        -1.7944e-02, -3.7822e-04,  2.9156e-02, -4.8688e-02,  4.1845e-02,\n",
      "         3.0634e-02,  3.8996e-02, -1.1830e-02,  8.6009e-03, -2.2593e-02,\n",
      "         3.8362e-02, -3.2044e-02, -5.0574e-02, -1.0211e-02,  1.5153e-04,\n",
      "        -2.5151e-02, -1.9218e-02,  4.8749e-02, -1.8593e-02,  4.9540e-02,\n",
      "         1.4684e-02,  5.2511e-02, -4.7313e-02, -2.0243e-02, -2.4787e-02,\n",
      "        -4.3737e-02,  2.0214e-02,  4.6095e-02,  5.7765e-02, -6.1123e-02,\n",
      "         3.7588e-02,  2.2944e-02, -9.6758e-03, -1.6853e-02, -2.7572e-03,\n",
      "         3.3202e-02, -2.3171e-02, -2.9303e-05,  4.1189e-02,  5.5262e-02,\n",
      "         3.2586e-02,  5.3956e-02,  3.0029e-02, -1.9485e-02,  2.9198e-02,\n",
      "         1.8846e-02, -5.3678e-02, -4.5735e-02, -1.2063e-02,  2.1307e-02,\n",
      "        -2.6266e-02, -9.3017e-03,  9.1172e-03,  1.3805e-02,  2.4292e-02,\n",
      "         6.1166e-02, -1.8470e-02,  3.8547e-02,  2.9120e-02, -3.6264e-02,\n",
      "        -4.0720e-02, -9.0942e-03, -9.4236e-03, -2.8087e-02, -8.5005e-03,\n",
      "        -3.0015e-02,  7.9125e-03,  1.2587e-02, -2.3988e-02,  1.7372e-02,\n",
      "        -1.4369e-02, -4.2021e-02, -3.1109e-02,  3.2372e-02, -1.3514e-02,\n",
      "         4.8241e-02,  6.5960e-03, -6.1108e-02, -6.0792e-02,  3.5750e-02,\n",
      "         2.9062e-02, -1.4958e-02, -5.2538e-02, -7.6422e-03,  5.3656e-02,\n",
      "         2.8161e-02, -1.0817e-02,  3.9780e-02,  2.9082e-02, -4.9432e-02,\n",
      "         5.6519e-02, -3.7662e-02,  5.7510e-02,  5.9476e-02, -5.5742e-02,\n",
      "        -4.6807e-03, -5.5916e-03,  1.5705e-02, -4.1735e-02,  1.8211e-03,\n",
      "         4.6158e-02, -3.3631e-02,  3.1536e-02, -5.2471e-02, -1.1320e-02,\n",
      "         5.2905e-04,  2.0363e-02, -4.6704e-02, -5.4685e-02,  3.1860e-02,\n",
      "         4.7811e-02, -7.2312e-03,  2.5225e-02,  2.6319e-02,  3.3051e-02,\n",
      "        -4.3698e-02,  4.9663e-02,  3.2317e-02,  7.4095e-03,  5.0520e-02,\n",
      "         4.1134e-02,  3.1725e-03,  5.9985e-04,  4.6813e-03,  4.0543e-02,\n",
      "         3.1548e-03,  4.1053e-03, -2.8463e-02,  4.2269e-02,  6.2256e-03,\n",
      "         6.2241e-02,  1.3908e-02,  3.1191e-02, -1.9717e-02, -2.2297e-02,\n",
      "         5.0064e-02,  5.4801e-03,  1.9503e-02,  2.5534e-02,  3.3041e-03,\n",
      "         1.9007e-02,  5.0288e-02,  4.1776e-02, -4.2183e-02, -3.6152e-02,\n",
      "        -4.7955e-03,  5.6412e-02, -1.0344e-02, -2.2306e-02, -5.6895e-03,\n",
      "         4.6070e-02,  1.0561e-02,  3.5738e-02, -2.1990e-02, -5.3145e-02,\n",
      "         3.7008e-02, -5.8744e-02,  2.3444e-02,  5.5630e-02, -3.3782e-02,\n",
      "         4.6870e-02,  3.5946e-02,  2.4282e-02,  4.8680e-02,  2.8074e-02,\n",
      "         6.2322e-02, -1.6728e-02,  2.4100e-02, -1.5029e-02, -5.7955e-02,\n",
      "         1.4384e-03,  2.3622e-02,  3.9178e-02,  1.5929e-02,  5.2266e-02,\n",
      "         6.3680e-03, -3.5594e-02, -1.9625e-02,  3.2376e-02, -4.5865e-02,\n",
      "        -6.2415e-02,  5.0246e-03, -9.6088e-03,  2.8139e-02,  5.4125e-02,\n",
      "        -1.0093e-02, -5.6475e-02, -2.1600e-02, -5.3768e-02, -6.0619e-02,\n",
      "         3.2873e-02, -5.4632e-02, -5.9746e-03,  4.2755e-02, -5.2363e-02,\n",
      "         5.8660e-02, -4.2486e-02], device='cuda:1', requires_grad=True)\n",
      "layers.1.weight Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1', requires_grad=True)\n",
      "layers.1.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:1', requires_grad=True)\n",
      "layers.4.weight Parameter containing:\n",
      "tensor([[ 0.0100, -0.0433, -0.0090,  ...,  0.0176, -0.0383, -0.0191],\n",
      "        [-0.0131, -0.0174, -0.0112,  ..., -0.0427,  0.0241, -0.0351],\n",
      "        [ 0.0171, -0.0395, -0.0185,  ..., -0.0072, -0.0284,  0.0072],\n",
      "        ...,\n",
      "        [ 0.0193, -0.0401, -0.0230,  ..., -0.0301,  0.0427,  0.0202],\n",
      "        [ 0.0359,  0.0416,  0.0169,  ...,  0.0331,  0.0233,  0.0425],\n",
      "        [ 0.0210, -0.0152,  0.0285,  ..., -0.0111, -0.0217,  0.0394]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.4.bias Parameter containing:\n",
      "tensor([ 4.3109e-02, -3.2801e-02,  5.4893e-03,  1.9541e-03,  2.1609e-02,\n",
      "         8.4390e-03,  4.1075e-02,  3.5171e-02,  2.4127e-02,  1.4859e-02,\n",
      "         4.0849e-03,  6.2853e-04,  7.7977e-03, -2.1608e-02,  3.5027e-02,\n",
      "        -2.7704e-02,  1.9759e-02, -2.8449e-02,  4.3136e-02,  1.1393e-02,\n",
      "        -1.2802e-03,  2.0987e-02, -1.9810e-02,  3.6144e-02,  1.4222e-02,\n",
      "         1.2956e-02, -1.4990e-02, -4.2190e-02, -3.1616e-02, -3.8419e-02,\n",
      "        -7.7576e-03,  7.0591e-03,  2.8426e-02,  1.7414e-02,  6.8182e-03,\n",
      "        -3.8094e-02,  2.0879e-02,  1.7931e-03,  1.6968e-02, -1.3236e-02,\n",
      "        -4.2944e-02,  3.6282e-02, -3.5225e-02, -2.0022e-02,  1.7775e-02,\n",
      "        -2.8686e-02,  2.2191e-02, -3.9404e-02,  4.1426e-02,  9.0580e-03,\n",
      "        -3.3127e-02, -1.2949e-02, -3.8474e-02, -2.4606e-02, -3.0061e-02,\n",
      "        -1.0087e-02,  3.0859e-02,  1.9462e-02,  2.5939e-02,  3.0675e-02,\n",
      "        -1.6916e-02,  1.3124e-02,  3.6195e-02, -6.8851e-03, -1.6885e-03,\n",
      "         4.3176e-02,  2.9656e-02, -2.5141e-02,  1.8248e-02,  2.9389e-02,\n",
      "         2.6034e-02, -3.0517e-02,  4.3517e-02,  1.1972e-02, -1.0587e-03,\n",
      "         1.6013e-02,  2.9560e-02, -3.3335e-03,  3.4769e-02,  1.5348e-02,\n",
      "        -1.8510e-02, -3.1483e-02,  4.0981e-02,  2.3227e-02, -5.6289e-03,\n",
      "         2.9862e-02,  3.1569e-02,  4.1336e-02, -1.1006e-02,  3.8704e-02,\n",
      "        -2.7399e-02, -2.4892e-02,  7.4938e-03, -2.3100e-02, -1.4845e-02,\n",
      "        -6.1292e-03,  7.3098e-03, -8.5192e-03, -3.7302e-02, -1.2493e-02,\n",
      "         4.3319e-02,  1.6150e-02, -4.1528e-02, -1.8493e-02,  1.6924e-04,\n",
      "         1.0211e-02, -1.6433e-02, -2.8021e-02, -5.7652e-03, -2.6810e-02,\n",
      "        -4.0823e-03,  1.2545e-02, -3.3099e-02, -4.0998e-02,  3.3958e-02,\n",
      "         3.6042e-02, -4.3817e-02,  8.2387e-03, -1.1388e-02,  5.5599e-03,\n",
      "        -6.2683e-04, -2.6675e-02,  4.3683e-02, -2.9045e-02, -1.3919e-03,\n",
      "        -2.9360e-02, -2.7954e-02,  2.5924e-02,  3.0274e-02, -4.3210e-02,\n",
      "         3.5167e-02, -1.7646e-02, -3.4955e-02, -7.7982e-03, -3.9240e-02,\n",
      "         5.3863e-03,  3.9913e-02,  7.5478e-03,  2.0647e-02,  1.0049e-02,\n",
      "         3.0217e-02, -3.9296e-02, -5.0180e-03, -2.3256e-02, -5.4626e-03,\n",
      "         4.4062e-02,  1.1940e-03, -3.0714e-02, -2.9896e-02, -2.8514e-02,\n",
      "        -2.2522e-02,  1.1523e-02, -1.9228e-02, -9.7999e-03,  3.7167e-02,\n",
      "        -2.8767e-02, -1.4526e-02, -2.1403e-02, -2.9467e-02, -1.7411e-02,\n",
      "        -2.4875e-02,  2.0065e-02,  2.6264e-02,  2.2610e-02,  3.0335e-02,\n",
      "        -3.4522e-02, -2.2902e-02,  5.1021e-04, -2.8779e-02, -3.8170e-02,\n",
      "         2.5926e-02, -8.6963e-03,  4.3734e-02, -3.6700e-02,  4.2794e-02,\n",
      "         4.3633e-02, -1.2117e-02, -3.8026e-02, -1.2612e-02,  2.7255e-02,\n",
      "        -2.9852e-02,  2.1948e-02,  4.0531e-02, -2.7395e-02, -2.4117e-02,\n",
      "         2.1865e-02,  2.3822e-02,  5.8869e-03, -1.7548e-02, -2.2580e-02,\n",
      "         1.2733e-03, -2.4444e-02,  2.1406e-03, -6.9550e-03,  2.7548e-02,\n",
      "         1.0376e-02, -2.6208e-02,  2.8147e-03, -1.0434e-02,  1.3887e-02,\n",
      "         1.5723e-02,  6.7882e-03,  3.5306e-03,  9.1755e-03,  3.9129e-02,\n",
      "        -1.0946e-02,  1.9474e-03, -2.3875e-02, -2.1818e-02,  3.7436e-02,\n",
      "         1.2680e-02,  2.0954e-02,  3.6763e-02,  3.5530e-02, -3.8331e-02,\n",
      "         1.5486e-02,  3.4144e-02,  9.0364e-03,  4.2919e-02,  2.5122e-02,\n",
      "        -4.4935e-03,  3.6971e-02, -5.3608e-03, -3.4608e-02,  2.7473e-02,\n",
      "        -4.0199e-02, -2.4200e-02,  9.5813e-03, -8.6628e-03, -1.3545e-02,\n",
      "         2.9943e-02, -3.9233e-03, -2.9477e-02,  3.9073e-02,  1.8457e-02,\n",
      "         2.4879e-02,  3.8518e-03,  3.9977e-02,  3.4581e-02,  2.9065e-02,\n",
      "        -1.3559e-02, -3.3995e-02,  2.6851e-02,  1.6438e-02,  5.7298e-03,\n",
      "         1.9793e-02, -3.2346e-02, -1.0175e-02,  4.6978e-04,  1.6957e-02,\n",
      "         3.3187e-02,  4.9299e-03, -2.9316e-02,  1.2434e-02, -4.2856e-02,\n",
      "         2.6996e-02, -3.9881e-02,  1.8328e-02, -3.6557e-03,  1.9166e-03,\n",
      "         4.3893e-02,  1.0254e-02,  4.3771e-02, -3.9707e-02, -2.9106e-02,\n",
      "        -7.2311e-03, -3.3277e-02,  3.3824e-02,  1.4928e-02, -2.9765e-02,\n",
      "         3.9885e-02,  3.7428e-02, -1.5667e-02,  5.3133e-03, -1.9716e-02,\n",
      "        -3.6484e-02, -2.5266e-02, -2.1284e-02,  3.6284e-02,  4.7794e-03,\n",
      "         4.0340e-02, -2.6893e-02, -2.9730e-03, -2.8955e-03,  4.3411e-02,\n",
      "         2.7180e-02,  2.2862e-02,  1.1668e-02, -4.5524e-04,  3.1496e-02,\n",
      "        -3.1123e-02,  5.6378e-03, -1.1275e-02,  8.4094e-03, -4.1904e-02,\n",
      "         1.2566e-04, -3.0519e-02,  1.4603e-02, -2.7297e-02,  1.6998e-03,\n",
      "        -1.9483e-02,  5.1006e-03,  3.4100e-02, -1.9706e-02,  2.2272e-02,\n",
      "         1.2221e-02, -1.4937e-02, -2.3118e-02, -4.1257e-02, -9.4642e-04,\n",
      "        -1.0788e-02, -8.3976e-04, -2.7687e-02,  2.2273e-02, -3.9409e-02,\n",
      "        -1.0902e-02, -1.7849e-02, -4.9106e-03,  1.5749e-02, -3.8639e-02,\n",
      "        -1.2518e-02, -3.5190e-02, -1.3585e-02,  2.1298e-02, -3.9421e-02,\n",
      "         3.2767e-02, -9.1625e-03,  1.5897e-02,  2.2111e-02, -2.6479e-02,\n",
      "        -4.0484e-02, -1.0165e-02, -3.2993e-02, -3.5899e-02,  1.1259e-02,\n",
      "        -1.8634e-02, -1.2612e-02, -6.8146e-06,  6.8744e-03, -2.0958e-02,\n",
      "         3.5825e-02, -2.3412e-02, -1.1782e-02,  2.2349e-02,  3.7564e-02,\n",
      "         4.4126e-02, -4.1211e-02,  5.5745e-03,  3.1958e-02, -2.3898e-02,\n",
      "        -3.6879e-02, -6.6003e-03,  3.5922e-02,  1.9791e-02,  1.1604e-03,\n",
      "         2.8502e-02,  8.3603e-04,  3.5926e-03, -2.1996e-03,  8.9141e-03,\n",
      "         4.3398e-02, -2.3355e-04, -1.0244e-02, -2.7663e-02, -4.5522e-03,\n",
      "        -3.3031e-02,  3.8411e-02, -3.9021e-02, -1.1211e-02,  1.3826e-02,\n",
      "        -4.3462e-02, -3.3328e-02,  1.8910e-02,  3.9653e-02, -9.3391e-03,\n",
      "         1.7690e-03, -2.3611e-02, -3.5312e-02, -1.4349e-02, -2.3985e-02,\n",
      "        -2.3629e-02,  4.7821e-03,  2.2806e-02,  4.2332e-02, -2.3238e-02,\n",
      "         3.5286e-02, -4.1627e-02, -2.6920e-02,  2.4756e-02,  1.3332e-02,\n",
      "         3.1509e-03, -2.3360e-02,  4.1854e-02,  3.0810e-02, -7.4145e-03,\n",
      "        -3.0433e-03,  3.3880e-02, -1.7597e-02,  4.0219e-02, -5.3175e-03,\n",
      "        -2.9078e-03,  1.8958e-02,  1.0435e-02,  5.6172e-03,  6.9912e-03,\n",
      "         4.3933e-02,  7.2460e-03, -8.0203e-03,  2.0221e-02, -1.7638e-02,\n",
      "        -4.1413e-02, -2.0513e-03, -1.4889e-02,  1.1743e-02, -3.1788e-02,\n",
      "        -4.4519e-03, -4.2869e-02,  9.6776e-03,  6.3440e-03, -1.2763e-02,\n",
      "         1.3264e-02,  1.2484e-02,  1.8101e-02,  2.1292e-02, -2.5342e-02,\n",
      "        -1.0654e-02,  1.5575e-02,  7.7722e-03, -4.4048e-02,  2.9614e-03,\n",
      "        -5.7868e-03, -4.3365e-03, -1.3171e-03, -5.4358e-03,  1.1101e-02,\n",
      "        -3.7351e-03,  3.4775e-02,  3.1069e-02,  7.6579e-03, -3.7058e-02,\n",
      "        -1.8655e-02, -5.8771e-04,  1.5265e-02,  4.4061e-04, -3.9215e-02,\n",
      "         1.9707e-02,  3.5757e-02, -2.2435e-02,  2.0704e-02,  2.5033e-02,\n",
      "        -1.4464e-02,  8.8140e-03, -3.9236e-02, -3.5673e-02,  3.4051e-02,\n",
      "        -3.7311e-02, -1.6690e-02, -3.3582e-02,  2.9312e-02,  2.1996e-02,\n",
      "         3.1715e-02,  5.2456e-03,  4.0597e-02,  4.1561e-02, -3.5729e-02,\n",
      "         1.3888e-02, -2.4256e-02, -2.1033e-02,  3.6927e-02, -5.8745e-03,\n",
      "        -4.0389e-02,  2.7486e-02, -2.5707e-02, -1.7120e-02,  3.5823e-03,\n",
      "         3.0939e-02,  9.3838e-03, -3.5785e-02, -5.2409e-03, -2.0895e-02,\n",
      "         4.2324e-02,  2.3184e-02, -2.2450e-02,  2.1191e-02,  2.6859e-02,\n",
      "        -4.2537e-02, -1.8679e-02,  7.5491e-03, -1.1060e-02,  4.0615e-02,\n",
      "         3.1540e-03, -3.4761e-02,  3.4485e-02, -4.0342e-02, -1.4978e-02,\n",
      "         2.2255e-02, -1.4663e-02,  4.3320e-02,  1.5348e-02, -2.0222e-02,\n",
      "         1.4348e-02,  1.5461e-02,  8.0341e-03,  3.2851e-02, -2.1730e-02,\n",
      "        -1.4279e-02, -2.0940e-02, -4.0513e-02, -3.0517e-02,  2.1618e-02,\n",
      "         2.5975e-02, -3.4657e-02,  1.2230e-04,  3.2243e-02, -3.6442e-02,\n",
      "        -4.8676e-03, -1.1283e-02, -1.7583e-02, -2.2571e-02,  3.8174e-02,\n",
      "        -2.5976e-02, -4.1454e-02,  1.2764e-02, -3.5972e-02,  4.1536e-02,\n",
      "         3.2119e-02,  4.3875e-02,  5.4506e-03,  3.1827e-02,  3.1714e-02,\n",
      "        -3.8190e-02, -3.5963e-03,  2.4782e-02, -1.4402e-02, -2.0487e-03,\n",
      "        -2.2635e-02,  2.2476e-02, -8.9806e-03, -2.1223e-02, -3.8253e-02,\n",
      "         2.4996e-02, -2.4027e-03,  1.4970e-03,  3.6042e-02, -2.1553e-02,\n",
      "         2.6411e-02,  2.1445e-02,  1.4528e-03,  2.9989e-02, -2.6047e-02,\n",
      "        -4.2457e-03,  2.8569e-02, -3.2480e-02,  3.2787e-02, -3.0775e-02,\n",
      "        -2.4485e-03,  4.2203e-02, -3.6351e-02, -1.9590e-02, -3.8843e-02,\n",
      "         2.5666e-02,  1.4409e-02, -7.6710e-03, -2.9835e-03, -1.7431e-02,\n",
      "         4.0246e-02, -9.6331e-04,  1.2155e-02,  5.3811e-05, -1.7384e-02,\n",
      "         2.4717e-02,  5.3348e-03,  9.9454e-03, -1.8040e-02,  4.0149e-02,\n",
      "        -2.3082e-02,  2.7852e-04, -1.7306e-02,  2.3418e-02,  1.8312e-02,\n",
      "         9.5495e-03, -2.8502e-02, -1.9746e-02, -3.8562e-02,  1.2360e-02,\n",
      "         1.3020e-02,  2.0078e-02,  2.9008e-02, -2.8139e-02,  1.4516e-02,\n",
      "         2.4350e-02, -1.0128e-03, -3.3110e-02,  1.4904e-02, -1.0751e-02,\n",
      "        -2.5614e-02,  4.3985e-02,  2.5793e-02,  3.3551e-02, -2.7259e-02,\n",
      "        -2.7296e-02, -3.8471e-02, -1.5363e-02, -6.6349e-03, -3.2464e-02,\n",
      "         5.5405e-03, -1.1571e-02, -3.2321e-02,  1.8183e-02, -1.2968e-02,\n",
      "        -2.6942e-02,  5.3289e-03, -2.1171e-02,  4.1398e-02,  4.3321e-02,\n",
      "        -3.0662e-02, -2.6116e-02,  6.1371e-03, -3.1858e-02,  8.3627e-03,\n",
      "         4.0378e-02, -8.5681e-03,  2.2886e-02, -1.6126e-02,  1.7800e-02,\n",
      "        -4.0073e-02, -3.3342e-02,  8.7607e-03,  4.2738e-02,  1.6589e-02,\n",
      "        -2.6488e-02,  2.2536e-02, -3.0228e-02,  4.2862e-02,  3.4873e-02,\n",
      "         7.9679e-03,  3.7492e-02,  3.6617e-02,  3.7276e-02,  4.1051e-02,\n",
      "        -6.2231e-03,  1.6639e-02, -3.9469e-02, -3.3944e-02,  2.4814e-02,\n",
      "         3.4302e-02, -2.7831e-02, -3.9809e-02, -1.5680e-02,  2.7381e-02,\n",
      "         3.3545e-02,  3.2762e-02,  2.3750e-02, -1.6342e-02,  4.0951e-02,\n",
      "        -7.5953e-03,  3.3143e-02,  5.8285e-03,  3.3013e-02, -4.0403e-02,\n",
      "        -8.1731e-03,  2.3389e-02, -1.3298e-02, -8.2850e-03,  1.0353e-02,\n",
      "        -3.0347e-02,  1.2960e-02, -9.1247e-03, -4.2200e-02,  1.1844e-02,\n",
      "         3.5688e-02, -1.8469e-02,  2.1990e-02, -2.4195e-02,  1.3792e-02,\n",
      "         4.3623e-02,  1.5351e-02,  2.4102e-02,  1.0443e-03,  4.3162e-02,\n",
      "        -3.5161e-02,  3.8266e-02, -2.1710e-02, -3.3835e-02, -1.2524e-02,\n",
      "        -4.1473e-02, -8.6337e-03,  3.0304e-02,  1.4875e-02, -3.4249e-02,\n",
      "        -3.8918e-03,  2.5816e-02, -1.9974e-02,  2.5072e-02, -3.5064e-02,\n",
      "         3.5053e-02, -1.5209e-02,  2.2202e-02,  2.7282e-02,  3.4537e-02,\n",
      "         6.6459e-03, -1.7395e-02,  4.4116e-02,  7.6944e-03, -2.6033e-02,\n",
      "        -4.1282e-02,  3.8659e-02,  2.7487e-02, -3.1846e-02, -7.2948e-04,\n",
      "        -1.2955e-02, -1.1533e-03,  3.7641e-02,  4.1498e-02, -5.0943e-03,\n",
      "         2.2162e-02,  2.7445e-02,  3.1787e-02,  1.2308e-03, -9.6876e-03,\n",
      "        -1.3526e-02,  2.4290e-02, -4.6618e-03, -1.6451e-04, -2.9782e-02,\n",
      "        -2.8320e-02,  2.2808e-02, -3.5925e-02, -1.9508e-02, -2.9657e-02,\n",
      "        -4.0247e-02, -9.4529e-03, -2.0424e-02, -2.8354e-02, -2.2849e-02,\n",
      "        -3.1005e-02,  1.7210e-02, -3.7836e-02,  3.6697e-02,  2.8565e-02,\n",
      "        -2.4344e-02, -2.6004e-02,  6.3194e-03,  1.3401e-02, -1.3282e-02,\n",
      "         1.8664e-02, -2.0943e-02, -2.0791e-02,  1.4291e-02, -3.8707e-02,\n",
      "         2.8368e-02, -1.2317e-02,  1.2545e-02,  4.1254e-02, -3.5186e-02,\n",
      "        -1.9311e-02, -3.1323e-02, -2.0291e-02, -4.0727e-02, -2.5305e-02,\n",
      "         2.0082e-02, -3.9312e-02,  3.1085e-02, -2.8188e-02, -4.0541e-02,\n",
      "         7.3679e-04, -1.1387e-02, -5.4193e-03,  3.0740e-02,  3.8719e-03,\n",
      "        -1.8849e-02,  8.0288e-03, -1.0543e-02,  2.3049e-02,  3.2951e-02,\n",
      "         3.6185e-02,  1.6065e-02,  1.8891e-02, -3.5182e-02, -9.6459e-04,\n",
      "        -1.5715e-02, -3.8661e-02,  3.8695e-02,  3.4272e-02,  1.7584e-02,\n",
      "        -4.2483e-03,  3.2158e-02,  3.0004e-02,  2.0887e-02, -3.5083e-02,\n",
      "         4.8657e-03, -3.0696e-02, -7.5341e-03, -1.9440e-02, -2.2496e-02,\n",
      "        -3.7458e-02,  4.3613e-02, -1.3040e-02, -1.9714e-02, -2.8884e-03,\n",
      "        -1.5787e-02, -4.3857e-02,  2.4227e-02, -2.7288e-02,  2.4073e-02,\n",
      "        -4.8899e-03,  4.9680e-03, -3.1392e-02, -1.3368e-02,  1.7061e-02,\n",
      "        -1.7203e-03,  2.1342e-03,  2.1913e-02, -2.9316e-02, -2.7818e-02,\n",
      "        -2.8422e-02, -3.4475e-02, -4.2850e-02,  1.6268e-02, -3.9480e-03,\n",
      "        -3.6767e-02, -3.1930e-02,  3.2166e-02, -1.1150e-02,  2.9414e-02,\n",
      "         1.6209e-02, -2.7109e-02,  2.9860e-03,  2.1285e-02, -1.3028e-02,\n",
      "        -5.0890e-03,  3.0761e-02, -3.4111e-02, -3.5554e-02,  2.0143e-02,\n",
      "         8.8051e-03,  1.9248e-02,  9.6030e-03,  3.8439e-02,  7.4859e-03,\n",
      "        -1.9371e-02, -3.7631e-02, -4.3367e-02,  3.6450e-02,  1.7824e-02,\n",
      "         2.6772e-02,  3.3340e-02, -3.8759e-02,  5.9965e-03, -2.2079e-02,\n",
      "         4.3279e-02,  3.6469e-02,  1.1451e-03,  1.2911e-02,  2.2246e-02,\n",
      "         4.4827e-03,  1.5509e-02, -1.8780e-02, -2.5878e-02,  2.8678e-02,\n",
      "         1.7479e-03, -2.0584e-02,  1.8143e-03,  4.2760e-02, -3.5867e-02,\n",
      "        -4.0037e-02, -3.7120e-02,  1.5289e-02, -1.6451e-02, -1.6295e-02,\n",
      "         4.1314e-02, -8.0213e-03, -4.3786e-02, -2.5036e-02, -1.2843e-02,\n",
      "        -4.0393e-02,  1.7452e-02,  2.8297e-02,  2.0075e-02, -4.3589e-02,\n",
      "         3.7197e-02, -2.6204e-02, -1.6286e-02, -4.3183e-02, -2.0402e-02,\n",
      "        -2.6619e-02,  2.0166e-02,  1.6414e-02, -3.6841e-02, -9.6221e-03,\n",
      "        -3.5068e-02,  2.2230e-02,  3.7687e-02,  1.4909e-02, -1.6485e-02,\n",
      "        -3.9623e-02, -2.8549e-02, -2.8438e-02,  1.9430e-02, -1.6406e-02,\n",
      "         1.6767e-02, -4.1524e-02,  1.3445e-02,  2.7890e-02, -2.4461e-03,\n",
      "        -1.9029e-02,  2.6642e-02,  1.7092e-02, -2.5564e-02, -8.9764e-03,\n",
      "         2.9272e-02,  4.2757e-02, -1.0283e-02,  2.2267e-02, -2.8174e-02,\n",
      "         1.4851e-02, -1.5408e-02,  3.0138e-02,  4.2053e-02, -2.1285e-02,\n",
      "         7.1044e-03,  1.2371e-02,  1.2139e-03,  3.0491e-02, -5.8850e-03,\n",
      "         3.1946e-03, -1.1493e-03,  3.3644e-02,  2.6593e-02, -1.6023e-02,\n",
      "         2.2457e-02, -3.0423e-02, -3.6448e-02,  1.1491e-02, -7.1700e-03,\n",
      "         2.9173e-02,  4.1156e-02, -3.4150e-02, -3.5918e-02,  2.0626e-02,\n",
      "         3.1206e-02,  8.7244e-03, -1.6577e-02,  3.6770e-02,  1.9441e-02,\n",
      "         1.2769e-02, -1.5142e-02, -2.5072e-02, -1.3721e-02, -2.0088e-02,\n",
      "        -2.6450e-02,  3.3264e-02, -1.4981e-03,  3.8622e-02,  3.7566e-02,\n",
      "         1.2077e-02], device='cuda:1', requires_grad=True)\n",
      "layers.7.weight Parameter containing:\n",
      "tensor([[ 0.0211, -0.0186,  0.0148,  ..., -0.0002,  0.0276,  0.0047],\n",
      "        [-0.0054, -0.0216, -0.0057,  ..., -0.0137,  0.0202, -0.0179]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.7.bias Parameter containing:\n",
      "tensor([ 0.0149, -0.0246], device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hgnn_trainer.layers\n",
    "for n,p in hgnn_trainer.named_parameters():\n",
    "    print(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnn_trainer.weight = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -5150.662890625\n",
      "                , loss1: -106.3420166015625\n",
      "                , loss2: 165.7999755859375\n",
      "=================================\n",
      "in 10 epoch, average loss: 32647.171875\n",
      "                , loss1: -1094.99443359375\n",
      "                , loss2: 87354.5\n",
      "=================================\n",
      "in 20 epoch, average loss: -37045.16875\n",
      "                , loss1: -1066.59384765625\n",
      "                , loss2: 16178.9609375\n",
      "=================================\n",
      "in 30 epoch, average loss: -48164.39375\n",
      "                , loss1: -1062.1421875\n",
      "                , loss2: 4773.846875\n",
      "=================================\n",
      "in 40 epoch, average loss: -50788.709375\n",
      "                , loss1: -1060.7703125\n",
      "                , loss2: 2017.4923828125\n",
      "=================================\n",
      "in 50 epoch, average loss: -52142.70625\n",
      "                , loss1: -1060.1615234375\n",
      "                , loss2: 569.59365234375\n",
      "=================================\n",
      "in 60 epoch, average loss: -52500.24375\n",
      "                , loss1: -1060.10009765625\n",
      "                , loss2: 145.3892822265625\n",
      "=================================\n",
      "in 70 epoch, average loss: -52543.10625\n",
      "                , loss1: -1060.103125\n",
      "                , loss2: 39.068338012695314\n",
      "=================================\n",
      "in 80 epoch, average loss: -52504.64375\n",
      "                , loss1: -1060.33955078125\n",
      "                , loss2: 25.644677734375\n",
      "=================================\n",
      "in 90 epoch, average loss: -52463.575\n",
      "                , loss1: -1060.56455078125\n",
      "                , loss2: 14.218226623535156\n",
      "=================================\n",
      "in 100 epoch, average loss: -52418.7\n",
      "                , loss1: -1060.795703125\n",
      "                , loss2: 6.884326934814453\n",
      "=================================\n",
      "in 110 epoch, average loss: -52370.43125\n",
      "                , loss1: -1061.05595703125\n",
      "                , loss2: 4.345548629760742\n",
      "=================================\n",
      "in 120 epoch, average loss: -52329.55\n",
      "                , loss1: -1061.508203125\n",
      "                , loss2: 3.8592605590820312\n",
      "=================================\n",
      "in 130 epoch, average loss: -52306.525\n",
      "                , loss1: -1062.340234375\n",
      "                , loss2: 4.1713706970214846\n",
      "=================================\n",
      "in 140 epoch, average loss: -52278.371875\n",
      "                , loss1: -1063.037109375\n",
      "                , loss2: 2.85107364654541\n",
      "=================================\n",
      "in 150 epoch, average loss: -52320.203125\n",
      "                , loss1: -1065.2259765625\n",
      "                , loss2: 4.7431682586669925\n",
      "=================================\n",
      "in 160 epoch, average loss: -52863.99375\n",
      "                , loss1: -1078.734765625\n",
      "                , loss2: 59.65101318359375\n",
      "=================================\n",
      "in 170 epoch, average loss: -65852.575\n",
      "                , loss1: -1346.8251953125\n",
      "                , loss2: 138.9665771484375\n",
      "=================================\n",
      "in 180 epoch, average loss: -199993.825\n",
      "                , loss1: -4097.63984375\n",
      "                , loss2: 532.432080078125\n",
      "=================================\n",
      "in 190 epoch, average loss: -292327.075\n",
      "                , loss1: -5987.76171875\n",
      "                , loss2: 357.45625\n",
      "=================================\n",
      "in 200 epoch, average loss: -308863.375\n",
      "                , loss1: -6333.0203125\n",
      "                , loss2: 320.2905517578125\n",
      "=================================\n",
      "in 210 epoch, average loss: -314179.675\n",
      "                , loss1: -6447.87578125\n",
      "                , loss2: 224.803759765625\n",
      "=================================\n",
      "in 220 epoch, average loss: -317308.65\n",
      "                , loss1: -6518.01640625\n",
      "                , loss2: 125.0316650390625\n",
      "=================================\n",
      "in 230 epoch, average loss: -319413.675\n",
      "                , loss1: -6571.0125\n",
      "                , loss2: 206.64970703125\n",
      "=================================\n",
      "in 240 epoch, average loss: -321848.4\n",
      "                , loss1: -6629.253125\n",
      "                , loss2: 207.1477294921875\n",
      "=================================\n",
      "in 250 epoch, average loss: -323157.925\n",
      "                , loss1: -6663.25703125\n",
      "                , loss2: 149.7833740234375\n",
      "=================================\n",
      "in 260 epoch, average loss: -323839.825\n",
      "                , loss1: -6684.00859375\n",
      "                , loss2: 73.85977783203126\n",
      "=================================\n",
      "in 270 epoch, average loss: -324008.625\n",
      "                , loss1: -6696.03125\n",
      "                , loss2: 85.97716064453125\n",
      "=================================\n",
      "in 280 epoch, average loss: -324259.4\n",
      "                , loss1: -6710.23203125\n",
      "                , loss2: 119.78271484375\n",
      "=================================\n",
      "in 290 epoch, average loss: -324733.4\n",
      "                , loss1: -6726.3875\n",
      "                , loss2: 23.26396026611328\n",
      "=================================\n",
      "in 300 epoch, average loss: -324593.475\n",
      "                , loss1: -6735.09609375\n",
      "                , loss2: 179.62911376953124\n",
      "=================================\n",
      "in 310 epoch, average loss: -324592.875\n",
      "                , loss1: -6740.80625\n",
      "                , loss2: 51.03006591796875\n",
      "=================================\n",
      "in 320 epoch, average loss: -324762.775\n",
      "                , loss1: -6752.70625\n",
      "                , loss2: 49.12291259765625\n",
      "=================================\n",
      "in 330 epoch, average loss: -324486.85\n",
      "                , loss1: -6755.6203125\n",
      "                , loss2: 59.8987060546875\n",
      "=================================\n",
      "in 340 epoch, average loss: -324256.275\n",
      "                , loss1: -6758.61015625\n",
      "                , loss2: 28.565127563476562\n",
      "=================================\n",
      "in 350 epoch, average loss: -324065.6\n",
      "                , loss1: -6762.953125\n",
      "                , loss2: 21.862925720214843\n",
      "=================================\n",
      "in 360 epoch, average loss: -323789.125\n",
      "                , loss1: -6765.67890625\n",
      "                , loss2: 23.01982421875\n",
      "=================================\n",
      "in 370 epoch, average loss: -323045.45\n",
      "                , loss1: -6760.63984375\n",
      "                , loss2: 119.91492919921875\n",
      "=================================\n",
      "in 380 epoch, average loss: -322809.375\n",
      "                , loss1: -6762.4375\n",
      "                , loss2: 36.14769287109375\n",
      "=================================\n",
      "in 390 epoch, average loss: -322516.575\n",
      "                , loss1: -6764.38203125\n",
      "                , loss2: 15.882498168945313\n",
      "=================================\n",
      "in 400 epoch, average loss: -322270.55\n",
      "                , loss1: -6767.63046875\n",
      "                , loss2: 10.799307250976563\n",
      "=================================\n",
      "in 410 epoch, average loss: -321951.9\n",
      "                , loss1: -6769.465625\n",
      "                , loss2: 10.651204681396484\n",
      "=================================\n",
      "in 420 epoch, average loss: -321622.125\n",
      "                , loss1: -6770.90703125\n",
      "                , loss2: 2.7111188888549806\n",
      "=================================\n",
      "in 430 epoch, average loss: -321230.95\n",
      "                , loss1: -6771.2515625\n",
      "                , loss2: 3.9754653930664063\n",
      "=================================\n",
      "in 440 epoch, average loss: -320845.15\n",
      "                , loss1: -6771.77890625\n",
      "                , loss2: 8.503884887695312\n",
      "=================================\n",
      "in 450 epoch, average loss: -319577.85\n",
      "                , loss1: -6759.22109375\n",
      "                , loss2: 275.241259765625\n",
      "=================================\n",
      "in 460 epoch, average loss: -319462.325\n",
      "                , loss1: -6760.159375\n",
      "                , loss2: 29.542205810546875\n",
      "=================================\n",
      "in 470 epoch, average loss: -319279.875\n",
      "                , loss1: -6764.6453125\n",
      "                , loss2: 18.15714111328125\n",
      "=================================\n",
      "in 480 epoch, average loss: -319033.4\n",
      "                , loss1: -6767.72734375\n",
      "                , loss2: 4.039916229248047\n",
      "=================================\n",
      "in 490 epoch, average loss: -318717.0\n",
      "                , loss1: -6769.590625\n",
      "                , loss2: 2.0833307266235352\n",
      "=================================\n",
      "in 500 epoch, average loss: -318384.475\n",
      "                , loss1: -6771.16328125\n",
      "                , loss2: 2.394938659667969\n",
      "=================================\n",
      "in 510 epoch, average loss: -318022.0\n",
      "                , loss1: -6772.096875\n",
      "                , loss2: 2.4219226837158203\n",
      "=================================\n",
      "in 520 epoch, average loss: -317646.425\n",
      "                , loss1: -6772.79375\n",
      "                , loss2: 4.362978744506836\n",
      "=================================\n",
      "in 530 epoch, average loss: -317256.85\n",
      "                , loss1: -6773.12578125\n",
      "                , loss2: 3.139496994018555\n",
      "=================================\n",
      "in 540 epoch, average loss: -316876.175\n",
      "                , loss1: -6773.65\n",
      "                , loss2: 1.9677770614624024\n",
      "=================================\n",
      "in 550 epoch, average loss: -316485.625\n",
      "                , loss1: -6773.97890625\n",
      "                , loss2: 1.4463936805725097\n",
      "=================================\n",
      "in 560 epoch, average loss: -316102.375\n",
      "                , loss1: -6774.4671875\n",
      "                , loss2: 1.0110321044921875\n",
      "=================================\n",
      "in 570 epoch, average loss: -315683.875\n",
      "                , loss1: -6774.25703125\n",
      "                , loss2: 3.2555389404296875\n",
      "=================================\n",
      "in 580 epoch, average loss: -315305.875\n",
      "                , loss1: -6774.81796875\n",
      "                , loss2: 0.9584630966186524\n",
      "=================================\n",
      "in 590 epoch, average loss: -314899.9\n",
      "                , loss1: -6774.85390625\n",
      "                , loss2: 2.056564140319824\n",
      "=================================\n",
      "in 600 epoch, average loss: -314515.4\n",
      "                , loss1: -6775.296875\n",
      "                , loss2: 0.6728496551513672\n",
      "=================================\n",
      "in 610 epoch, average loss: -314123.225\n",
      "                , loss1: -6775.60234375\n",
      "                , loss2: 0.42338972091674804\n",
      "=================================\n",
      "in 620 epoch, average loss: -313711.45\n",
      "                , loss1: -6775.534375\n",
      "                , loss2: 2.5935850143432617\n",
      "=================================\n",
      "in 630 epoch, average loss: -313317.55\n",
      "                , loss1: -6775.7578125\n",
      "                , loss2: 0.2841357707977295\n",
      "=================================\n",
      "in 640 epoch, average loss: -312910.075\n",
      "                , loss1: -6775.7671875\n",
      "                , loss2: 1.6410694122314453\n",
      "=================================\n",
      "in 650 epoch, average loss: -312481.675\n",
      "                , loss1: -6775.3\n",
      "                , loss2: 1.9315635681152343\n",
      "=================================\n",
      "in 660 epoch, average loss: -312011.375\n",
      "                , loss1: -6774.0796875\n",
      "                , loss2: 9.529611968994141\n",
      "=================================\n",
      "in 670 epoch, average loss: -310761.65\n",
      "                , loss1: -6764.6578125\n",
      "                , loss2: 419.539501953125\n",
      "=================================\n",
      "in 680 epoch, average loss: -310658.425\n",
      "                , loss1: -6763.925\n",
      "                , loss2: 83.1223876953125\n",
      "=================================\n",
      "in 690 epoch, average loss: -310429.275\n",
      "                , loss1: -6766.63125\n",
      "                , loss2: 30.461376953125\n",
      "=================================\n",
      "in 700 epoch, average loss: -310260.825\n",
      "                , loss1: -6771.23515625\n",
      "                , loss2: 3.948001480102539\n",
      "=================================\n",
      "in 710 epoch, average loss: -309982.65\n",
      "                , loss1: -6773.984375\n",
      "                , loss2: 1.6012960433959962\n",
      "=================================\n",
      "in 720 epoch, average loss: -309644.2\n",
      "                , loss1: -6775.46484375\n",
      "                , loss2: 1.3219590187072754\n",
      "=================================\n",
      "in 730 epoch, average loss: -309277.5\n",
      "                , loss1: -6776.3296875\n",
      "                , loss2: 0.9524730682373047\n",
      "=================================\n",
      "in 740 epoch, average loss: -308891.1\n",
      "                , loss1: -6776.76328125\n",
      "                , loss2: 0.5635807514190674\n",
      "=================================\n",
      "in 750 epoch, average loss: -308512.075\n",
      "                , loss1: -6777.378125\n",
      "                , loss2: 1.0118367195129394\n",
      "=================================\n",
      "in 760 epoch, average loss: -308115.0\n",
      "                , loss1: -6777.58828125\n",
      "                , loss2: 0.9303440093994141\n",
      "=================================\n",
      "in 770 epoch, average loss: -307726.675\n",
      "                , loss1: -6777.98671875\n",
      "                , loss2: 0.7503684997558594\n",
      "=================================\n",
      "in 780 epoch, average loss: -307346.825\n",
      "                , loss1: -6778.57578125\n",
      "                , loss2: 0.5539194107055664\n",
      "=================================\n",
      "in 790 epoch, average loss: -306937.35\n",
      "                , loss1: -6778.5234375\n",
      "                , loss2: 0.939830207824707\n",
      "=================================\n",
      "in 800 epoch, average loss: -306544.0\n",
      "                , loss1: -6778.81015625\n",
      "                , loss2: 0.6234452247619628\n",
      "=================================\n",
      "in 810 epoch, average loss: -306140.95\n",
      "                , loss1: -6778.8953125\n",
      "                , loss2: 0.7621650695800781\n",
      "=================================\n",
      "in 820 epoch, average loss: -305738.175\n",
      "                , loss1: -6778.98125\n",
      "                , loss2: 0.62696213722229\n",
      "=================================\n",
      "in 830 epoch, average loss: -305347.1\n",
      "                , loss1: -6779.32265625\n",
      "                , loss2: 0.39296703338623046\n",
      "=================================\n",
      "in 840 epoch, average loss: -304946.075\n",
      "                , loss1: -6779.4515625\n",
      "                , loss2: 0.42776265144348147\n",
      "=================================\n",
      "in 850 epoch, average loss: -304513.8\n",
      "                , loss1: -6778.90546875\n",
      "                , loss2: 1.4125564575195313\n",
      "=================================\n",
      "in 860 epoch, average loss: -304121.35\n",
      "                , loss1: -6779.2046875\n",
      "                , loss2: 0.5488632202148438\n",
      "=================================\n",
      "in 870 epoch, average loss: -303727.4\n",
      "                , loss1: -6779.48984375\n",
      "                , loss2: 0.5062816619873047\n",
      "=================================\n",
      "in 880 epoch, average loss: -303318.075\n",
      "                , loss1: -6779.43359375\n",
      "                , loss2: 0.5836048126220703\n",
      "=================================\n",
      "in 890 epoch, average loss: -302907.75\n",
      "                , loss1: -6779.353125\n",
      "                , loss2: 0.532371187210083\n",
      "=================================\n",
      "in 900 epoch, average loss: -302504.05\n",
      "                , loss1: -6779.41640625\n",
      "                , loss2: 0.2890250444412231\n",
      "=================================\n",
      "in 910 epoch, average loss: -302103.875\n",
      "                , loss1: -6779.565625\n",
      "                , loss2: 0.3092081308364868\n",
      "=================================\n",
      "in 920 epoch, average loss: -301706.65\n",
      "                , loss1: -6779.78203125\n",
      "                , loss2: 0.4554738998413086\n",
      "=================================\n",
      "in 930 epoch, average loss: -301297.5\n",
      "                , loss1: -6779.72578125\n",
      "                , loss2: 0.2645371913909912\n",
      "=================================\n",
      "in 940 epoch, average loss: -300894.625\n",
      "                , loss1: -6779.81796875\n",
      "                , loss2: 0.47876696586608886\n",
      "=================================\n",
      "in 950 epoch, average loss: -300483.0\n",
      "                , loss1: -6779.70859375\n",
      "                , loss2: 0.48464508056640626\n",
      "=================================\n",
      "in 960 epoch, average loss: -300078.25\n",
      "                , loss1: -6779.74921875\n",
      "                , loss2: 0.21159193515777588\n",
      "=================================\n",
      "in 970 epoch, average loss: -299642.375\n",
      "                , loss1: -6779.3625\n",
      "                , loss2: 12.252665710449218\n",
      "=================================\n",
      "in 980 epoch, average loss: -299242.025\n",
      "                , loss1: -6779.728125\n",
      "                , loss2: 21.933193969726563\n",
      "=================================\n",
      "in 990 epoch, average loss: -298837.1\n",
      "                , loss1: -6779.8109375\n",
      "                , loss2: 23.73994140625\n",
      "=================================\n",
      "in 1000 epoch, average loss: -298424.5\n",
      "                , loss1: -6779.61015625\n",
      "                , loss2: 20.745167541503907\n",
      "=================================\n",
      "in 1010 epoch, average loss: -298029.725\n",
      "                , loss1: -6779.89296875\n",
      "                , loss2: 21.131065368652344\n",
      "=================================\n",
      "in 1020 epoch, average loss: -297641.65\n",
      "                , loss1: -6780.178125\n",
      "                , loss2: 14.97425537109375\n",
      "=================================\n",
      "in 1030 epoch, average loss: -297242.2\n",
      "                , loss1: -6780.26796875\n",
      "                , loss2: 11.558634948730468\n",
      "=================================\n",
      "in 1040 epoch, average loss: -296842.175\n",
      "                , loss1: -6780.45625\n",
      "                , loss2: 12.997055053710938\n",
      "=================================\n",
      "in 1050 epoch, average loss: -296436.9\n",
      "                , loss1: -6780.47265625\n",
      "                , loss2: 12.15313262939453\n",
      "=================================\n",
      "in 1060 epoch, average loss: -296035.0\n",
      "                , loss1: -6780.5203125\n",
      "                , loss2: 9.287215423583984\n",
      "=================================\n",
      "in 1070 epoch, average loss: -295626.25\n",
      "                , loss1: -6780.51875\n",
      "                , loss2: 11.159451293945313\n",
      "=================================\n",
      "in 1080 epoch, average loss: -295221.8\n",
      "                , loss1: -6780.61875\n",
      "                , loss2: 13.141705322265626\n",
      "=================================\n",
      "in 1090 epoch, average loss: -294823.2\n",
      "                , loss1: -6780.72265625\n",
      "                , loss2: 9.405685424804688\n",
      "=================================\n",
      "in 1100 epoch, average loss: -294416.35\n",
      "                , loss1: -6780.73125\n",
      "                , loss2: 9.806973266601563\n",
      "=================================\n",
      "in 1110 epoch, average loss: -294001.9\n",
      "                , loss1: -6780.5359375\n",
      "                , loss2: 8.947127532958984\n",
      "=================================\n",
      "in 1120 epoch, average loss: -293593.425\n",
      "                , loss1: -6780.53828125\n",
      "                , loss2: 10.651205444335938\n",
      "=================================\n",
      "in 1130 epoch, average loss: -293197.3\n",
      "                , loss1: -6780.7484375\n",
      "                , loss2: 9.030032348632812\n",
      "=================================\n",
      "in 1140 epoch, average loss: -292788.575\n",
      "                , loss1: -6780.70625\n",
      "                , loss2: 9.11447982788086\n",
      "=================================\n",
      "in 1150 epoch, average loss: -292384.1\n",
      "                , loss1: -6780.7703125\n",
      "                , loss2: 9.478173828125\n",
      "=================================\n",
      "in 1160 epoch, average loss: -291981.4\n",
      "                , loss1: -6780.7953125\n",
      "                , loss2: 6.445555114746094\n",
      "=================================\n",
      "in 1170 epoch, average loss: -291566.025\n",
      "                , loss1: -6780.64296875\n",
      "                , loss2: 8.420516204833984\n",
      "=================================\n",
      "in 1180 epoch, average loss: -291154.7\n",
      "                , loss1: -6780.553125\n",
      "                , loss2: 9.025829315185547\n",
      "=================================\n",
      "in 1190 epoch, average loss: -290754.7\n",
      "                , loss1: -6780.68671875\n",
      "                , loss2: 7.9155128479003904\n",
      "=================================\n",
      "in 1200 epoch, average loss: -290353.625\n",
      "                , loss1: -6780.7625\n",
      "                , loss2: 5.3777107238769535\n",
      "=================================\n",
      "in 1210 epoch, average loss: -289943.05\n",
      "                , loss1: -6780.6921875\n",
      "                , loss2: 6.114778900146485\n",
      "=================================\n",
      "in 1220 epoch, average loss: -289540.3\n",
      "                , loss1: -6780.8171875\n",
      "                , loss2: 7.397419738769531\n",
      "=================================\n",
      "in 1230 epoch, average loss: -289139.325\n",
      "                , loss1: -6780.95625\n",
      "                , loss2: 7.433373260498047\n",
      "=================================\n",
      "in 1240 epoch, average loss: -288734.1\n",
      "                , loss1: -6780.96484375\n",
      "                , loss2: 6.1590576171875\n",
      "=================================\n",
      "in 1250 epoch, average loss: -288330.925\n",
      "                , loss1: -6781.028125\n",
      "                , loss2: 5.1841388702392575\n",
      "=================================\n",
      "in 1260 epoch, average loss: -287924.25\n",
      "                , loss1: -6781.009375\n",
      "                , loss2: 4.186194610595703\n",
      "=================================\n",
      "in 1270 epoch, average loss: -287518.225\n",
      "                , loss1: -6781.0125\n",
      "                , loss2: 3.5056175231933593\n",
      "=================================\n",
      "in 1280 epoch, average loss: -287106.225\n",
      "                , loss1: -6780.90703125\n",
      "                , loss2: 4.171278381347657\n",
      "=================================\n",
      "in 1290 epoch, average loss: -286647.975\n",
      "                , loss1: -6779.86015625\n",
      "                , loss2: 11.319590759277343\n",
      "=================================\n",
      "in 1300 epoch, average loss: -286252.15\n",
      "                , loss1: -6780.25078125\n",
      "                , loss2: 16.854368591308592\n",
      "=================================\n",
      "in 1310 epoch, average loss: -285844.375\n",
      "                , loss1: -6780.134375\n",
      "                , loss2: 12.841751098632812\n",
      "=================================\n",
      "in 1320 epoch, average loss: -285456.375\n",
      "                , loss1: -6780.46640625\n",
      "                , loss2: 8.024683380126953\n",
      "=================================\n",
      "in 1330 epoch, average loss: -285055.55\n",
      "                , loss1: -6780.65859375\n",
      "                , loss2: 10.096088409423828\n",
      "=================================\n",
      "in 1340 epoch, average loss: -284652.825\n",
      "                , loss1: -6780.70625\n",
      "                , loss2: 7.997466278076172\n",
      "=================================\n",
      "in 1350 epoch, average loss: -284221.05\n",
      "                , loss1: -6780.21328125\n",
      "                , loss2: 12.258741760253907\n",
      "=================================\n",
      "in 1360 epoch, average loss: -283801.625\n",
      "                , loss1: -6779.9921875\n",
      "                , loss2: 15.612942504882813\n",
      "=================================\n",
      "in 1370 epoch, average loss: -283392.55\n",
      "                , loss1: -6779.896875\n",
      "                , loss2: 13.904373168945312\n",
      "=================================\n",
      "in 1380 epoch, average loss: -282983.825\n",
      "                , loss1: -6780.190625\n",
      "                , loss2: 28.122885131835936\n",
      "=================================\n",
      "in 1390 epoch, average loss: -282590.4\n",
      "                , loss1: -6780.3703125\n",
      "                , loss2: 22.190512084960936\n",
      "=================================\n",
      "in 1400 epoch, average loss: -282204.2\n",
      "                , loss1: -6780.9328125\n",
      "                , loss2: 24.988174438476562\n",
      "=================================\n",
      "in 1410 epoch, average loss: -281802.9\n",
      "                , loss1: -6780.99375\n",
      "                , loss2: 21.93660125732422\n",
      "=================================\n",
      "in 1420 epoch, average loss: -281395.425\n",
      "                , loss1: -6780.9625\n",
      "                , loss2: 21.301123046875\n",
      "=================================\n",
      "in 1430 epoch, average loss: -280990.75\n",
      "                , loss1: -6781.003125\n",
      "                , loss2: 20.809881591796874\n",
      "=================================\n",
      "in 1440 epoch, average loss: -280585.35\n",
      "                , loss1: -6781.01796875\n",
      "                , loss2: 19.968708801269532\n",
      "=================================\n",
      "in 1450 epoch, average loss: -280178.45\n",
      "                , loss1: -6781.04609375\n",
      "                , loss2: 21.2233642578125\n",
      "=================================\n",
      "in 1460 epoch, average loss: -279772.0\n",
      "                , loss1: -6781.0296875\n",
      "                , loss2: 20.04053955078125\n",
      "=================================\n",
      "in 1470 epoch, average loss: -279366.375\n",
      "                , loss1: -6781.04921875\n",
      "                , loss2: 19.633181762695312\n",
      "=================================\n",
      "in 1480 epoch, average loss: -278959.975\n",
      "                , loss1: -6781.084375\n",
      "                , loss2: 20.661639404296874\n",
      "=================================\n",
      "in 1490 epoch, average loss: -278548.975\n",
      "                , loss1: -6780.990625\n",
      "                , loss2: 20.91571502685547\n",
      "=================================\n",
      "in 1500 epoch, average loss: -278143.725\n",
      "                , loss1: -6781.06875\n",
      "                , loss2: 22.51529541015625\n",
      "=================================\n",
      "in 1510 epoch, average loss: -277736.3\n",
      "                , loss1: -6780.9875\n",
      "                , loss2: 19.682801818847658\n",
      "=================================\n",
      "in 1520 epoch, average loss: -277332.5\n",
      "                , loss1: -6781.075\n",
      "                , loss2: 20.248876953125\n",
      "=================================\n",
      "in 1530 epoch, average loss: -276924.15\n",
      "                , loss1: -6781.0484375\n",
      "                , loss2: 20.61697998046875\n",
      "=================================\n",
      "in 1540 epoch, average loss: -276521.3\n",
      "                , loss1: -6781.0984375\n",
      "                , loss2: 18.676614379882814\n",
      "=================================\n",
      "in 1550 epoch, average loss: -276115.425\n",
      "                , loss1: -6781.146875\n",
      "                , loss2: 19.625942993164063\n",
      "=================================\n",
      "in 1560 epoch, average loss: -275709.225\n",
      "                , loss1: -6781.18515625\n",
      "                , loss2: 20.499537658691406\n",
      "=================================\n",
      "in 1570 epoch, average loss: -275302.225\n",
      "                , loss1: -6781.1578125\n",
      "                , loss2: 19.53583526611328\n",
      "=================================\n",
      "in 1580 epoch, average loss: -274892.925\n",
      "                , loss1: -6781.1609375\n",
      "                , loss2: 22.161965942382814\n",
      "=================================\n",
      "in 1590 epoch, average loss: -274489.7\n",
      "                , loss1: -6781.22265625\n",
      "                , loss2: 20.954428100585936\n",
      "=================================\n",
      "in 1600 epoch, average loss: -274082.325\n",
      "                , loss1: -6781.17578125\n",
      "                , loss2: 19.610345458984376\n",
      "=================================\n",
      "in 1610 epoch, average loss: -273668.725\n",
      "                , loss1: -6781.0515625\n",
      "                , loss2: 21.3164306640625\n",
      "=================================\n",
      "in 1620 epoch, average loss: -273265.05\n",
      "                , loss1: -6781.06640625\n",
      "                , loss2: 18.742410278320314\n",
      "=================================\n",
      "in 1630 epoch, average loss: -272861.95\n",
      "                , loss1: -6781.14921875\n",
      "                , loss2: 18.294979858398438\n",
      "=================================\n",
      "in 1640 epoch, average loss: -272453.425\n",
      "                , loss1: -6781.11640625\n",
      "                , loss2: 18.625421142578126\n",
      "=================================\n",
      "in 1650 epoch, average loss: -272047.475\n",
      "                , loss1: -6781.178125\n",
      "                , loss2: 20.128636169433594\n",
      "=================================\n",
      "in 1660 epoch, average loss: -271638.1\n",
      "                , loss1: -6781.06796875\n",
      "                , loss2: 18.275473022460936\n",
      "=================================\n",
      "in 1670 epoch, average loss: -271224.625\n",
      "                , loss1: -6780.934375\n",
      "                , loss2: 19.552362060546876\n",
      "=================================\n",
      "in 1680 epoch, average loss: -270823.3\n",
      "                , loss1: -6781.071875\n",
      "                , loss2: 19.470602416992186\n",
      "=================================\n",
      "in 1690 epoch, average loss: -270413.375\n",
      "                , loss1: -6780.9875\n",
      "                , loss2: 19.178517150878907\n",
      "=================================\n",
      "in 1700 epoch, average loss: -270012.4\n",
      "                , loss1: -6781.11796875\n",
      "                , loss2: 18.494635009765624\n",
      "=================================\n",
      "in 1710 epoch, average loss: -269606.825\n",
      "                , loss1: -6781.17421875\n",
      "                , loss2: 19.482679748535155\n",
      "=================================\n",
      "in 1720 epoch, average loss: -269197.3\n",
      "                , loss1: -6781.12890625\n",
      "                , loss2: 20.274774169921876\n",
      "=================================\n",
      "in 1730 epoch, average loss: -268788.75\n",
      "                , loss1: -6781.03203125\n",
      "                , loss2: 18.09967498779297\n",
      "=================================\n",
      "in 1740 epoch, average loss: -268382.475\n",
      "                , loss1: -6781.0859375\n",
      "                , loss2: 19.686260986328126\n",
      "=================================\n",
      "in 1750 epoch, average loss: -267978.925\n",
      "                , loss1: -6781.153125\n",
      "                , loss2: 18.999806213378907\n",
      "=================================\n",
      "in 1760 epoch, average loss: -267574.3\n",
      "                , loss1: -6781.18359375\n",
      "                , loss2: 17.978512573242188\n",
      "=================================\n",
      "in 1770 epoch, average loss: -267165.8\n",
      "                , loss1: -6781.1421875\n",
      "                , loss2: 17.965731811523437\n",
      "=================================\n",
      "in 1780 epoch, average loss: -266757.425\n",
      "                , loss1: -6781.14453125\n",
      "                , loss2: 19.583663940429688\n",
      "=================================\n",
      "in 1790 epoch, average loss: -266347.225\n",
      "                , loss1: -6780.97109375\n",
      "                , loss2: 16.08564453125\n",
      "=================================\n",
      "in 1800 epoch, average loss: -265943.55\n",
      "                , loss1: -6781.08828125\n",
      "                , loss2: 17.540481567382812\n",
      "=================================\n",
      "in 1810 epoch, average loss: -265470.15\n",
      "                , loss1: -6779.80859375\n",
      "                , loss2: 33.958657836914064\n",
      "=================================\n",
      "in 1820 epoch, average loss: -265053.475\n",
      "                , loss1: -6779.3515625\n",
      "                , loss2: 25.927691650390624\n",
      "=================================\n",
      "in 1830 epoch, average loss: -264359.325\n",
      "                , loss1: -6776.1265625\n",
      "                , loss2: 187.560791015625\n",
      "=================================\n",
      "in 1840 epoch, average loss: -262862.275\n",
      "                , loss1: -6754.590625\n",
      "                , loss2: 438.51376953125\n",
      "=================================\n",
      "in 1850 epoch, average loss: -263180.9\n",
      "                , loss1: -6765.2640625\n",
      "                , loss2: 129.87349853515624\n",
      "=================================\n",
      "in 1860 epoch, average loss: -263284.0\n",
      "                , loss1: -6775.41796875\n",
      "                , loss2: 15.478363037109375\n",
      "=================================\n",
      "in 1870 epoch, average loss: -263050.65\n",
      "                , loss1: -6780.1140625\n",
      "                , loss2: 24.614332580566405\n",
      "=================================\n",
      "in 1880 epoch, average loss: -262673.375\n",
      "                , loss1: -6780.92265625\n",
      "                , loss2: 26.36256103515625\n",
      "=================================\n",
      "in 1890 epoch, average loss: -262281.775\n",
      "                , loss1: -6781.32734375\n",
      "                , loss2: 26.747894287109375\n",
      "=================================\n",
      "in 1900 epoch, average loss: -261877.725\n",
      "                , loss1: -6781.365625\n",
      "                , loss2: 25.403366088867188\n",
      "=================================\n",
      "in 1910 epoch, average loss: -261477.925\n",
      "                , loss1: -6781.4875\n",
      "                , loss2: 23.026651000976564\n",
      "=================================\n",
      "in 1920 epoch, average loss: -261081.4\n",
      "                , loss1: -6781.68515625\n",
      "                , loss2: 20.251705932617188\n",
      "=================================\n",
      "in 1930 epoch, average loss: -260669.6\n",
      "                , loss1: -6781.5875\n",
      "                , loss2: 21.377536010742187\n",
      "=================================\n",
      "in 1940 epoch, average loss: -260261.65\n",
      "                , loss1: -6781.5703125\n",
      "                , loss2: 21.79840545654297\n",
      "=================================\n",
      "in 1950 epoch, average loss: -259862.375\n",
      "                , loss1: -6781.71328125\n",
      "                , loss2: 19.66333923339844\n",
      "=================================\n",
      "in 1960 epoch, average loss: -259455.5\n",
      "                , loss1: -6781.7671875\n",
      "                , loss2: 21.672097778320314\n",
      "=================================\n",
      "in 1970 epoch, average loss: -259052.125\n",
      "                , loss1: -6781.83359375\n",
      "                , loss2: 20.696066284179686\n",
      "=================================\n",
      "in 1980 epoch, average loss: -258644.475\n",
      "                , loss1: -6781.80703125\n",
      "                , loss2: 20.445088195800782\n",
      "=================================\n",
      "in 1990 epoch, average loss: -258239.875\n",
      "                , loss1: -6781.8421875\n",
      "                , loss2: 19.426547241210937\n",
      "=================================\n",
      "in 2000 epoch, average loss: -257834.75\n",
      "                , loss1: -6781.86328125\n",
      "                , loss2: 18.509982299804687\n",
      "=================================\n",
      "in 2010 epoch, average loss: -257428.9\n",
      "                , loss1: -6781.896875\n",
      "                , loss2: 18.7209228515625\n",
      "=================================\n",
      "in 2020 epoch, average loss: -257021.775\n",
      "                , loss1: -6781.88046875\n",
      "                , loss2: 18.277500915527344\n",
      "=================================\n",
      "in 2030 epoch, average loss: -256607.975\n",
      "                , loss1: -6781.70625\n",
      "                , loss2: 18.54888000488281\n",
      "=================================\n",
      "in 2040 epoch, average loss: -256203.0\n",
      "                , loss1: -6781.8328125\n",
      "                , loss2: 21.437818908691405\n",
      "=================================\n",
      "in 2050 epoch, average loss: -255797.825\n",
      "                , loss1: -6781.85\n",
      "                , loss2: 20.344970703125\n",
      "=================================\n",
      "in 2060 epoch, average loss: -255394.1\n",
      "                , loss1: -6781.8828125\n",
      "                , loss2: 18.38929443359375\n",
      "=================================\n",
      "in 2070 epoch, average loss: -254983.725\n",
      "                , loss1: -6781.8640625\n",
      "                , loss2: 21.175331115722656\n",
      "=================================\n",
      "in 2080 epoch, average loss: -254581.55\n",
      "                , loss1: -6781.9203125\n",
      "                , loss2: 18.544029235839844\n",
      "=================================\n",
      "in 2090 epoch, average loss: -254175.2\n",
      "                , loss1: -6781.9328125\n",
      "                , loss2: 18.422885131835937\n",
      "=================================\n",
      "in 2100 epoch, average loss: -253768.0\n",
      "                , loss1: -6781.934375\n",
      "                , loss2: 18.784585571289064\n",
      "=================================\n",
      "in 2110 epoch, average loss: -253361.45\n",
      "                , loss1: -6781.946875\n",
      "                , loss2: 18.8569091796875\n",
      "=================================\n",
      "in 2120 epoch, average loss: -252954.775\n",
      "                , loss1: -6781.953125\n",
      "                , loss2: 18.84405059814453\n",
      "=================================\n",
      "in 2130 epoch, average loss: -252546.4\n",
      "                , loss1: -6781.9125\n",
      "                , loss2: 18.78948211669922\n",
      "=================================\n",
      "in 2140 epoch, average loss: -252139.975\n",
      "                , loss1: -6781.921875\n",
      "                , loss2: 18.671026611328124\n",
      "=================================\n",
      "in 2150 epoch, average loss: -251734.375\n",
      "                , loss1: -6781.96796875\n",
      "                , loss2: 19.051123046875\n",
      "=================================\n",
      "in 2160 epoch, average loss: -251327.85\n",
      "                , loss1: -6781.96484375\n",
      "                , loss2: 18.555697631835937\n",
      "=================================\n",
      "in 2170 epoch, average loss: -250921.95\n",
      "                , loss1: -6781.9796875\n",
      "                , loss2: 18.07095947265625\n",
      "=================================\n",
      "in 2180 epoch, average loss: -250515.0\n",
      "                , loss1: -6781.9890625\n",
      "                , loss2: 18.43392333984375\n",
      "=================================\n",
      "in 2190 epoch, average loss: -250107.35\n",
      "                , loss1: -6781.990625\n",
      "                , loss2: 19.262281799316405\n",
      "=================================\n",
      "in 2200 epoch, average loss: -249699.625\n",
      "                , loss1: -6781.953125\n",
      "                , loss2: 18.69139709472656\n",
      "=================================\n",
      "in 2210 epoch, average loss: -249294.075\n",
      "                , loss1: -6781.98046875\n",
      "                , loss2: 18.29047546386719\n",
      "=================================\n",
      "in 2220 epoch, average loss: -248887.5\n",
      "                , loss1: -6782.00078125\n",
      "                , loss2: 18.741481018066406\n",
      "=================================\n",
      "in 2230 epoch, average loss: -248481.225\n",
      "                , loss1: -6781.99921875\n",
      "                , loss2: 18.00188446044922\n",
      "=================================\n",
      "in 2240 epoch, average loss: -248073.325\n",
      "                , loss1: -6781.99453125\n",
      "                , loss2: 18.82650146484375\n",
      "=================================\n",
      "in 2250 epoch, average loss: -247667.225\n",
      "                , loss1: -6781.98359375\n",
      "                , loss2: 17.56802978515625\n",
      "=================================\n",
      "in 2260 epoch, average loss: -247260.525\n",
      "                , loss1: -6782.00625\n",
      "                , loss2: 18.209619140625\n",
      "=================================\n",
      "in 2270 epoch, average loss: -246854.15\n",
      "                , loss1: -6782.00625\n",
      "                , loss2: 17.67469940185547\n",
      "=================================\n",
      "in 2280 epoch, average loss: -246446.875\n",
      "                , loss1: -6782.021875\n",
      "                , loss2: 18.59227600097656\n",
      "=================================\n",
      "in 2290 epoch, average loss: -246040.125\n",
      "                , loss1: -6782.015625\n",
      "                , loss2: 18.181732177734375\n",
      "=================================\n",
      "in 2300 epoch, average loss: -245633.625\n",
      "                , loss1: -6782.02421875\n",
      "                , loss2: 18.078897094726564\n",
      "=================================\n",
      "in 2310 epoch, average loss: -245224.875\n",
      "                , loss1: -6781.96796875\n",
      "                , loss2: 17.845596313476562\n",
      "=================================\n",
      "in 2320 epoch, average loss: -244819.1\n",
      "                , loss1: -6782.009375\n",
      "                , loss2: 18.241619873046876\n",
      "=================================\n",
      "in 2330 epoch, average loss: -244410.925\n",
      "                , loss1: -6781.98046875\n",
      "                , loss2: 18.4294921875\n",
      "=================================\n",
      "in 2340 epoch, average loss: -244005.575\n",
      "                , loss1: -6782.01484375\n",
      "                , loss2: 18.09681396484375\n",
      "=================================\n",
      "in 2350 epoch, average loss: -243599.425\n",
      "                , loss1: -6782.028125\n",
      "                , loss2: 17.79537353515625\n",
      "=================================\n",
      "in 2360 epoch, average loss: -243192.15\n",
      "                , loss1: -6782.02578125\n",
      "                , loss2: 18.078799438476562\n",
      "=================================\n",
      "in 2370 epoch, average loss: -242785.3\n",
      "                , loss1: -6782.01484375\n",
      "                , loss2: 17.584347534179688\n",
      "=================================\n",
      "in 2380 epoch, average loss: -242376.35\n",
      "                , loss1: -6781.98203125\n",
      "                , loss2: 18.48079071044922\n",
      "=================================\n",
      "in 2390 epoch, average loss: -241970.375\n",
      "                , loss1: -6781.98359375\n",
      "                , loss2: 17.6005126953125\n",
      "=================================\n",
      "in 2400 epoch, average loss: -241562.125\n",
      "                , loss1: -6781.96875\n",
      "                , loss2: 18.41265869140625\n",
      "=================================\n",
      "in 2410 epoch, average loss: -241158.65\n",
      "                , loss1: -6782.04296875\n",
      "                , loss2: 17.565679931640624\n",
      "=================================\n",
      "in 2420 epoch, average loss: -240751.175\n",
      "                , loss1: -6782.01640625\n",
      "                , loss2: 17.18875732421875\n",
      "=================================\n",
      "in 2430 epoch, average loss: -240344.075\n",
      "                , loss1: -6782.0390625\n",
      "                , loss2: 18.203337097167967\n",
      "=================================\n",
      "in 2440 epoch, average loss: -239937.925\n",
      "                , loss1: -6782.03203125\n",
      "                , loss2: 17.148194885253908\n",
      "=================================\n",
      "in 2450 epoch, average loss: -239530.225\n",
      "                , loss1: -6782.02578125\n",
      "                , loss2: 17.718905639648437\n",
      "=================================\n",
      "in 2460 epoch, average loss: -239124.025\n",
      "                , loss1: -6781.9609375\n",
      "                , loss2: 14.697306823730468\n",
      "=================================\n",
      "in 2470 epoch, average loss: -238715.65\n",
      "                , loss1: -6782.0109375\n",
      "                , loss2: 17.88524627685547\n",
      "=================================\n",
      "in 2480 epoch, average loss: -238309.7\n",
      "                , loss1: -6782.015625\n",
      "                , loss2: 17.134420776367186\n",
      "=================================\n",
      "in 2490 epoch, average loss: -237903.775\n",
      "                , loss1: -6782.03046875\n",
      "                , loss2: 16.644480895996093\n",
      "=================================\n",
      "in 2500 epoch, average loss: -237494.65\n",
      "                , loss1: -6781.978125\n",
      "                , loss2: 17.008456420898437\n",
      "=================================\n",
      "in 2510 epoch, average loss: -237083.5\n",
      "                , loss1: -6781.80625\n",
      "                , loss2: 15.222073364257813\n",
      "=================================\n",
      "in 2520 epoch, average loss: -236680.975\n",
      "                , loss1: -6781.990625\n",
      "                , loss2: 17.301588439941405\n",
      "=================================\n",
      "in 2530 epoch, average loss: -236275.7\n",
      "                , loss1: -6782.02890625\n",
      "                , loss2: 16.97063293457031\n",
      "=================================\n",
      "in 2540 epoch, average loss: -235867.4\n",
      "                , loss1: -6781.928125\n",
      "                , loss2: 14.837753295898438\n",
      "=================================\n",
      "in 2550 epoch, average loss: -235456.4\n",
      "                , loss1: -6781.81640625\n",
      "                , loss2: 15.023805236816406\n",
      "=================================\n",
      "in 2560 epoch, average loss: -235050.275\n",
      "                , loss1: -6781.865625\n",
      "                , loss2: 15.931475830078124\n",
      "=================================\n",
      "in 2570 epoch, average loss: -234647.65\n",
      "                , loss1: -6782.02734375\n",
      "                , loss2: 17.260639953613282\n",
      "=================================\n",
      "in 2580 epoch, average loss: -234241.125\n",
      "                , loss1: -6781.94921875\n",
      "                , loss2: 14.215802001953126\n",
      "=================================\n",
      "in 2590 epoch, average loss: -233836.9\n",
      "                , loss1: -6782.0359375\n",
      "                , loss2: 14.485787963867187\n",
      "=================================\n",
      "in 2600 epoch, average loss: -233429.775\n",
      "                , loss1: -6781.9703125\n",
      "                , loss2: 12.414957427978516\n",
      "=================================\n",
      "in 2610 epoch, average loss: -233011.275\n",
      "                , loss1: -6781.5546875\n",
      "                , loss2: 9.737750244140624\n",
      "=================================\n",
      "in 2620 epoch, average loss: -232615.0\n",
      "                , loss1: -6782.00625\n",
      "                , loss2: 14.595736694335937\n",
      "=================================\n",
      "in 2630 epoch, average loss: -232200.75\n",
      "                , loss1: -6781.73828125\n",
      "                , loss2: 12.740719604492188\n",
      "=================================\n",
      "in 2640 epoch, average loss: -231799.675\n",
      "                , loss1: -6781.88671875\n",
      "                , loss2: 11.981098937988282\n",
      "=================================\n",
      "in 2650 epoch, average loss: -231391.425\n",
      "                , loss1: -6781.846875\n",
      "                , loss2: 11.963062286376953\n",
      "=================================\n",
      "in 2660 epoch, average loss: -230979.475\n",
      "                , loss1: -6781.6890625\n",
      "                , loss2: 11.652128601074219\n",
      "=================================\n",
      "in 2670 epoch, average loss: -230578.2\n",
      "                , loss1: -6781.8515625\n",
      "                , loss2: 11.511576843261718\n",
      "=================================\n",
      "in 2680 epoch, average loss: -230173.575\n",
      "                , loss1: -6781.97734375\n",
      "                , loss2: 13.51175537109375\n",
      "=================================\n",
      "in 2690 epoch, average loss: -229768.1\n",
      "                , loss1: -6781.9828125\n",
      "                , loss2: 12.260316467285156\n",
      "=================================\n",
      "in 2700 epoch, average loss: -229364.85\n",
      "                , loss1: -6781.99609375\n",
      "                , loss2: 9.027731323242188\n",
      "=================================\n",
      "in 2710 epoch, average loss: -228956.825\n",
      "                , loss1: -6782.0140625\n",
      "                , loss2: 10.774002075195312\n",
      "=================================\n",
      "in 2720 epoch, average loss: -228549.0\n",
      "                , loss1: -6781.97421875\n",
      "                , loss2: 10.320013427734375\n",
      "=================================\n",
      "in 2730 epoch, average loss: -228143.275\n",
      "                , loss1: -6781.9875\n",
      "                , loss2: 9.548895263671875\n",
      "=================================\n",
      "in 2740 epoch, average loss: -227736.5\n",
      "                , loss1: -6782.03125\n",
      "                , loss2: 10.885298156738282\n",
      "=================================\n",
      "in 2750 epoch, average loss: -227330.9\n",
      "                , loss1: -6782.01171875\n",
      "                , loss2: 8.930601501464844\n",
      "=================================\n",
      "in 2760 epoch, average loss: -226920.0\n",
      "                , loss1: -6781.91171875\n",
      "                , loss2: 9.56915740966797\n",
      "=================================\n",
      "in 2770 epoch, average loss: -226516.825\n",
      "                , loss1: -6782.015625\n",
      "                , loss2: 9.256790924072266\n",
      "=================================\n",
      "in 2780 epoch, average loss: -226108.85\n",
      "                , loss1: -6781.9921875\n",
      "                , loss2: 9.526293182373047\n",
      "=================================\n",
      "in 2790 epoch, average loss: -225701.8\n",
      "                , loss1: -6782.05625\n",
      "                , loss2: 11.844697570800781\n",
      "=================================\n",
      "in 2800 epoch, average loss: -225294.65\n",
      "                , loss1: -6781.9640625\n",
      "                , loss2: 8.97596435546875\n",
      "=================================\n",
      "in 2810 epoch, average loss: -224889.1\n",
      "                , loss1: -6781.9875\n",
      "                , loss2: 8.394423675537109\n",
      "=================================\n",
      "in 2820 epoch, average loss: -224481.7\n",
      "                , loss1: -6782.0328125\n",
      "                , loss2: 10.391720581054688\n",
      "=================================\n",
      "in 2830 epoch, average loss: -224075.475\n",
      "                , loss1: -6782.00234375\n",
      "                , loss2: 8.642340850830077\n",
      "=================================\n",
      "in 2840 epoch, average loss: -223667.725\n",
      "                , loss1: -6781.98671875\n",
      "                , loss2: 8.984325408935547\n",
      "=================================\n",
      "in 2850 epoch, average loss: -223262.4\n",
      "                , loss1: -6782.02734375\n",
      "                , loss2: 8.721643829345703\n",
      "=================================\n",
      "in 2860 epoch, average loss: -222855.8\n",
      "                , loss1: -6782.0234375\n",
      "                , loss2: 8.241209411621094\n",
      "=================================\n",
      "in 2870 epoch, average loss: -222449.125\n",
      "                , loss1: -6782.0609375\n",
      "                , loss2: 9.273057556152343\n",
      "=================================\n",
      "in 2880 epoch, average loss: -222039.725\n",
      "                , loss1: -6782.01015625\n",
      "                , loss2: 10.096725463867188\n",
      "=================================\n",
      "in 2890 epoch, average loss: -221634.95\n",
      "                , loss1: -6782.03203125\n",
      "                , loss2: 8.642266082763673\n",
      "=================================\n",
      "in 2900 epoch, average loss: -221228.575\n",
      "                , loss1: -6782.0546875\n",
      "                , loss2: 8.833680725097656\n",
      "=================================\n",
      "in 2910 epoch, average loss: -220818.15\n",
      "                , loss1: -6781.9515625\n",
      "                , loss2: 8.982044982910157\n",
      "=================================\n",
      "in 2920 epoch, average loss: -220413.85\n",
      "                , loss1: -6782.0453125\n",
      "                , loss2: 9.381704711914063\n",
      "=================================\n",
      "in 2930 epoch, average loss: -220007.85\n",
      "                , loss1: -6782.03125\n",
      "                , loss2: 8.023204803466797\n",
      "=================================\n",
      "in 2940 epoch, average loss: -219555.575\n",
      "                , loss1: -6780.8765625\n",
      "                , loss2: 16.02099151611328\n",
      "=================================\n",
      "in 2950 epoch, average loss: -219165.3\n",
      "                , loss1: -6781.4984375\n",
      "                , loss2: 19.496849060058594\n",
      "=================================\n",
      "in 2960 epoch, average loss: -218762.4\n",
      "                , loss1: -6781.54140625\n",
      "                , loss2: 16.925444030761717\n",
      "=================================\n",
      "in 2970 epoch, average loss: -218352.075\n",
      "                , loss1: -6781.43125\n",
      "                , loss2: 16.799835205078125\n",
      "=================================\n",
      "in 2980 epoch, average loss: -217963.625\n",
      "                , loss1: -6781.7890625\n",
      "                , loss2: 9.863970947265624\n",
      "=================================\n",
      "in 2990 epoch, average loss: -217562.225\n",
      "                , loss1: -6782.02890625\n",
      "                , loss2: 12.049928283691406\n",
      "=================================\n",
      "in 3000 epoch, average loss: -217154.8\n",
      "                , loss1: -6781.928125\n",
      "                , loss2: 9.314027404785156\n",
      "=================================\n",
      "in 3010 epoch, average loss: -216750.15\n",
      "                , loss1: -6781.99140625\n",
      "                , loss2: 9.07977294921875\n",
      "=================================\n",
      "in 3020 epoch, average loss: -216344.125\n",
      "                , loss1: -6782.0234375\n",
      "                , loss2: 9.22075958251953\n",
      "=================================\n",
      "in 3030 epoch, average loss: -215937.775\n",
      "                , loss1: -6782.0125\n",
      "                , loss2: 8.28775405883789\n",
      "=================================\n",
      "in 3040 epoch, average loss: -215531.125\n",
      "                , loss1: -6782.02421875\n",
      "                , loss2: 8.384876251220703\n",
      "=================================\n",
      "in 3050 epoch, average loss: -215124.125\n",
      "                , loss1: -6782.0421875\n",
      "                , loss2: 9.03098373413086\n",
      "=================================\n",
      "in 3060 epoch, average loss: -214716.65\n",
      "                , loss1: -6782.046875\n",
      "                , loss2: 9.739395141601562\n",
      "=================================\n",
      "in 3070 epoch, average loss: -214308.1\n",
      "                , loss1: -6781.96484375\n",
      "                , loss2: 8.751478576660157\n",
      "=================================\n",
      "in 3080 epoch, average loss: -213903.7\n",
      "                , loss1: -6782.0328125\n",
      "                , loss2: 8.397286987304687\n",
      "=================================\n",
      "in 3090 epoch, average loss: -213497.1\n",
      "                , loss1: -6782.0375\n",
      "                , loss2: 8.228236389160156\n",
      "=================================\n",
      "in 3100 epoch, average loss: -213089.975\n",
      "                , loss1: -6782.05078125\n",
      "                , loss2: 8.832672119140625\n",
      "=================================\n",
      "in 3110 epoch, average loss: -212682.275\n",
      "                , loss1: -6782.01875\n",
      "                , loss2: 8.632087707519531\n",
      "=================================\n",
      "in 3120 epoch, average loss: -212276.625\n",
      "                , loss1: -6782.0453125\n",
      "                , loss2: 8.167795562744141\n",
      "=================================\n",
      "in 3130 epoch, average loss: -211869.875\n",
      "                , loss1: -6782.05546875\n",
      "                , loss2: 8.326914978027343\n",
      "=================================\n",
      "in 3140 epoch, average loss: -211463.75\n",
      "                , loss1: -6782.06484375\n",
      "                , loss2: 7.810009765625\n",
      "=================================\n",
      "in 3150 epoch, average loss: -211056.1\n",
      "                , loss1: -6782.06484375\n",
      "                , loss2: 8.541295623779297\n",
      "=================================\n",
      "in 3160 epoch, average loss: -210649.225\n",
      "                , loss1: -6782.0625\n",
      "                , loss2: 8.441976165771484\n",
      "=================================\n",
      "in 3170 epoch, average loss: -210241.6\n",
      "                , loss1: -6782.0484375\n",
      "                , loss2: 8.704464721679688\n",
      "=================================\n",
      "in 3180 epoch, average loss: -209835.9\n",
      "                , loss1: -6782.02578125\n",
      "                , loss2: 6.768514251708984\n",
      "=================================\n",
      "in 3190 epoch, average loss: -209432.0375\n",
      "                , loss1: -6782.0046875\n",
      "                , loss2: 3.0505039215087892\n",
      "=================================\n",
      "in 3200 epoch, average loss: -209025.9875\n",
      "                , loss1: -6781.96953125\n",
      "                , loss2: 1.0805943489074707\n",
      "=================================\n",
      "in 3210 epoch, average loss: -208620.225\n",
      "                , loss1: -6781.97890625\n",
      "                , loss2: 0.24916014671325684\n",
      "=================================\n",
      "in 3220 epoch, average loss: -208213.4375\n",
      "                , loss1: -6781.98125\n",
      "                , loss2: 0.17079370021820067\n",
      "=================================\n",
      "in 3230 epoch, average loss: -207802.0125\n",
      "                , loss1: -6781.87265625\n",
      "                , loss2: 1.3557094573974608\n",
      "=================================\n",
      "in 3240 epoch, average loss: -207393.1\n",
      "                , loss1: -6781.828125\n",
      "                , loss2: 1.9878427505493164\n",
      "=================================\n",
      "in 3250 epoch, average loss: -206988.8375\n",
      "                , loss1: -6782.00703125\n",
      "                , loss2: 4.804637908935547\n",
      "=================================\n",
      "in 3260 epoch, average loss: -206589.725\n",
      "                , loss1: -6782.99375\n",
      "                , loss2: 27.044842529296876\n",
      "=================================\n",
      "in 3270 epoch, average loss: -206182.15\n",
      "                , loss1: -6783.278125\n",
      "                , loss2: 36.27088623046875\n",
      "=================================\n",
      "in 3280 epoch, average loss: -205792.975\n",
      "                , loss1: -6783.47265625\n",
      "                , loss2: 24.396063232421874\n",
      "=================================\n",
      "in 3290 epoch, average loss: -205389.4\n",
      "                , loss1: -6783.51640625\n",
      "                , loss2: 22.27308349609375\n",
      "=================================\n",
      "in 3300 epoch, average loss: -204984.2\n",
      "                , loss1: -6783.5484375\n",
      "                , loss2: 21.430081176757813\n",
      "=================================\n",
      "in 3310 epoch, average loss: -204584.425\n",
      "                , loss1: -6783.70859375\n",
      "                , loss2: 19.00402374267578\n",
      "=================================\n",
      "in 3320 epoch, average loss: -204173.95\n",
      "                , loss1: -6783.66875\n",
      "                , loss2: 21.239254760742188\n",
      "=================================\n",
      "in 3330 epoch, average loss: -203768.6375\n",
      "                , loss1: -6783.60234375\n",
      "                , loss2: 17.56067810058594\n",
      "=================================\n",
      "in 3340 epoch, average loss: -203359.6125\n",
      "                , loss1: -6783.6171875\n",
      "                , loss2: 20.023138427734374\n",
      "=================================\n",
      "in 3350 epoch, average loss: -202956.775\n",
      "                , loss1: -6783.71640625\n",
      "                , loss2: 18.809071350097657\n",
      "=================================\n",
      "in 3360 epoch, average loss: -202549.0\n",
      "                , loss1: -6783.6953125\n",
      "                , loss2: 18.917935180664063\n",
      "=================================\n",
      "in 3370 epoch, average loss: -202143.6625\n",
      "                , loss1: -6783.73984375\n",
      "                , loss2: 18.561982727050783\n",
      "=================================\n",
      "in 3380 epoch, average loss: -201656.4375\n",
      "                , loss1: -6780.52734375\n",
      "                , loss2: 3.2346988677978517\n",
      "=================================\n",
      "in 3390 epoch, average loss: -201287.3375\n",
      "                , loss1: -6782.46640625\n",
      "                , loss2: 23.024551391601562\n",
      "=================================\n",
      "in 3400 epoch, average loss: -200902.3375\n",
      "                , loss1: -6783.215625\n",
      "                , loss2: 23.273579406738282\n",
      "=================================\n",
      "in 3410 epoch, average loss: -200496.9125\n",
      "                , loss1: -6783.1734375\n",
      "                , loss2: 20.500485229492188\n",
      "=================================\n",
      "in 3420 epoch, average loss: -200095.3125\n",
      "                , loss1: -6783.39296875\n",
      "                , loss2: 21.54602813720703\n",
      "=================================\n",
      "in 3430 epoch, average loss: -199690.1625\n",
      "                , loss1: -6783.30546875\n",
      "                , loss2: 17.135662841796876\n",
      "=================================\n",
      "in 3440 epoch, average loss: -199283.7125\n",
      "                , loss1: -6783.38046875\n",
      "                , loss2: 18.81632843017578\n",
      "=================================\n",
      "in 3450 epoch, average loss: -198882.25\n",
      "                , loss1: -6783.684375\n",
      "                , loss2: 22.14434051513672\n",
      "=================================\n",
      "in 3460 epoch, average loss: -198476.8125\n",
      "                , loss1: -6783.6171875\n",
      "                , loss2: 18.62389373779297\n",
      "=================================\n",
      "in 3470 epoch, average loss: -198070.2625\n",
      "                , loss1: -6783.6453125\n",
      "                , loss2: 18.975871276855468\n",
      "=================================\n",
      "in 3480 epoch, average loss: -197664.675\n",
      "                , loss1: -6783.70234375\n",
      "                , loss2: 19.197727966308594\n",
      "=================================\n",
      "in 3490 epoch, average loss: -197257.725\n",
      "                , loss1: -6783.6734375\n",
      "                , loss2: 18.286405944824217\n",
      "=================================\n",
      "in 3500 epoch, average loss: -196849.0875\n",
      "                , loss1: -6783.60078125\n",
      "                , loss2: 17.786936950683593\n",
      "=================================\n",
      "in 3510 epoch, average loss: -196443.125\n",
      "                , loss1: -6783.71875\n",
      "                , loss2: 20.153656005859375\n",
      "=================================\n",
      "in 3520 epoch, average loss: -196037.825\n",
      "                , loss1: -6783.703125\n",
      "                , loss2: 17.982502746582032\n",
      "=================================\n",
      "in 3530 epoch, average loss: -195630.275\n",
      "                , loss1: -6783.72109375\n",
      "                , loss2: 19.024874877929687\n",
      "=================================\n",
      "in 3540 epoch, average loss: -195223.6125\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 18.250125122070312\n",
      "=================================\n",
      "in 3550 epoch, average loss: -194815.9\n",
      "                , loss1: -6783.71796875\n",
      "                , loss2: 19.252952575683594\n",
      "=================================\n",
      "in 3560 epoch, average loss: -194409.4625\n",
      "                , loss1: -6783.69609375\n",
      "                , loss2: 18.06822967529297\n",
      "=================================\n",
      "in 3570 epoch, average loss: -194002.525\n",
      "                , loss1: -6783.72578125\n",
      "                , loss2: 18.808230590820312\n",
      "=================================\n",
      "in 3580 epoch, average loss: -193594.6625\n",
      "                , loss1: -6783.7140625\n",
      "                , loss2: 19.331546020507812\n",
      "=================================\n",
      "in 3590 epoch, average loss: -193188.5875\n",
      "                , loss1: -6783.7015625\n",
      "                , loss2: 18.03638458251953\n",
      "=================================\n",
      "in 3600 epoch, average loss: -192782.0125\n",
      "                , loss1: -6783.7359375\n",
      "                , loss2: 18.58905792236328\n",
      "=================================\n",
      "in 3610 epoch, average loss: -192375.7125\n",
      "                , loss1: -6783.75\n",
      "                , loss2: 18.253385925292967\n",
      "=================================\n",
      "in 3620 epoch, average loss: -191968.6125\n",
      "                , loss1: -6783.75390625\n",
      "                , loss2: 18.381890869140626\n",
      "=================================\n",
      "in 3630 epoch, average loss: -191560.8625\n",
      "                , loss1: -6783.71875\n",
      "                , loss2: 18.153131103515626\n",
      "=================================\n",
      "in 3640 epoch, average loss: -191153.7875\n",
      "                , loss1: -6783.7328125\n",
      "                , loss2: 18.585417175292967\n",
      "=================================\n",
      "in 3650 epoch, average loss: -190744.8625\n",
      "                , loss1: -6783.6390625\n",
      "                , loss2: 17.855487060546874\n",
      "=================================\n",
      "in 3660 epoch, average loss: -190339.7\n",
      "                , loss1: -6783.740625\n",
      "                , loss2: 18.840725708007813\n",
      "=================================\n",
      "in 3670 epoch, average loss: -189932.9625\n",
      "                , loss1: -6783.7453125\n",
      "                , loss2: 18.656399536132813\n",
      "=================================\n",
      "in 3680 epoch, average loss: -189526.4125\n",
      "                , loss1: -6783.740625\n",
      "                , loss2: 18.07259521484375\n",
      "=================================\n",
      "in 3690 epoch, average loss: -189119.575\n",
      "                , loss1: -6783.7609375\n",
      "                , loss2: 18.4669677734375\n",
      "=================================\n",
      "in 3700 epoch, average loss: -188712.7\n",
      "                , loss1: -6783.7546875\n",
      "                , loss2: 18.13865966796875\n",
      "=================================\n",
      "in 3710 epoch, average loss: -188304.725\n",
      "                , loss1: -6783.7203125\n",
      "                , loss2: 18.14049377441406\n",
      "=================================\n",
      "in 3720 epoch, average loss: -187895.125\n",
      "                , loss1: -6783.61328125\n",
      "                , loss2: 17.719760131835937\n",
      "=================================\n",
      "in 3730 epoch, average loss: -187488.325\n",
      "                , loss1: -6783.7515625\n",
      "                , loss2: 21.360563659667967\n",
      "=================================\n",
      "in 3740 epoch, average loss: -187083.3375\n",
      "                , loss1: -6783.70546875\n",
      "                , loss2: 18.059544372558594\n",
      "=================================\n",
      "in 3750 epoch, average loss: -186677.5625\n",
      "                , loss1: -6783.76015625\n",
      "                , loss2: 18.314093017578124\n",
      "=================================\n",
      "in 3760 epoch, average loss: -186271.675\n",
      "                , loss1: -6783.471875\n",
      "                , loss2: 9.239830017089844\n",
      "=================================\n",
      "in 3770 epoch, average loss: -185856.725\n",
      "                , loss1: -6783.25078125\n",
      "                , loss2: 11.121839904785157\n",
      "=================================\n",
      "in 3780 epoch, average loss: -184548.7375\n",
      "                , loss1: -6769.19609375\n",
      "                , loss2: 527.918017578125\n",
      "=================================\n",
      "in 3790 epoch, average loss: -184638.2375\n",
      "                , loss1: -6772.13671875\n",
      "                , loss2: 112.38720703125\n",
      "=================================\n",
      "in 3800 epoch, average loss: -184435.975\n",
      "                , loss1: -6775.91171875\n",
      "                , loss2: 11.105596923828125\n",
      "=================================\n",
      "in 3810 epoch, average loss: -184138.9875\n",
      "                , loss1: -6779.8625\n",
      "                , loss2: 8.8715576171875\n",
      "=================================\n",
      "in 3820 epoch, average loss: -183789.85\n",
      "                , loss1: -6782.16640625\n",
      "                , loss2: 13.655335998535156\n",
      "=================================\n",
      "in 3830 epoch, average loss: -183404.8375\n",
      "                , loss1: -6782.775\n",
      "                , loss2: 8.158705902099609\n",
      "=================================\n",
      "in 3840 epoch, average loss: -182995.2\n",
      "                , loss1: -6782.57421875\n",
      "                , loss2: 5.4351551055908205\n",
      "=================================\n",
      "in 3850 epoch, average loss: -182597.5\n",
      "                , loss1: -6782.75546875\n",
      "                , loss2: 1.0561570167541503\n",
      "=================================\n",
      "in 3860 epoch, average loss: -182202.1875\n",
      "                , loss1: -6783.16796875\n",
      "                , loss2: 0.4943717956542969\n",
      "=================================\n",
      "in 3870 epoch, average loss: -181801.4125\n",
      "                , loss1: -6783.4234375\n",
      "                , loss2: 1.1073417663574219\n",
      "=================================\n",
      "in 3880 epoch, average loss: -181394.025\n",
      "                , loss1: -6783.41484375\n",
      "                , loss2: 1.2807723999023437\n",
      "=================================\n",
      "in 3890 epoch, average loss: -180988.675\n",
      "                , loss1: -6783.459375\n",
      "                , loss2: 0.8168147087097168\n",
      "=================================\n",
      "in 3900 epoch, average loss: -180583.4\n",
      "                , loss1: -6783.503125\n",
      "                , loss2: 0.2541053295135498\n",
      "=================================\n",
      "in 3910 epoch, average loss: -180177.05\n",
      "                , loss1: -6783.525\n",
      "                , loss2: 0.1473994255065918\n",
      "=================================\n",
      "in 3920 epoch, average loss: -179770.2\n",
      "                , loss1: -6783.55546875\n",
      "                , loss2: 0.8221346855163574\n",
      "=================================\n",
      "in 3930 epoch, average loss: -179363.025\n",
      "                , loss1: -6783.534375\n",
      "                , loss2: 0.4062911033630371\n",
      "=================================\n",
      "in 3940 epoch, average loss: -178955.9375\n",
      "                , loss1: -6783.54140625\n",
      "                , loss2: 0.6684622287750244\n",
      "=================================\n",
      "in 3950 epoch, average loss: -178549.75\n",
      "                , loss1: -6783.565625\n",
      "                , loss2: 0.4764570236206055\n",
      "=================================\n",
      "in 3960 epoch, average loss: -178142.125\n",
      "                , loss1: -6783.54140625\n",
      "                , loss2: 0.44037322998046874\n",
      "=================================\n",
      "in 3970 epoch, average loss: -177735.4375\n",
      "                , loss1: -6783.5421875\n",
      "                , loss2: 0.1271735191345215\n",
      "=================================\n",
      "in 3980 epoch, average loss: -177327.4125\n",
      "                , loss1: -6783.56875\n",
      "                , loss2: 1.845880126953125\n",
      "=================================\n",
      "in 3990 epoch, average loss: -176922.9875\n",
      "                , loss1: -6783.60234375\n",
      "                , loss2: 0.15020058155059815\n",
      "=================================\n",
      "in 4000 epoch, average loss: -176514.85\n",
      "                , loss1: -6783.59296875\n",
      "                , loss2: 1.034391212463379\n",
      "=================================\n",
      "in 4010 epoch, average loss: -176108.8125\n",
      "                , loss1: -6783.5984375\n",
      "                , loss2: 0.17716363668441773\n",
      "=================================\n",
      "in 4020 epoch, average loss: -175701.2625\n",
      "                , loss1: -6783.5796875\n",
      "                , loss2: 0.23846869468688964\n",
      "=================================\n",
      "in 4030 epoch, average loss: -175294.9\n",
      "                , loss1: -6783.6\n",
      "                , loss2: 0.13360425233840942\n",
      "=================================\n",
      "in 4040 epoch, average loss: -174888.35\n",
      "                , loss1: -6783.6265625\n",
      "                , loss2: 0.32367088794708254\n",
      "=================================\n",
      "in 4050 epoch, average loss: -174481.325\n",
      "                , loss1: -6783.621875\n",
      "                , loss2: 0.22483451366424562\n",
      "=================================\n",
      "in 4060 epoch, average loss: -174074.5125\n",
      "                , loss1: -6783.625\n",
      "                , loss2: 0.08782176971435547\n",
      "=================================\n",
      "in 4070 epoch, average loss: -173666.275\n",
      "                , loss1: -6783.58515625\n",
      "                , loss2: 0.290712833404541\n",
      "=================================\n",
      "in 4080 epoch, average loss: -173259.375\n",
      "                , loss1: -6783.5890625\n",
      "                , loss2: 0.2726105213165283\n",
      "=================================\n",
      "in 4090 epoch, average loss: -172853.6625\n",
      "                , loss1: -6783.63125\n",
      "                , loss2: 0.05708357095718384\n",
      "=================================\n",
      "in 4100 epoch, average loss: -172446.2375\n",
      "                , loss1: -6783.61640625\n",
      "                , loss2: 0.07825708389282227\n",
      "=================================\n",
      "in 4110 epoch, average loss: -172039.2375\n",
      "                , loss1: -6783.6171875\n",
      "                , loss2: 0.07530096769332886\n",
      "=================================\n",
      "in 4120 epoch, average loss: -171632.975\n",
      "                , loss1: -6783.65234375\n",
      "                , loss2: 0.2160128116607666\n",
      "=================================\n",
      "in 4130 epoch, average loss: -171225.525\n",
      "                , loss1: -6783.64609375\n",
      "                , loss2: 0.49190292358398435\n",
      "=================================\n",
      "in 4140 epoch, average loss: -170817.3375\n",
      "                , loss1: -6783.58671875\n",
      "                , loss2: 0.1508720874786377\n",
      "=================================\n",
      "in 4150 epoch, average loss: -170412.0375\n",
      "                , loss1: -6783.65625\n",
      "                , loss2: 0.18691171407699586\n",
      "=================================\n",
      "in 4160 epoch, average loss: -170004.5625\n",
      "                , loss1: -6783.64375\n",
      "                , loss2: 0.3186401128768921\n",
      "=================================\n",
      "in 4170 epoch, average loss: -169595.4\n",
      "                , loss1: -6783.55390625\n",
      "                , loss2: 0.2355720043182373\n",
      "=================================\n",
      "in 4180 epoch, average loss: -169185.725\n",
      "                , loss1: -6783.4546875\n",
      "                , loss2: 0.4234346866607666\n",
      "=================================\n",
      "in 4190 epoch, average loss: -168780.6875\n",
      "                , loss1: -6783.534375\n",
      "                , loss2: 0.4382315158843994\n",
      "=================================\n",
      "in 4200 epoch, average loss: -168372.45\n",
      "                , loss1: -6783.47890625\n",
      "                , loss2: 0.2816422462463379\n",
      "=================================\n",
      "in 4210 epoch, average loss: -167967.975\n",
      "                , loss1: -6783.5859375\n",
      "                , loss2: 0.42005338668823244\n",
      "=================================\n",
      "in 4220 epoch, average loss: -167561.9\n",
      "                , loss1: -6783.63828125\n",
      "                , loss2: 0.739731740951538\n",
      "=================================\n",
      "in 4230 epoch, average loss: -167154.45\n",
      "                , loss1: -6783.5984375\n",
      "                , loss2: 0.2008967638015747\n",
      "=================================\n",
      "in 4240 epoch, average loss: -166748.7\n",
      "                , loss1: -6783.64375\n",
      "                , loss2: 0.05822905898094177\n",
      "=================================\n",
      "in 4250 epoch, average loss: -166340.3375\n",
      "                , loss1: -6783.61796875\n",
      "                , loss2: 0.7397692203521729\n",
      "=================================\n",
      "in 4260 epoch, average loss: -165933.575\n",
      "                , loss1: -6783.6015625\n",
      "                , loss2: 0.10087045431137084\n",
      "=================================\n",
      "in 4270 epoch, average loss: -165526.5625\n",
      "                , loss1: -6783.61640625\n",
      "                , loss2: 0.4562175750732422\n",
      "=================================\n",
      "in 4280 epoch, average loss: -165120.025\n",
      "                , loss1: -6783.61875\n",
      "                , loss2: 0.04994420111179352\n",
      "=================================\n",
      "in 4290 epoch, average loss: -164713.35\n",
      "                , loss1: -6783.64453125\n",
      "                , loss2: 0.31316356658935546\n",
      "=================================\n",
      "in 4300 epoch, average loss: -164307.025\n",
      "                , loss1: -6783.665625\n",
      "                , loss2: 0.12188246250152587\n",
      "=================================\n",
      "in 4310 epoch, average loss: -163899.875\n",
      "                , loss1: -6783.66640625\n",
      "                , loss2: 0.28870792388916017\n",
      "=================================\n",
      "in 4320 epoch, average loss: -163493.2\n",
      "                , loss1: -6783.671875\n",
      "                , loss2: 0.07447255849838257\n",
      "=================================\n",
      "in 4330 epoch, average loss: -163086.075\n",
      "                , loss1: -6783.66796875\n",
      "                , loss2: 0.06837322115898133\n",
      "=================================\n",
      "in 4340 epoch, average loss: -162679.275\n",
      "                , loss1: -6783.68046875\n",
      "                , loss2: 0.16023937463760377\n",
      "=================================\n",
      "in 4350 epoch, average loss: -162272.1375\n",
      "                , loss1: -6783.6765625\n",
      "                , loss2: 0.18714696168899536\n",
      "=================================\n",
      "in 4360 epoch, average loss: -161864.825\n",
      "                , loss1: -6783.6625\n",
      "                , loss2: 0.15864651203155516\n",
      "=================================\n",
      "in 4370 epoch, average loss: -161456.7375\n",
      "                , loss1: -6783.62578125\n",
      "                , loss2: 0.33087525367736814\n",
      "=================================\n",
      "in 4380 epoch, average loss: -161049.6125\n",
      "                , loss1: -6783.6078125\n",
      "                , loss2: 0.011638979613780975\n",
      "=================================\n",
      "in 4390 epoch, average loss: -160643.675\n",
      "                , loss1: -6783.65546875\n",
      "                , loss2: 0.06353392601013183\n",
      "=================================\n",
      "in 4400 epoch, average loss: -160236.9625\n",
      "                , loss1: -6783.67421875\n",
      "                , loss2: 0.2175549030303955\n",
      "=================================\n",
      "in 4410 epoch, average loss: -159830.0\n",
      "                , loss1: -6783.6734375\n",
      "                , loss2: 0.12174289226531983\n",
      "=================================\n",
      "in 4420 epoch, average loss: -159423.25\n",
      "                , loss1: -6783.68203125\n",
      "                , loss2: 0.07223830819129944\n",
      "=================================\n",
      "in 4430 epoch, average loss: -159015.675\n",
      "                , loss1: -6783.659375\n",
      "                , loss2: 0.07942908406257629\n",
      "=================================\n",
      "in 4440 epoch, average loss: -158608.475\n",
      "                , loss1: -6783.6484375\n",
      "                , loss2: 0.00803651362657547\n",
      "=================================\n",
      "in 4450 epoch, average loss: -158201.775\n",
      "                , loss1: -6783.68046875\n",
      "                , loss2: 0.4345868110656738\n",
      "=================================\n",
      "in 4460 epoch, average loss: -157795.275\n",
      "                , loss1: -6783.68828125\n",
      "                , loss2: 0.09874798655509949\n",
      "=================================\n",
      "in 4470 epoch, average loss: -157388.075\n",
      "                , loss1: -6783.6796875\n",
      "                , loss2: 0.07002737522125244\n",
      "=================================\n",
      "in 4480 epoch, average loss: -156980.3875\n",
      "                , loss1: -6783.66484375\n",
      "                , loss2: 0.39876155853271483\n",
      "=================================\n",
      "in 4490 epoch, average loss: -156573.2875\n",
      "                , loss1: -6783.64609375\n",
      "                , loss2: 0.04632321298122406\n",
      "=================================\n",
      "in 4500 epoch, average loss: -156167.0875\n",
      "                , loss1: -6783.6859375\n",
      "                , loss2: 0.13170500993728637\n",
      "=================================\n",
      "in 4510 epoch, average loss: -155760.125\n",
      "                , loss1: -6783.68515625\n",
      "                , loss2: 0.059920322895050046\n",
      "=================================\n",
      "in 4520 epoch, average loss: -155351.9\n",
      "                , loss1: -6783.63828125\n",
      "                , loss2: 0.1873860239982605\n",
      "=================================\n",
      "in 4530 epoch, average loss: -154942.2\n",
      "                , loss1: -6783.5203125\n",
      "                , loss2: 0.19789232015609742\n",
      "=================================\n",
      "in 4540 epoch, average loss: -154538.675\n",
      "                , loss1: -6783.67890625\n",
      "                , loss2: 0.3145512580871582\n",
      "=================================\n",
      "in 4550 epoch, average loss: -154130.95\n",
      "                , loss1: -6783.64453125\n",
      "                , loss2: 0.24483604431152345\n",
      "=================================\n",
      "in 4560 epoch, average loss: -153724.375\n",
      "                , loss1: -6783.67578125\n",
      "                , loss2: 0.5125891208648682\n",
      "=================================\n",
      "in 4570 epoch, average loss: -153316.9\n",
      "                , loss1: -6783.63828125\n",
      "                , loss2: 0.09888375997543335\n",
      "=================================\n",
      "in 4580 epoch, average loss: -152910.7875\n",
      "                , loss1: -6783.68671875\n",
      "                , loss2: 0.30324563980102537\n",
      "=================================\n",
      "in 4590 epoch, average loss: -152503.9125\n",
      "                , loss1: -6783.6859375\n",
      "                , loss2: 0.125591504573822\n",
      "=================================\n",
      "in 4600 epoch, average loss: -152097.0375\n",
      "                , loss1: -6783.68984375\n",
      "                , loss2: 0.0727544367313385\n",
      "=================================\n",
      "in 4610 epoch, average loss: -151689.7875\n",
      "                , loss1: -6783.68515625\n",
      "                , loss2: 0.21453702449798584\n",
      "=================================\n",
      "in 4620 epoch, average loss: -151282.975\n",
      "                , loss1: -6783.68671875\n",
      "                , loss2: 0.028060325980186464\n",
      "=================================\n",
      "in 4630 epoch, average loss: -150875.75\n",
      "                , loss1: -6783.67890625\n",
      "                , loss2: 0.0444703608751297\n",
      "=================================\n",
      "in 4640 epoch, average loss: -150469.175\n",
      "                , loss1: -6783.69765625\n",
      "                , loss2: 0.023981580138206483\n",
      "=================================\n",
      "in 4650 epoch, average loss: -150061.9875\n",
      "                , loss1: -6783.69375\n",
      "                , loss2: 0.10724420547485351\n",
      "=================================\n",
      "in 4660 epoch, average loss: -149654.8\n",
      "                , loss1: -6783.690625\n",
      "                , loss2: 0.1870921015739441\n",
      "=================================\n",
      "in 4670 epoch, average loss: -149246.55\n",
      "                , loss1: -6783.6453125\n",
      "                , loss2: 0.4337592601776123\n",
      "=================================\n",
      "in 4680 epoch, average loss: -148827.1375\n",
      "                , loss1: -6783.1828125\n",
      "                , loss2: 2.663214111328125\n",
      "=================================\n",
      "in 4690 epoch, average loss: -148429.2\n",
      "                , loss1: -6783.665625\n",
      "                , loss2: 4.1847881317138675\n",
      "=================================\n",
      "in 4700 epoch, average loss: -148024.375\n",
      "                , loss1: -6783.66796875\n",
      "                , loss2: 2.0634637832641602\n",
      "=================================\n",
      "in 4710 epoch, average loss: -147617.8\n",
      "                , loss1: -6783.68828125\n",
      "                , loss2: 2.0441556930541993\n",
      "=================================\n",
      "in 4720 epoch, average loss: -147210.225\n",
      "                , loss1: -6783.68203125\n",
      "                , loss2: 2.4689836502075195\n",
      "=================================\n",
      "in 4730 epoch, average loss: -146803.675\n",
      "                , loss1: -6783.68359375\n",
      "                , loss2: 2.033528709411621\n",
      "=================================\n",
      "in 4740 epoch, average loss: -146395.95\n",
      "                , loss1: -6783.6546875\n",
      "                , loss2: 2.099960517883301\n",
      "=================================\n",
      "in 4750 epoch, average loss: -145989.2\n",
      "                , loss1: -6783.69375\n",
      "                , loss2: 2.6772792816162108\n",
      "=================================\n",
      "in 4760 epoch, average loss: -145582.6\n",
      "                , loss1: -6783.6859375\n",
      "                , loss2: 2.0646595001220702\n",
      "=================================\n",
      "in 4770 epoch, average loss: -145175.875\n",
      "                , loss1: -6783.7\n",
      "                , loss2: 2.0947967529296876\n",
      "=================================\n",
      "in 4780 epoch, average loss: -144768.1875\n",
      "                , loss1: -6783.66953125\n",
      "                , loss2: 2.0950639724731444\n",
      "=================================\n",
      "in 4790 epoch, average loss: -144361.65\n",
      "                , loss1: -6783.69453125\n",
      "                , loss2: 2.1622596740722657\n",
      "=================================\n",
      "in 4800 epoch, average loss: -143953.85\n",
      "                , loss1: -6783.64921875\n",
      "                , loss2: 1.967362403869629\n",
      "=================================\n",
      "in 4810 epoch, average loss: -143547.05\n",
      "                , loss1: -6783.69609375\n",
      "                , loss2: 2.7417987823486327\n",
      "=================================\n",
      "in 4820 epoch, average loss: -143140.1125\n",
      "                , loss1: -6783.68828125\n",
      "                , loss2: 2.486563873291016\n",
      "=================================\n",
      "in 4830 epoch, average loss: -142733.0875\n",
      "                , loss1: -6783.671875\n",
      "                , loss2: 2.1656749725341795\n",
      "=================================\n",
      "in 4840 epoch, average loss: -142326.2375\n",
      "                , loss1: -6783.69375\n",
      "                , loss2: 2.4492609024047853\n",
      "=================================\n",
      "in 4850 epoch, average loss: -141919.475\n",
      "                , loss1: -6783.684375\n",
      "                , loss2: 1.991683578491211\n",
      "=================================\n",
      "in 4860 epoch, average loss: -141512.6125\n",
      "                , loss1: -6783.70390625\n",
      "                , loss2: 2.2406984329223634\n",
      "=================================\n",
      "in 4870 epoch, average loss: -141104.9\n",
      "                , loss1: -6783.671875\n",
      "                , loss2: 2.270143508911133\n",
      "=================================\n",
      "in 4880 epoch, average loss: -140697.6875\n",
      "                , loss1: -6783.65234375\n",
      "                , loss2: 2.0470794677734374\n",
      "=================================\n",
      "in 4890 epoch, average loss: -140291.5625\n",
      "                , loss1: -6783.696875\n",
      "                , loss2: 2.068868064880371\n",
      "=================================\n",
      "in 4900 epoch, average loss: -139884.3875\n",
      "                , loss1: -6783.6953125\n",
      "                , loss2: 2.1931671142578124\n",
      "=================================\n",
      "in 4910 epoch, average loss: -139477.225\n",
      "                , loss1: -6783.68203125\n",
      "                , loss2: 2.060944366455078\n",
      "=================================\n",
      "in 4920 epoch, average loss: -139069.6375\n",
      "                , loss1: -6783.69453125\n",
      "                , loss2: 2.894623565673828\n",
      "=================================\n",
      "in 4930 epoch, average loss: -138663.2375\n",
      "                , loss1: -6783.68828125\n",
      "                , loss2: 2.1237632751464846\n",
      "=================================\n",
      "in 4940 epoch, average loss: -138256.0625\n",
      "                , loss1: -6783.69140625\n",
      "                , loss2: 2.345694923400879\n",
      "=================================\n",
      "in 4950 epoch, average loss: -137849.5375\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 2.140672302246094\n",
      "=================================\n",
      "in 4960 epoch, average loss: -137442.15\n",
      "                , loss1: -6783.68671875\n",
      "                , loss2: 2.123320388793945\n",
      "=================================\n",
      "in 4970 epoch, average loss: -137035.0125\n",
      "                , loss1: -6783.684375\n",
      "                , loss2: 2.201431655883789\n",
      "=================================\n",
      "in 4980 epoch, average loss: -136628.225\n",
      "                , loss1: -6783.690625\n",
      "                , loss2: 2.0865461349487306\n",
      "=================================\n",
      "in 4990 epoch, average loss: -136220.9875\n",
      "                , loss1: -6783.68046875\n",
      "                , loss2: 2.0970083236694337\n",
      "=================================\n",
      "in 5000 epoch, average loss: -135814.325\n",
      "                , loss1: -6783.69765625\n",
      "                , loss2: 2.085684585571289\n",
      "=================================\n",
      "in 5010 epoch, average loss: -135407.225\n",
      "                , loss1: -6783.69375\n",
      "                , loss2: 2.079378128051758\n",
      "=================================\n",
      "in 5020 epoch, average loss: -135000.4\n",
      "                , loss1: -6783.70390625\n",
      "                , loss2: 2.0883544921875\n",
      "=================================\n",
      "in 5030 epoch, average loss: -134591.4\n",
      "                , loss1: -6783.6015625\n",
      "                , loss2: 2.033443832397461\n",
      "=================================\n",
      "in 5040 epoch, average loss: -134185.7\n",
      "                , loss1: -6783.67890625\n",
      "                , loss2: 2.2718862533569335\n",
      "=================================\n",
      "in 5050 epoch, average loss: -133779.275\n",
      "                , loss1: -6783.7\n",
      "                , loss2: 2.0735498428344727\n",
      "=================================\n",
      "in 5060 epoch, average loss: -133372.1125\n",
      "                , loss1: -6783.7\n",
      "                , loss2: 2.2177528381347655\n",
      "=================================\n",
      "in 5070 epoch, average loss: -132965.1875\n",
      "                , loss1: -6783.6859375\n",
      "                , loss2: 1.8395828247070312\n",
      "=================================\n",
      "in 5080 epoch, average loss: -132558.1125\n",
      "                , loss1: -6783.70234375\n",
      "                , loss2: 2.219368553161621\n",
      "=================================\n",
      "in 5090 epoch, average loss: -132150.975\n",
      "                , loss1: -6783.6921875\n",
      "                , loss2: 2.1385425567626952\n",
      "=================================\n",
      "in 5100 epoch, average loss: -131744.35\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 1.998834228515625\n",
      "=================================\n",
      "in 5110 epoch, average loss: -131337.225\n",
      "                , loss1: -6783.7\n",
      "                , loss2: 2.013134002685547\n",
      "=================================\n",
      "in 5120 epoch, average loss: -130930.375\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 1.9299501419067382\n",
      "=================================\n",
      "in 5130 epoch, average loss: -130522.975\n",
      "                , loss1: -6783.69921875\n",
      "                , loss2: 2.1877395629882814\n",
      "=================================\n",
      "in 5140 epoch, average loss: -130116.0875\n",
      "                , loss1: -6783.7015625\n",
      "                , loss2: 2.085225296020508\n",
      "=================================\n",
      "in 5150 epoch, average loss: -129709.125\n",
      "                , loss1: -6783.70078125\n",
      "                , loss2: 2.039328765869141\n",
      "=================================\n",
      "in 5160 epoch, average loss: -129301.925\n",
      "                , loss1: -6783.70703125\n",
      "                , loss2: 2.3161319732666015\n",
      "=================================\n",
      "in 5170 epoch, average loss: -128895.2125\n",
      "                , loss1: -6783.7015625\n",
      "                , loss2: 1.9052616119384767\n",
      "=================================\n",
      "in 5180 epoch, average loss: -128487.825\n",
      "                , loss1: -6783.69375\n",
      "                , loss2: 2.1211727142333983\n",
      "=================================\n",
      "in 5190 epoch, average loss: -128081.1875\n",
      "                , loss1: -6783.70546875\n",
      "                , loss2: 1.95113525390625\n",
      "=================================\n",
      "in 5200 epoch, average loss: -127674.45\n",
      "                , loss1: -6783.66953125\n",
      "                , loss2: 0.9922925949096679\n",
      "=================================\n",
      "in 5210 epoch, average loss: -127267.175\n",
      "                , loss1: -6783.69296875\n",
      "                , loss2: 1.682011032104492\n",
      "=================================\n",
      "in 5220 epoch, average loss: -126860.2375\n",
      "                , loss1: -6783.67734375\n",
      "                , loss2: 1.3209794998168944\n",
      "=================================\n",
      "in 5230 epoch, average loss: -126453.15\n",
      "                , loss1: -6783.6953125\n",
      "                , loss2: 1.7143083572387696\n",
      "=================================\n",
      "in 5240 epoch, average loss: -126046.975\n",
      "                , loss1: -6783.67578125\n",
      "                , loss2: 0.5103932857513428\n",
      "=================================\n",
      "in 5250 epoch, average loss: -125639.5875\n",
      "                , loss1: -6783.66875\n",
      "                , loss2: 0.7375930786132813\n",
      "=================================\n",
      "in 5260 epoch, average loss: -125231.575\n",
      "                , loss1: -6783.68203125\n",
      "                , loss2: 1.9803142547607422\n",
      "=================================\n",
      "in 5270 epoch, average loss: -124825.5125\n",
      "                , loss1: -6783.6609375\n",
      "                , loss2: 0.6270572662353515\n",
      "=================================\n",
      "in 5280 epoch, average loss: -124418.8875\n",
      "                , loss1: -6783.6953125\n",
      "                , loss2: 0.8691044807434082\n",
      "=================================\n",
      "in 5290 epoch, average loss: -124010.5125\n",
      "                , loss1: -6783.5875\n",
      "                , loss2: 0.25309817790985106\n",
      "=================================\n",
      "in 5300 epoch, average loss: -123603.7125\n",
      "                , loss1: -6783.59921875\n",
      "                , loss2: 0.23657588958740233\n",
      "=================================\n",
      "in 5310 epoch, average loss: -123198.1125\n",
      "                , loss1: -6783.678125\n",
      "                , loss2: 0.2675973415374756\n",
      "=================================\n",
      "in 5320 epoch, average loss: -122790.8625\n",
      "                , loss1: -6783.6765625\n",
      "                , loss2: 0.46048789024353026\n",
      "=================================\n",
      "in 5330 epoch, average loss: -122384.55\n",
      "                , loss1: -6783.69921875\n",
      "                , loss2: 0.16540803909301757\n",
      "=================================\n",
      "in 5340 epoch, average loss: -121977.375\n",
      "                , loss1: -6783.6921875\n",
      "                , loss2: 0.19218821525573732\n",
      "=================================\n",
      "in 5350 epoch, average loss: -121570.3125\n",
      "                , loss1: -6783.69609375\n",
      "                , loss2: 0.3101701259613037\n",
      "=================================\n",
      "in 5360 epoch, average loss: -121163.6125\n",
      "                , loss1: -6783.69921875\n",
      "                , loss2: 0.025474348664283754\n",
      "=================================\n",
      "in 5370 epoch, average loss: -120756.4125\n",
      "                , loss1: -6783.69453125\n",
      "                , loss2: 0.14216488599777222\n",
      "=================================\n",
      "in 5380 epoch, average loss: -120349.3375\n",
      "                , loss1: -6783.690625\n",
      "                , loss2: 0.12475647926330566\n",
      "=================================\n",
      "in 5390 epoch, average loss: -119942.4875\n",
      "                , loss1: -6783.70078125\n",
      "                , loss2: 0.1260457992553711\n",
      "=================================\n",
      "in 5400 epoch, average loss: -119535.225\n",
      "                , loss1: -6783.68046875\n",
      "                , loss2: 0.009349977225065231\n",
      "=================================\n",
      "in 5410 epoch, average loss: -119127.9375\n",
      "                , loss1: -6783.6703125\n",
      "                , loss2: 0.10277833938598632\n",
      "=================================\n",
      "in 5420 epoch, average loss: -118721.0375\n",
      "                , loss1: -6783.675\n",
      "                , loss2: 0.06316279172897339\n",
      "=================================\n",
      "in 5430 epoch, average loss: -118314.1625\n",
      "                , loss1: -6783.69609375\n",
      "                , loss2: 0.2766244888305664\n",
      "=================================\n",
      "in 5440 epoch, average loss: -117906.8875\n",
      "                , loss1: -6783.6953125\n",
      "                , loss2: 0.5282899379730225\n",
      "=================================\n",
      "in 5450 epoch, average loss: -117500.025\n",
      "                , loss1: -6783.67578125\n",
      "                , loss2: 0.02460014969110489\n",
      "=================================\n",
      "in 5460 epoch, average loss: -117093.225\n",
      "                , loss1: -6783.70078125\n",
      "                , loss2: 0.23356990814208983\n",
      "=================================\n",
      "in 5470 epoch, average loss: -116686.5\n",
      "                , loss1: -6783.7046875\n",
      "                , loss2: 0.009746364504098892\n",
      "=================================\n",
      "in 5480 epoch, average loss: -116279.3625\n",
      "                , loss1: -6783.6984375\n",
      "                , loss2: 0.01629294455051422\n",
      "=================================\n",
      "in 5490 epoch, average loss: -115872.3\n",
      "                , loss1: -6783.69609375\n",
      "                , loss2: 0.00871407613158226\n",
      "=================================\n",
      "in 5500 epoch, average loss: -115465.075\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 0.39520535469055174\n",
      "=================================\n",
      "in 5510 epoch, average loss: -115058.3875\n",
      "                , loss1: -6783.703125\n",
      "                , loss2: 0.008874069899320602\n",
      "=================================\n",
      "in 5520 epoch, average loss: -114651.2875\n",
      "                , loss1: -6783.7\n",
      "                , loss2: 0.022846904397010804\n",
      "=================================\n",
      "in 5530 epoch, average loss: -114244.1625\n",
      "                , loss1: -6783.709375\n",
      "                , loss2: 0.27153759002685546\n",
      "=================================\n",
      "in 5540 epoch, average loss: -113837.3375\n",
      "                , loss1: -6783.70546875\n",
      "                , loss2: 0.022923143208026887\n",
      "=================================\n",
      "in 5550 epoch, average loss: -113430.275\n",
      "                , loss1: -6783.70546875\n",
      "                , loss2: 0.06921203136444092\n",
      "=================================\n",
      "in 5560 epoch, average loss: -113023.325\n",
      "                , loss1: -6783.7109375\n",
      "                , loss2: 0.08271446228027343\n",
      "=================================\n",
      "in 5570 epoch, average loss: -112615.7625\n",
      "                , loss1: -6783.675\n",
      "                , loss2: 0.010901310294866563\n",
      "=================================\n",
      "in 5580 epoch, average loss: -112208.95\n",
      "                , loss1: -6783.68671875\n",
      "                , loss2: 0.013817328214645385\n",
      "=================================\n",
      "in 5590 epoch, average loss: -111802.075\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 0.1855807900428772\n",
      "=================================\n",
      "in 5600 epoch, average loss: -111393.9375\n",
      "                , loss1: -6783.64921875\n",
      "                , loss2: 0.34694225788116456\n",
      "=================================\n",
      "in 5610 epoch, average loss: -110987.9875\n",
      "                , loss1: -6783.70234375\n",
      "                , loss2: 0.16468924283981323\n",
      "=================================\n",
      "in 5620 epoch, average loss: -110580.925\n",
      "                , loss1: -6783.690625\n",
      "                , loss2: 0.011407974362373351\n",
      "=================================\n",
      "in 5630 epoch, average loss: -110173.0125\n",
      "                , loss1: -6783.63515625\n",
      "                , loss2: 0.017221182584762573\n",
      "=================================\n",
      "in 5640 epoch, average loss: -109766.7375\n",
      "                , loss1: -6783.7015625\n",
      "                , loss2: 0.3435231685638428\n",
      "=================================\n",
      "in 5650 epoch, average loss: -109359.5875\n",
      "                , loss1: -6783.69140625\n",
      "                , loss2: 0.28969264030456543\n",
      "=================================\n",
      "in 5660 epoch, average loss: -108952.325\n",
      "                , loss1: -6783.66640625\n",
      "                , loss2: 0.14874027967453002\n",
      "=================================\n",
      "in 5670 epoch, average loss: -108545.4375\n",
      "                , loss1: -6783.7046875\n",
      "                , loss2: 0.6187786102294922\n",
      "=================================\n",
      "in 5680 epoch, average loss: -108138.775\n",
      "                , loss1: -6783.69375\n",
      "                , loss2: 0.08375160694122315\n",
      "=================================\n",
      "in 5690 epoch, average loss: -107731.9\n",
      "                , loss1: -6783.6984375\n",
      "                , loss2: 0.026656767725944518\n",
      "=================================\n",
      "in 5700 epoch, average loss: -107324.95\n",
      "                , loss1: -6783.70234375\n",
      "                , loss2: 0.013292330503463744\n",
      "=================================\n",
      "in 5710 epoch, average loss: -106917.925\n",
      "                , loss1: -6783.70546875\n",
      "                , loss2: 0.06360633969306946\n",
      "=================================\n",
      "in 5720 epoch, average loss: -106510.95\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 0.003534131497144699\n",
      "=================================\n",
      "in 5730 epoch, average loss: -106103.675\n",
      "                , loss1: -6783.709375\n",
      "                , loss2: 0.3314695358276367\n",
      "=================================\n",
      "in 5740 epoch, average loss: -105696.7875\n",
      "                , loss1: -6783.6984375\n",
      "                , loss2: 0.0019595153629779817\n",
      "=================================\n",
      "in 5750 epoch, average loss: -105289.7625\n",
      "                , loss1: -6783.69921875\n",
      "                , loss2: 0.03103964328765869\n",
      "=================================\n",
      "in 5760 epoch, average loss: -104882.875\n",
      "                , loss1: -6783.70859375\n",
      "                , loss2: 0.05296719670295715\n",
      "=================================\n",
      "in 5770 epoch, average loss: -104475.875\n",
      "                , loss1: -6783.71015625\n",
      "                , loss2: 0.039154914021492\n",
      "=================================\n",
      "in 5780 epoch, average loss: -104068.74375\n",
      "                , loss1: -6783.70078125\n",
      "                , loss2: 0.0030600003898143767\n",
      "=================================\n",
      "in 5790 epoch, average loss: -103661.775\n",
      "                , loss1: -6783.7046875\n",
      "                , loss2: 0.01409270167350769\n",
      "=================================\n",
      "in 5800 epoch, average loss: -103254.79375\n",
      "                , loss1: -6783.709375\n",
      "                , loss2: 0.04480827450752258\n",
      "=================================\n",
      "in 5810 epoch, average loss: -102847.78125\n",
      "                , loss1: -6783.7078125\n",
      "                , loss2: 0.006653697043657303\n",
      "=================================\n",
      "in 5820 epoch, average loss: -102440.775\n",
      "                , loss1: -6783.7078125\n",
      "                , loss2: 0.002314450591802597\n",
      "=================================\n",
      "in 5830 epoch, average loss: -102033.68125\n",
      "                , loss1: -6783.70859375\n",
      "                , loss2: 0.08535256385803222\n",
      "=================================\n",
      "in 5840 epoch, average loss: -101626.725\n",
      "                , loss1: -6783.7078125\n",
      "                , loss2: 0.0033954255282878875\n",
      "=================================\n",
      "in 5850 epoch, average loss: -101219.6875\n",
      "                , loss1: -6783.7078125\n",
      "                , loss2: 0.007771572470664978\n",
      "=================================\n",
      "in 5860 epoch, average loss: -100812.75625\n",
      "                , loss1: -6783.7140625\n",
      "                , loss2: 0.01863401532173157\n",
      "=================================\n",
      "in 5870 epoch, average loss: -100405.70625\n",
      "                , loss1: -6783.71171875\n",
      "                , loss2: 0.014709554612636566\n",
      "=================================\n",
      "in 5880 epoch, average loss: -99998.525\n",
      "                , loss1: -6783.70625\n",
      "                , loss2: 0.1000185489654541\n",
      "=================================\n",
      "in 5890 epoch, average loss: -99591.525\n",
      "                , loss1: -6783.7078125\n",
      "                , loss2: 0.09547317028045654\n",
      "=================================\n",
      "in 5900 epoch, average loss: -99184.65625\n",
      "                , loss1: -6783.71328125\n",
      "                , loss2: 0.0152677983045578\n",
      "=================================\n",
      "in 5910 epoch, average loss: -98777.5875\n",
      "                , loss1: -6783.71015625\n",
      "                , loss2: 0.005355724692344665\n",
      "=================================\n",
      "in 5920 epoch, average loss: -98370.63125\n",
      "                , loss1: -6783.71640625\n",
      "                , loss2: 0.052798140048980716\n",
      "=================================\n",
      "in 5930 epoch, average loss: -97962.95625\n",
      "                , loss1: -6783.675\n",
      "                , loss2: 0.09923259019851685\n",
      "=================================\n",
      "in 5940 epoch, average loss: -97556.2\n",
      "                , loss1: -6783.7\n",
      "                , loss2: 0.19155248403549194\n",
      "=================================\n",
      "in 5950 epoch, average loss: -97149.4875\n",
      "                , loss1: -6783.70859375\n",
      "                , loss2: 0.007314998656511307\n",
      "=================================\n",
      "in 5960 epoch, average loss: -96742.4125\n",
      "                , loss1: -6783.7046875\n",
      "                , loss2: 0.00946379154920578\n",
      "=================================\n",
      "in 5970 epoch, average loss: -96335.45625\n",
      "                , loss1: -6783.71015625\n",
      "                , loss2: 0.019268256425857545\n",
      "=================================\n",
      "in 5980 epoch, average loss: -95928.31875\n",
      "                , loss1: -6783.70234375\n",
      "                , loss2: 0.021140840649604798\n",
      "=================================\n",
      "in 5990 epoch, average loss: -95521.41875\n",
      "                , loss1: -6783.7109375\n",
      "                , loss2: 0.01283433437347412\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(6000):\n",
    "    hgnn_trainer.weight = hgnn_trainer.weight - 0.006\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    # validation\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1912., 1912.], device='cuda:1', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = torch.sum(outs_straight, dim = 0)\n",
    "#bs = torch.sum(outs, dim = 0)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2: tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.0,1.,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0]])\n",
    "H = torch.tensor([[1.,1],[1,1],[0,1],[1,1],[0,1],[1,1]])\n",
    "#H = torch.tensor([[0,1.,0,1,0,1],[1,0,1,0,1,1]])\n",
    "nn = torch.matmul(a, (1 - torch.transpose(a, 0, 1)))\n",
    "#ne_k = torch.matmul(nn, H)\n",
    "ne_k = torch.matmul(nn, H)\n",
    "ne_k = ne_k.mul(H)\n",
    "H_degree = torch.sum(H, dim=0)\n",
    "H_degree = H_degree\n",
    "H_1 = ne_k / H_degree\n",
    "    #bs = torch.where(H_1>=1)\n",
    "    #print(bs)\n",
    "a2 = 1 - H_1\n",
    "a2 = a2.sqrt()\n",
    "print('a2:',a2)\n",
    "a3 = torch.prod(a2, dim=0)\n",
    "print(a3)\n",
    "a3 = a3.sum()\n",
    "loss_1 = -1 * a3\n",
    "\n",
    "a3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
