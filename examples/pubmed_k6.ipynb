{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行前请安装dhg: `pip install git+https://github.com/iMoonLab/DeepHypergraph.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycq/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seed = 600\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import dhg\n",
    "from dhg import Hypergraph\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP\n",
    "from hgp.loss import loss_bs_matrix\n",
    "from hgp.utils import from_pickle_to_hypergraph\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 6\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 2048, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 128, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers54\"] = {\"in_channels\": 256, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers55\"] = {\"in_channels\": 512, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":3824, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer34\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":3824, \"out_channels\":6, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "        1.计算与顶点``vₙ``处于不同partition的顶点在超边``eₖ``上的数量``ne_k``.  \n",
    "        2.计算与顶点``vₙ``是否处于该超边``eₖ``上.  \n",
    "        3.若在,则说明``vₙ``所在的边为 **cut** , 记录该边的损失.  \n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "\n",
    "    #loss = 50 * loss_1 + loss_2\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 3824)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = from_pickle_to_hypergraph(\"../data/pubmed\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "X = torch.eye(n=G.num_v)\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "optim = optim.Adam(hgnn_trainer.parameters(), lr=8e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.layers.0.theta.weight Parameter containing:\n",
      "tensor([[-0.0044,  0.0065, -0.0097,  ...,  0.0069, -0.0004, -0.0056],\n",
      "        [-0.0120,  0.0150,  0.0120,  ...,  0.0038,  0.0021,  0.0075],\n",
      "        [ 0.0050, -0.0109,  0.0076,  ..., -0.0153, -0.0105, -0.0149],\n",
      "        ...,\n",
      "        [-0.0006,  0.0116,  0.0071,  ..., -0.0029,  0.0108,  0.0047],\n",
      "        [-0.0084, -0.0004, -0.0116,  ..., -0.0037, -0.0057,  0.0100],\n",
      "        [ 0.0005, -0.0104, -0.0114,  ..., -0.0055, -0.0038,  0.0012]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.0.theta.bias Parameter containing:\n",
      "tensor([ 0.0128, -0.0013,  0.0099,  ...,  0.0081,  0.0049,  0.0131],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.1.theta.weight Parameter containing:\n",
      "tensor([[ 0.0050,  0.0102,  0.0084,  ...,  0.0196, -0.0082,  0.0123],\n",
      "        [ 0.0218, -0.0199, -0.0189,  ...,  0.0130, -0.0195, -0.0004],\n",
      "        [-0.0141, -0.0007,  0.0033,  ..., -0.0171,  0.0197, -0.0134],\n",
      "        ...,\n",
      "        [-0.0144,  0.0064, -0.0122,  ...,  0.0047, -0.0121,  0.0045],\n",
      "        [ 0.0176, -0.0128,  0.0037,  ..., -0.0168, -0.0020,  0.0067],\n",
      "        [ 0.0063, -0.0215,  0.0171,  ...,  0.0012,  0.0180,  0.0155]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.1.theta.bias Parameter containing:\n",
      "tensor([-0.0096, -0.0164,  0.0200,  ..., -0.0121,  0.0006, -0.0151],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.2.theta.weight Parameter containing:\n",
      "tensor([[ 0.0229,  0.0058, -0.0178,  ..., -0.0192, -0.0241,  0.0125],\n",
      "        [ 0.0077,  0.0113, -0.0018,  ..., -0.0170,  0.0041,  0.0114],\n",
      "        [ 0.0008,  0.0237, -0.0302,  ...,  0.0043, -0.0270, -0.0125],\n",
      "        ...,\n",
      "        [-0.0046,  0.0164, -0.0234,  ..., -0.0204, -0.0177, -0.0283],\n",
      "        [-0.0160, -0.0078, -0.0271,  ..., -0.0023,  0.0289, -0.0269],\n",
      "        [ 0.0286, -0.0034, -0.0055,  ..., -0.0145, -0.0128,  0.0174]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.2.theta.bias Parameter containing:\n",
      "tensor([-0.0008, -0.0270, -0.0076,  ..., -0.0032,  0.0152,  0.0229],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.3.theta.weight Parameter containing:\n",
      "tensor([[ 0.0229, -0.0169,  0.0241,  ...,  0.0137, -0.0108,  0.0019],\n",
      "        [-0.0096,  0.0203, -0.0210,  ...,  0.0010,  0.0199,  0.0228],\n",
      "        [ 0.0278, -0.0034,  0.0092,  ...,  0.0229, -0.0203,  0.0172],\n",
      "        ...,\n",
      "        [ 0.0154, -0.0054,  0.0263,  ..., -0.0223,  0.0052,  0.0166],\n",
      "        [ 0.0060, -0.0028,  0.0216,  ..., -0.0144, -0.0095, -0.0040],\n",
      "        [-0.0239, -0.0270, -0.0074,  ...,  0.0266,  0.0083, -0.0001]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.3.theta.bias Parameter containing:\n",
      "tensor([ 0.0292, -0.0188, -0.0270,  ..., -0.0140, -0.0155,  0.0077],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.4.theta.weight Parameter containing:\n",
      "tensor([[ 0.0169,  0.0053,  0.0164,  ..., -0.0076, -0.0087, -0.0312],\n",
      "        [-0.0138, -0.0030,  0.0210,  ..., -0.0164, -0.0087, -0.0150],\n",
      "        [ 0.0300,  0.0045,  0.0204,  ...,  0.0022, -0.0216, -0.0197],\n",
      "        ...,\n",
      "        [ 0.0034, -0.0299, -0.0259,  ...,  0.0301,  0.0300, -0.0183],\n",
      "        [-0.0160, -0.0306,  0.0027,  ..., -0.0201,  0.0290, -0.0065],\n",
      "        [ 0.0307,  0.0011,  0.0212,  ..., -0.0039, -0.0231, -0.0110]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.4.theta.bias Parameter containing:\n",
      "tensor([-0.0087,  0.0239,  0.0106,  0.0179, -0.0120, -0.0046,  0.0017,  0.0285,\n",
      "        -0.0076, -0.0087,  0.0022, -0.0085,  0.0218, -0.0154,  0.0308,  0.0269,\n",
      "         0.0254, -0.0074, -0.0020,  0.0194,  0.0295, -0.0078, -0.0292, -0.0040,\n",
      "         0.0024,  0.0198, -0.0224, -0.0044, -0.0037, -0.0108,  0.0171, -0.0092,\n",
      "        -0.0131,  0.0224,  0.0003,  0.0165, -0.0258, -0.0010,  0.0057, -0.0235,\n",
      "         0.0259,  0.0182,  0.0080,  0.0112, -0.0095, -0.0190, -0.0095, -0.0271,\n",
      "         0.0091, -0.0157, -0.0250,  0.0280,  0.0190,  0.0293, -0.0291, -0.0072,\n",
      "         0.0017,  0.0014, -0.0159, -0.0101,  0.0197,  0.0077, -0.0194,  0.0184,\n",
      "         0.0046,  0.0272,  0.0035,  0.0288,  0.0041, -0.0171,  0.0041, -0.0191,\n",
      "         0.0066, -0.0148, -0.0011,  0.0149, -0.0200,  0.0214,  0.0148,  0.0132,\n",
      "         0.0189,  0.0114, -0.0231,  0.0302, -0.0193,  0.0288,  0.0125,  0.0146,\n",
      "        -0.0140,  0.0223, -0.0076, -0.0224,  0.0209, -0.0160, -0.0244,  0.0260,\n",
      "        -0.0079, -0.0164,  0.0001, -0.0173,  0.0134, -0.0122, -0.0060, -0.0256,\n",
      "         0.0122,  0.0110, -0.0017,  0.0301, -0.0182,  0.0094, -0.0208,  0.0123,\n",
      "         0.0308,  0.0198,  0.0076, -0.0305,  0.0114, -0.0207, -0.0240,  0.0208,\n",
      "         0.0290,  0.0187,  0.0008, -0.0061,  0.0071,  0.0023, -0.0235, -0.0163,\n",
      "         0.0019,  0.0226, -0.0231, -0.0231, -0.0305, -0.0062, -0.0065,  0.0230,\n",
      "        -0.0047,  0.0128, -0.0136,  0.0189, -0.0184,  0.0161, -0.0177, -0.0268,\n",
      "         0.0215,  0.0312,  0.0098, -0.0185, -0.0224, -0.0067,  0.0253, -0.0293,\n",
      "        -0.0138, -0.0279, -0.0228, -0.0154,  0.0119, -0.0128, -0.0230,  0.0093,\n",
      "        -0.0212,  0.0090, -0.0231,  0.0069, -0.0163,  0.0228, -0.0154, -0.0158,\n",
      "        -0.0012,  0.0152,  0.0048,  0.0176,  0.0092, -0.0003,  0.0031, -0.0157,\n",
      "        -0.0247, -0.0198, -0.0186,  0.0110,  0.0275,  0.0234,  0.0079,  0.0078,\n",
      "         0.0178,  0.0296,  0.0088,  0.0263, -0.0125,  0.0103,  0.0178,  0.0185,\n",
      "         0.0019, -0.0202, -0.0104, -0.0031,  0.0201,  0.0245,  0.0210,  0.0259,\n",
      "        -0.0312,  0.0172, -0.0004, -0.0064, -0.0311, -0.0070, -0.0001,  0.0050,\n",
      "         0.0048, -0.0300, -0.0217,  0.0186, -0.0064, -0.0039,  0.0211, -0.0239,\n",
      "        -0.0152,  0.0277, -0.0225,  0.0106, -0.0227, -0.0247, -0.0222,  0.0117,\n",
      "        -0.0303,  0.0094, -0.0117,  0.0138,  0.0256, -0.0305, -0.0312, -0.0131,\n",
      "        -0.0170,  0.0143,  0.0003, -0.0013,  0.0233, -0.0045,  0.0266, -0.0058,\n",
      "        -0.0006, -0.0246, -0.0094,  0.0118,  0.0214,  0.0275, -0.0298,  0.0138,\n",
      "         0.0255,  0.0238, -0.0027, -0.0216,  0.0039, -0.0147, -0.0114, -0.0117,\n",
      "        -0.0267, -0.0049,  0.0159,  0.0254,  0.0038,  0.0144, -0.0015, -0.0010,\n",
      "        -0.0146,  0.0081, -0.0066, -0.0111, -0.0079,  0.0212, -0.0293,  0.0020,\n",
      "         0.0196,  0.0192, -0.0223,  0.0028, -0.0233,  0.0252,  0.0088,  0.0235,\n",
      "        -0.0022,  0.0024,  0.0178,  0.0126, -0.0309, -0.0045, -0.0266,  0.0185,\n",
      "         0.0302, -0.0089,  0.0199,  0.0051, -0.0228, -0.0254,  0.0056,  0.0136,\n",
      "         0.0101,  0.0095,  0.0202,  0.0103, -0.0151,  0.0031,  0.0309,  0.0103,\n",
      "         0.0292, -0.0262, -0.0279, -0.0208, -0.0217, -0.0127, -0.0025,  0.0034,\n",
      "        -0.0152,  0.0021,  0.0136, -0.0042,  0.0310,  0.0303, -0.0160,  0.0028,\n",
      "        -0.0022,  0.0167, -0.0191, -0.0210,  0.0027,  0.0111,  0.0139,  0.0301,\n",
      "         0.0300, -0.0119, -0.0190, -0.0257,  0.0042,  0.0233, -0.0259, -0.0295,\n",
      "         0.0119,  0.0288,  0.0222, -0.0150, -0.0306, -0.0253, -0.0289, -0.0016,\n",
      "        -0.0139, -0.0046,  0.0048, -0.0210, -0.0182,  0.0048, -0.0312,  0.0265,\n",
      "         0.0061,  0.0159, -0.0284,  0.0310, -0.0208, -0.0007,  0.0232,  0.0099,\n",
      "         0.0148, -0.0215, -0.0010, -0.0014,  0.0154,  0.0009, -0.0129, -0.0014,\n",
      "        -0.0252,  0.0002,  0.0263, -0.0052, -0.0138,  0.0230, -0.0071,  0.0147,\n",
      "         0.0022, -0.0279,  0.0024,  0.0210, -0.0014, -0.0297, -0.0173, -0.0041,\n",
      "        -0.0189, -0.0051,  0.0221, -0.0251, -0.0240, -0.0213,  0.0205, -0.0113,\n",
      "         0.0311, -0.0075,  0.0068,  0.0137, -0.0219, -0.0295, -0.0121, -0.0269,\n",
      "         0.0132,  0.0149,  0.0062,  0.0183,  0.0023,  0.0059, -0.0310, -0.0098,\n",
      "        -0.0145, -0.0231, -0.0286, -0.0200, -0.0073,  0.0285, -0.0199, -0.0144,\n",
      "        -0.0295,  0.0075,  0.0067,  0.0150,  0.0136,  0.0306, -0.0298,  0.0008,\n",
      "        -0.0297, -0.0225,  0.0274,  0.0299, -0.0041,  0.0131,  0.0103, -0.0223,\n",
      "         0.0199,  0.0091,  0.0258,  0.0286, -0.0127,  0.0115, -0.0156,  0.0034,\n",
      "        -0.0298, -0.0161, -0.0301,  0.0214,  0.0235,  0.0209, -0.0132,  0.0095,\n",
      "        -0.0072, -0.0173, -0.0308,  0.0002,  0.0173,  0.0105,  0.0274, -0.0030,\n",
      "         0.0308, -0.0218, -0.0127,  0.0170,  0.0237, -0.0167,  0.0011, -0.0269,\n",
      "         0.0232, -0.0080,  0.0038, -0.0296,  0.0079,  0.0094, -0.0134,  0.0145,\n",
      "         0.0015, -0.0258,  0.0171, -0.0257,  0.0211,  0.0131,  0.0215, -0.0030,\n",
      "         0.0100,  0.0133,  0.0212, -0.0100, -0.0221,  0.0292,  0.0232, -0.0265,\n",
      "         0.0223, -0.0295, -0.0281,  0.0158,  0.0233,  0.0094, -0.0223, -0.0178,\n",
      "         0.0029, -0.0003,  0.0132, -0.0139,  0.0133, -0.0131,  0.0252, -0.0232,\n",
      "        -0.0191,  0.0297, -0.0099, -0.0079, -0.0116, -0.0056, -0.0258,  0.0311],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.5.theta.weight Parameter containing:\n",
      "tensor([[ 0.0186,  0.0209, -0.0128,  ..., -0.0262, -0.0435,  0.0406],\n",
      "        [-0.0019, -0.0114,  0.0413,  ...,  0.0221,  0.0422,  0.0153],\n",
      "        [-0.0325,  0.0289, -0.0309,  ..., -0.0396,  0.0295, -0.0156],\n",
      "        ...,\n",
      "        [ 0.0216, -0.0068,  0.0170,  ...,  0.0165, -0.0181,  0.0048],\n",
      "        [ 0.0318,  0.0002, -0.0024,  ..., -0.0064, -0.0005, -0.0080],\n",
      "        [ 0.0179,  0.0432, -0.0112,  ..., -0.0340,  0.0090, -0.0123]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.5.theta.bias Parameter containing:\n",
      "tensor([-3.2017e-02, -3.3147e-02, -2.2151e-03, -1.6371e-02, -4.3300e-02,\n",
      "         8.3616e-03, -7.0753e-03, -1.4145e-02,  1.2911e-02,  5.1307e-03,\n",
      "         3.6655e-02,  1.5464e-02, -4.6045e-03,  1.1592e-02,  7.0963e-03,\n",
      "         2.4022e-02, -2.0578e-02,  1.7589e-02, -4.0133e-02, -2.1073e-02,\n",
      "         1.7094e-02, -1.2560e-02, -9.1496e-03,  2.0676e-03,  6.7359e-04,\n",
      "        -1.8111e-02, -1.1332e-02, -2.1944e-02, -5.9322e-03,  3.3826e-02,\n",
      "         1.0011e-02, -2.4970e-02, -3.1249e-02, -3.9020e-02,  4.1617e-02,\n",
      "        -3.5796e-02,  3.9277e-02,  2.3899e-02,  3.0760e-02,  3.5854e-02,\n",
      "         2.6362e-02, -3.2900e-02, -2.2610e-02,  1.7717e-02,  1.7152e-03,\n",
      "        -2.6001e-02, -1.5218e-02,  2.1243e-02,  2.5948e-02, -1.4198e-02,\n",
      "        -3.4000e-02,  3.4218e-02,  4.2326e-02,  2.0989e-02, -4.1139e-02,\n",
      "         1.5279e-02, -1.8563e-02,  1.1200e-02, -3.3137e-02, -3.1130e-03,\n",
      "        -1.4846e-02, -3.0969e-03,  1.3187e-02,  1.6642e-02,  2.0103e-02,\n",
      "         3.9047e-02, -4.3643e-02,  1.3281e-02, -3.7224e-02, -1.1601e-02,\n",
      "        -1.1148e-02, -1.9280e-02, -2.4294e-02, -3.4850e-02,  7.2504e-03,\n",
      "        -4.0019e-02,  3.5252e-02,  3.1696e-02,  7.2598e-03,  1.6913e-03,\n",
      "        -5.4033e-03, -1.9817e-02,  1.3439e-02,  2.1708e-02,  3.1188e-02,\n",
      "        -4.9971e-03,  5.5792e-03, -4.2803e-02,  2.2133e-02, -1.3576e-03,\n",
      "        -1.7113e-02, -4.9979e-04,  3.0444e-02,  2.7791e-02, -4.4122e-02,\n",
      "        -1.2899e-02, -3.7829e-02, -3.2435e-02, -3.1873e-02, -1.7192e-02,\n",
      "        -2.2889e-02,  1.2971e-02,  1.2299e-02, -1.0930e-02, -1.8704e-02,\n",
      "         1.9008e-02, -3.1496e-03, -2.7156e-03,  1.8354e-02, -2.9863e-02,\n",
      "         8.0098e-03, -3.2363e-02, -4.2640e-02,  2.9204e-02, -4.1528e-02,\n",
      "         5.5571e-03, -4.2492e-02, -3.1878e-02, -4.1960e-02,  1.4478e-02,\n",
      "         2.2765e-02, -2.9065e-02,  2.1150e-02,  8.2766e-06, -9.8730e-03,\n",
      "        -1.0206e-02, -3.0710e-03,  3.5187e-02, -6.7610e-03,  3.5807e-02,\n",
      "        -4.3572e-02,  4.0718e-02, -1.0996e-02,  3.4930e-02,  3.3233e-02,\n",
      "         3.1500e-02,  1.1083e-02, -1.6283e-02, -1.4178e-03, -1.1034e-02,\n",
      "        -3.6061e-02, -3.7829e-02, -1.3607e-02,  1.3895e-02, -2.0875e-02,\n",
      "         1.4243e-02, -4.0705e-02, -1.3947e-02,  9.0716e-04, -4.1522e-02,\n",
      "         3.2097e-03,  2.9101e-02, -3.3252e-02,  1.8034e-02, -9.7536e-04,\n",
      "         1.4803e-02, -3.1488e-02, -5.1562e-03,  1.0419e-02,  4.6307e-03,\n",
      "        -2.7836e-02, -3.5835e-03, -4.2040e-02,  4.1558e-02,  4.3018e-02,\n",
      "         2.7397e-02,  2.6355e-02, -1.1126e-02,  2.9488e-02,  2.9979e-02,\n",
      "        -4.7949e-03,  1.8284e-02,  3.1303e-02, -3.9998e-02, -2.7284e-02,\n",
      "         3.1244e-02, -3.0470e-02,  1.4897e-02,  3.1233e-02, -2.5253e-02,\n",
      "         3.2102e-02,  8.5583e-03,  3.6699e-02, -1.5008e-03, -2.1111e-02,\n",
      "        -2.3693e-02,  3.8042e-02, -9.4809e-03,  3.9173e-02,  1.4307e-02,\n",
      "         2.7995e-02,  2.1072e-02, -1.5000e-02, -3.6517e-02, -2.8706e-02,\n",
      "         1.3693e-02,  2.5741e-02, -3.6005e-02, -3.0820e-02,  3.9204e-02,\n",
      "         1.5309e-02,  4.0527e-02,  1.2421e-03, -2.0100e-02, -1.3296e-02,\n",
      "         8.1686e-03, -3.3094e-02,  3.1709e-02,  8.4016e-03,  2.3919e-02,\n",
      "         1.8384e-02, -3.2535e-02,  2.3796e-02,  2.1602e-02,  2.2863e-02,\n",
      "         2.6606e-03,  2.0267e-02,  2.9554e-02,  5.1139e-03, -3.9918e-02,\n",
      "         3.6311e-02,  2.4204e-02,  1.1026e-02, -3.9190e-02, -9.1511e-03,\n",
      "         3.2234e-02,  1.9050e-02, -3.7493e-02, -4.8767e-03, -1.4850e-03,\n",
      "        -2.7111e-02,  3.4794e-02, -2.5023e-03, -1.7042e-02,  4.0840e-02,\n",
      "        -2.3102e-02,  4.0541e-02,  2.9620e-02,  8.2670e-03,  3.7387e-02,\n",
      "         6.5136e-03, -3.2046e-02,  3.4779e-03, -3.7566e-02, -6.0578e-03,\n",
      "         3.1306e-02, -2.0669e-02,  2.8196e-02,  3.3564e-02,  1.6339e-02,\n",
      "        -1.4092e-02,  4.5314e-03, -1.9366e-02, -5.7000e-03, -3.6287e-02,\n",
      "         1.3349e-02, -3.9079e-02,  8.5950e-03, -2.3535e-02,  1.8521e-02,\n",
      "        -2.7688e-03, -4.3491e-02,  4.7136e-03,  3.5677e-02,  1.8821e-02,\n",
      "         3.7247e-02,  3.3680e-03,  1.8329e-02, -2.8735e-02,  1.5676e-02,\n",
      "         3.2370e-02, -1.3874e-03, -2.1713e-02, -3.6299e-02, -6.0370e-03,\n",
      "        -9.4318e-03, -2.6222e-02, -1.3025e-03, -3.7343e-02,  4.8265e-03,\n",
      "        -3.4308e-02,  3.3547e-03,  5.5186e-04, -5.2667e-03,  1.6518e-03,\n",
      "        -3.3472e-02, -5.8134e-04, -3.2128e-02,  3.5792e-02,  2.1424e-02,\n",
      "         3.4682e-02,  3.3965e-03, -8.0409e-03, -2.2900e-02, -4.2160e-02,\n",
      "        -1.6615e-02,  1.1343e-02,  2.7017e-02,  3.5129e-03,  2.0154e-04,\n",
      "         4.3425e-03, -2.7008e-02, -3.3416e-02,  6.9685e-03,  1.2925e-03,\n",
      "        -1.8176e-02, -1.9123e-02,  1.5564e-02,  2.3254e-02,  8.4451e-03,\n",
      "        -1.3793e-02, -1.9806e-02,  3.8698e-02,  1.7941e-02,  4.1516e-02,\n",
      "         3.4738e-03, -2.6311e-02,  1.5822e-02,  3.7930e-02, -5.4386e-04,\n",
      "        -4.2678e-02,  1.6752e-02, -3.6009e-03, -1.4539e-02, -4.2027e-02,\n",
      "         3.3997e-02, -1.2840e-02,  3.0900e-02,  3.2696e-02, -3.8477e-02,\n",
      "         3.9812e-02,  2.5301e-02, -4.0227e-02, -4.3706e-02,  8.5559e-03,\n",
      "        -2.5361e-02, -2.5160e-02,  3.2617e-02, -4.1972e-02,  4.5836e-03,\n",
      "         6.6223e-03,  4.1905e-02,  2.8396e-02, -4.1700e-02,  7.1938e-03,\n",
      "        -4.3591e-02,  2.4189e-02,  2.6935e-02, -4.0511e-02,  5.3861e-03,\n",
      "         3.3358e-02,  3.9052e-02,  5.2969e-03, -2.3194e-02,  1.4912e-04,\n",
      "         3.8955e-02,  3.9084e-03, -2.6274e-02, -2.7304e-02, -4.1228e-03,\n",
      "        -2.7054e-02, -1.9688e-02,  2.7091e-03,  3.3058e-02,  2.1611e-02,\n",
      "        -4.3342e-02,  2.8376e-02, -4.2317e-02, -2.2276e-02, -2.4041e-02,\n",
      "        -2.5371e-02, -5.6209e-03, -1.6963e-02,  1.2820e-02, -3.9870e-02,\n",
      "         4.4123e-02,  3.3823e-02, -3.1604e-02,  2.0284e-02, -6.0782e-03,\n",
      "         2.7904e-02,  4.2666e-02,  3.9033e-02, -2.8932e-03,  3.4297e-02,\n",
      "        -1.5716e-02, -3.4458e-02, -2.0026e-02, -1.0400e-02,  3.1452e-02,\n",
      "         1.8712e-02, -2.9523e-02, -1.7331e-03,  2.2191e-02, -3.6538e-02,\n",
      "         1.1395e-02, -6.2753e-03, -2.3685e-02, -1.4385e-02, -2.8408e-03,\n",
      "         1.3585e-02,  7.1339e-03,  5.8302e-03, -2.4229e-02,  6.2993e-03,\n",
      "        -3.2907e-02, -6.6340e-03,  1.4086e-02,  2.4510e-02,  1.0202e-02,\n",
      "        -3.0833e-03,  3.8013e-02,  1.0449e-02,  3.4837e-02,  4.9011e-03,\n",
      "         2.8652e-02,  3.4455e-02, -7.2874e-03,  2.7681e-02,  3.3450e-02,\n",
      "        -3.2591e-02,  1.6968e-04, -8.6522e-03,  4.3098e-02,  3.1431e-02,\n",
      "        -1.8205e-03, -2.7593e-02,  1.9729e-04, -2.6221e-02,  2.7666e-02,\n",
      "         3.0715e-02, -4.3734e-02, -3.7414e-02,  2.9509e-02, -1.8245e-02,\n",
      "         2.1805e-02, -5.8113e-03,  1.7161e-02,  4.0669e-02, -3.5156e-02,\n",
      "         3.0700e-02, -2.7893e-02, -1.0733e-02, -1.9324e-03, -9.2389e-03,\n",
      "        -3.0383e-02, -3.4645e-02, -3.9805e-03, -4.0650e-02, -2.9724e-02,\n",
      "         3.3908e-02,  3.2589e-02,  3.5630e-02, -8.9195e-03,  4.0577e-02,\n",
      "        -2.2609e-02,  1.4019e-02,  2.0331e-02,  1.7716e-02,  7.6071e-03,\n",
      "         2.6236e-02, -1.3495e-02,  5.4582e-03,  1.1823e-02, -3.6824e-02,\n",
      "         1.7953e-02, -4.3840e-02, -7.2288e-03, -1.6121e-02,  3.5506e-02,\n",
      "        -2.9265e-02, -3.2726e-02, -4.0484e-02, -2.9782e-02, -1.5301e-03,\n",
      "        -8.6205e-03,  2.6923e-02, -3.2462e-03,  4.0581e-02, -4.2405e-02,\n",
      "        -8.0923e-03,  3.1094e-02,  2.3796e-02, -2.5567e-02, -4.1022e-02,\n",
      "         2.5817e-02, -4.7458e-03, -3.1110e-05,  1.1101e-02,  2.4773e-02,\n",
      "        -3.9166e-02, -2.4254e-02,  1.6472e-03,  2.2785e-02,  2.8872e-02,\n",
      "         1.2074e-02,  1.5449e-02,  5.0175e-03, -4.0224e-02, -2.5394e-02,\n",
      "         2.9705e-03, -2.1556e-02, -1.8133e-02, -1.7495e-04,  2.1818e-02,\n",
      "        -5.4073e-03, -3.3052e-02, -3.5054e-02, -9.7865e-03, -2.5807e-02,\n",
      "         1.3280e-02, -3.9661e-03], device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.6.theta.weight Parameter containing:\n",
      "tensor([[ 0.0196,  0.0167, -0.0045,  ...,  0.0431,  0.0314, -0.0074],\n",
      "        [-0.0326, -0.0276, -0.0417,  ...,  0.0210,  0.0211, -0.0028],\n",
      "        [-0.0114,  0.0160,  0.0306,  ..., -0.0359, -0.0170, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0075,  0.0320,  0.0351,  ..., -0.0154, -0.0202,  0.0350],\n",
      "        [ 0.0271,  0.0184,  0.0354,  ..., -0.0251, -0.0302, -0.0316],\n",
      "        [-0.0416, -0.0148,  0.0212,  ...,  0.0128, -0.0183,  0.0192]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.6.theta.bias Parameter containing:\n",
      "tensor([ 0.0357, -0.0243,  0.0080,  0.0318,  0.0010,  0.0090,  0.0370, -0.0375,\n",
      "        -0.0180,  0.0402,  0.0387, -0.0331, -0.0234,  0.0180, -0.0303, -0.0068,\n",
      "        -0.0339,  0.0434,  0.0303,  0.0016, -0.0103,  0.0421, -0.0059,  0.0195,\n",
      "        -0.0378,  0.0193, -0.0156,  0.0127, -0.0313, -0.0055,  0.0099, -0.0207,\n",
      "        -0.0188,  0.0251,  0.0247,  0.0321, -0.0262, -0.0014,  0.0012, -0.0013,\n",
      "        -0.0109, -0.0126, -0.0146, -0.0125,  0.0065, -0.0045, -0.0084,  0.0207,\n",
      "         0.0240, -0.0144, -0.0079,  0.0333, -0.0172, -0.0414,  0.0069,  0.0427,\n",
      "         0.0233,  0.0114, -0.0246, -0.0366, -0.0419,  0.0306, -0.0359, -0.0160,\n",
      "         0.0172, -0.0411, -0.0250,  0.0307, -0.0061, -0.0217, -0.0259, -0.0156,\n",
      "        -0.0181,  0.0369, -0.0017,  0.0383,  0.0038, -0.0270, -0.0111,  0.0122,\n",
      "         0.0074, -0.0335,  0.0284,  0.0439, -0.0343,  0.0416, -0.0217,  0.0146,\n",
      "        -0.0009,  0.0002,  0.0059, -0.0144, -0.0357,  0.0159,  0.0393, -0.0205,\n",
      "         0.0354,  0.0008, -0.0068, -0.0036,  0.0294,  0.0328,  0.0078, -0.0096,\n",
      "        -0.0433,  0.0213, -0.0262, -0.0034,  0.0324,  0.0281, -0.0350,  0.0074,\n",
      "         0.0408, -0.0134,  0.0336, -0.0221, -0.0006,  0.0336,  0.0294, -0.0065,\n",
      "         0.0200, -0.0221,  0.0384, -0.0043,  0.0051,  0.0308,  0.0413, -0.0162,\n",
      "        -0.0254,  0.0441,  0.0255, -0.0238, -0.0243,  0.0250,  0.0232,  0.0321,\n",
      "         0.0146, -0.0214,  0.0370,  0.0120,  0.0435, -0.0294, -0.0344, -0.0142,\n",
      "         0.0397,  0.0133,  0.0220, -0.0123, -0.0215, -0.0127, -0.0081,  0.0397,\n",
      "         0.0304, -0.0194,  0.0314, -0.0441,  0.0179, -0.0424,  0.0070, -0.0060,\n",
      "        -0.0218, -0.0401, -0.0120,  0.0200,  0.0045, -0.0434,  0.0311,  0.0228,\n",
      "        -0.0439, -0.0097, -0.0234, -0.0144, -0.0362, -0.0096, -0.0058, -0.0334,\n",
      "         0.0424,  0.0013,  0.0163, -0.0306, -0.0173,  0.0049, -0.0232,  0.0425,\n",
      "         0.0238,  0.0077,  0.0165, -0.0323, -0.0274, -0.0058, -0.0289,  0.0016,\n",
      "         0.0409, -0.0272,  0.0281,  0.0043,  0.0387,  0.0243,  0.0242,  0.0018,\n",
      "        -0.0259, -0.0372, -0.0387,  0.0060, -0.0244,  0.0193, -0.0071,  0.0157,\n",
      "        -0.0233,  0.0028,  0.0014,  0.0044, -0.0004,  0.0157,  0.0087, -0.0141,\n",
      "         0.0350,  0.0272,  0.0432, -0.0362, -0.0127,  0.0112,  0.0162, -0.0151,\n",
      "         0.0129, -0.0172, -0.0158,  0.0142,  0.0148,  0.0210,  0.0079, -0.0008,\n",
      "         0.0440, -0.0150, -0.0095,  0.0152, -0.0362,  0.0110,  0.0061, -0.0305,\n",
      "        -0.0312,  0.0065,  0.0120,  0.0409,  0.0320,  0.0164, -0.0091,  0.0023,\n",
      "        -0.0386,  0.0333,  0.0308, -0.0158, -0.0278,  0.0355,  0.0391,  0.0402],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.7.theta.weight Parameter containing:\n",
      "tensor([[ 0.0478, -0.0437, -0.0504,  ...,  0.0305, -0.0582,  0.0098],\n",
      "        [ 0.0344, -0.0369,  0.0383,  ..., -0.0331, -0.0160, -0.0460],\n",
      "        [ 0.0369, -0.0051, -0.0190,  ...,  0.0566, -0.0412,  0.0599],\n",
      "        ...,\n",
      "        [ 0.0064, -0.0472, -0.0337,  ...,  0.0533,  0.0165, -0.0458],\n",
      "        [ 0.0103, -0.0513, -0.0381,  ..., -0.0471,  0.0303,  0.0016],\n",
      "        [-0.0310, -0.0322, -0.0289,  ...,  0.0120,  0.0354,  0.0235]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.0.layers.7.theta.bias Parameter containing:\n",
      "tensor([ 1.7603e-02, -2.5320e-02, -6.4592e-03, -1.1661e-02, -1.5161e-02,\n",
      "         5.4255e-02, -5.3661e-02, -1.2197e-02, -2.9250e-02,  9.4672e-03,\n",
      "        -6.4868e-03,  3.7093e-02,  6.0352e-02, -4.1229e-03, -3.7798e-03,\n",
      "         1.9810e-02,  4.9854e-02,  4.5050e-02,  5.1505e-02, -5.0466e-02,\n",
      "        -2.2209e-02,  1.8898e-03, -4.4365e-03,  6.0584e-02,  3.6162e-02,\n",
      "        -4.0371e-02,  6.2979e-03, -1.9202e-02, -3.3909e-03, -2.5362e-02,\n",
      "        -6.1198e-02, -4.9639e-02, -6.1363e-02,  2.5660e-03,  3.8139e-02,\n",
      "        -3.2343e-02, -4.7526e-03,  5.3402e-02, -3.2756e-02,  4.3034e-02,\n",
      "        -5.8117e-02,  2.8091e-02, -4.0277e-02,  2.0254e-02,  2.4415e-02,\n",
      "         2.2339e-02,  1.2350e-02, -4.0387e-02,  1.6420e-02,  2.7968e-02,\n",
      "         5.4639e-02,  5.0809e-02,  5.2309e-02, -2.5333e-03,  4.9854e-02,\n",
      "         4.0606e-02,  1.0070e-02,  1.4140e-02,  6.4695e-03, -8.7097e-06,\n",
      "        -3.3898e-02,  6.1639e-02,  4.3021e-02, -4.4439e-02, -2.8731e-02,\n",
      "         2.2862e-02, -5.1747e-02, -3.9642e-02, -3.2922e-02,  3.9775e-02,\n",
      "         3.9972e-02, -1.9842e-02, -8.1572e-03, -3.0764e-02, -5.0893e-02,\n",
      "         1.0681e-02,  6.7564e-03, -7.6989e-04,  5.6722e-02, -4.8196e-02,\n",
      "         5.9794e-02,  5.7508e-02, -3.6564e-02, -9.1852e-03,  2.5494e-02,\n",
      "        -6.1277e-02, -3.2309e-02,  2.3676e-02, -1.4865e-02,  2.9661e-02,\n",
      "        -5.7636e-02,  5.5841e-02,  3.7077e-02,  2.6910e-02, -1.3542e-02,\n",
      "        -2.0260e-02,  2.5244e-02,  5.3881e-02, -4.9968e-02,  5.2369e-02,\n",
      "        -1.9472e-02,  8.8818e-03,  3.4287e-03,  1.3073e-02,  1.4496e-02,\n",
      "        -3.0808e-02, -1.4176e-02, -2.8315e-02, -3.9390e-02,  1.6162e-02,\n",
      "         5.2171e-02, -5.9403e-02,  9.2006e-03,  3.0668e-02,  6.0463e-02,\n",
      "        -3.6006e-03, -4.3988e-02, -1.1077e-02, -2.3161e-02,  4.5787e-02,\n",
      "         3.1444e-02,  2.3686e-02, -2.2301e-02,  5.6189e-02,  5.9199e-03,\n",
      "         5.2891e-02, -3.8297e-02, -2.2469e-02], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "layers.1.weight Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:1', requires_grad=True)\n",
      "layers.1.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:1', requires_grad=True)\n",
      "layers.4.weight Parameter containing:\n",
      "tensor([[-0.0168, -0.0469, -0.0281,  ..., -0.0238, -0.0329,  0.0345],\n",
      "        [-0.0428, -0.0008, -0.0511,  ..., -0.0266, -0.0633, -0.0063],\n",
      "        [ 0.0084, -0.0530, -0.0418,  ...,  0.0152,  0.0374, -0.0343],\n",
      "        ...,\n",
      "        [ 0.0844,  0.0368,  0.0313,  ...,  0.0086, -0.0568,  0.0598],\n",
      "        [-0.0032, -0.0174,  0.0251,  ..., -0.0193, -0.0085, -0.0862],\n",
      "        [ 0.0853,  0.0699,  0.0055,  ..., -0.0582,  0.0237,  0.0836]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.4.bias Parameter containing:\n",
      "tensor([ 0.0188, -0.0267,  0.0693,  ..., -0.0679, -0.0135, -0.0826],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.5.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:1', requires_grad=True)\n",
      "layers.5.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1', requires_grad=True)\n",
      "layers.8.weight Parameter containing:\n",
      "tensor([[ 0.0148, -0.0055,  0.0080,  ..., -0.0031,  0.0009,  0.0071],\n",
      "        [-0.0033,  0.0088,  0.0013,  ...,  0.0117, -0.0003,  0.0024],\n",
      "        [-0.0030, -0.0148, -0.0040,  ..., -0.0028, -0.0024,  0.0094],\n",
      "        [ 0.0030,  0.0084,  0.0028,  ..., -0.0063,  0.0088,  0.0097],\n",
      "        [ 0.0133, -0.0138,  0.0069,  ..., -0.0076,  0.0132,  0.0053],\n",
      "        [-0.0016,  0.0125,  0.0073,  ..., -0.0060,  0.0104,  0.0132]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "layers.8.bias Parameter containing:\n",
      "tensor([-0.0039, -0.0136,  0.0151, -0.0122, -0.0083, -0.0081], device='cuda:1',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hgnn_trainer.layers\n",
    "for n,p in hgnn_trainer.named_parameters():\n",
    "    print(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnn_trainer.weight = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -8952.6375\n",
      "                , loss1: -563.803515625\n",
      "                , loss2: 58.07064208984375\n",
      "=================================\n",
      "in 10 epoch, average loss: -89432.71875\n",
      "                , loss1: -5634.11328125\n",
      "                , loss2: 580.696728515625\n",
      "=================================\n",
      "in 20 epoch, average loss: -89406.98125\n",
      "                , loss1: -5637.219921875\n",
      "                , loss2: 599.68779296875\n",
      "=================================\n",
      "in 30 epoch, average loss: -89234.4875\n",
      "                , loss1: -5635.8140625\n",
      "                , loss2: 693.3791015625\n",
      "=================================\n",
      "in 40 epoch, average loss: -89206.1875\n",
      "                , loss1: -5638.421484375\n",
      "                , loss2: 706.891259765625\n",
      "=================================\n",
      "in 50 epoch, average loss: -89183.5125\n",
      "                , loss1: -5637.69609375\n",
      "                , loss2: 661.638525390625\n",
      "=================================\n",
      "in 60 epoch, average loss: -89131.70625\n",
      "                , loss1: -5637.09609375\n",
      "                , loss2: 647.491455078125\n",
      "=================================\n",
      "in 70 epoch, average loss: -89094.94375\n",
      "                , loss1: -5639.40859375\n",
      "                , loss2: 664.705224609375\n",
      "=================================\n",
      "in 80 epoch, average loss: -89022.33125\n",
      "                , loss1: -5637.24296875\n",
      "                , loss2: 646.48291015625\n",
      "=================================\n",
      "in 90 epoch, average loss: -89012.7\n",
      "                , loss1: -5638.938671875\n",
      "                , loss2: 626.681396484375\n",
      "=================================\n",
      "in 100 epoch, average loss: -88969.0625\n",
      "                , loss1: -5639.830859375\n",
      "                , loss2: 628.09287109375\n",
      "=================================\n",
      "in 110 epoch, average loss: -88902.99375\n",
      "                , loss1: -5637.78046875\n",
      "                , loss2: 605.22265625\n",
      "=================================\n",
      "in 120 epoch, average loss: -88850.28125\n",
      "                , loss1: -5638.5546875\n",
      "                , loss2: 613.8439453125\n",
      "=================================\n",
      "in 130 epoch, average loss: -88809.8125\n",
      "                , loss1: -5638.528125\n",
      "                , loss2: 597.498583984375\n",
      "=================================\n",
      "in 140 epoch, average loss: -88748.64375\n",
      "                , loss1: -5637.62578125\n",
      "                , loss2: 587.984619140625\n",
      "=================================\n",
      "in 150 epoch, average loss: -88686.8\n",
      "                , loss1: -5640.455859375\n",
      "                , loss2: 638.2787109375\n",
      "=================================\n",
      "in 160 epoch, average loss: -88633.7375\n",
      "                , loss1: -5640.96484375\n",
      "                , loss2: 642.99482421875\n",
      "=================================\n",
      "in 170 epoch, average loss: -88583.6\n",
      "                , loss1: -5639.6765625\n",
      "                , loss2: 616.3216796875\n",
      "=================================\n",
      "in 180 epoch, average loss: -88539.73125\n",
      "                , loss1: -5640.09453125\n",
      "                , loss2: 610.427099609375\n",
      "=================================\n",
      "in 190 epoch, average loss: -88471.225\n",
      "                , loss1: -5638.932421875\n",
      "                , loss2: 604.182861328125\n",
      "=================================\n",
      "in 200 epoch, average loss: -88400.76875\n",
      "                , loss1: -5636.0953125\n",
      "                , loss2: 573.447119140625\n",
      "=================================\n",
      "in 210 epoch, average loss: -88333.3625\n",
      "                , loss1: -5638.49140625\n",
      "                , loss2: 622.285400390625\n",
      "=================================\n",
      "in 220 epoch, average loss: -88289.5125\n",
      "                , loss1: -5637.401171875\n",
      "                , loss2: 592.56552734375\n",
      "=================================\n",
      "in 230 epoch, average loss: -88234.7375\n",
      "                , loss1: -5638.39765625\n",
      "                , loss2: 606.685107421875\n",
      "=================================\n",
      "in 240 epoch, average loss: -88181.28125\n",
      "                , loss1: -5639.655078125\n",
      "                , loss2: 623.54169921875\n",
      "=================================\n",
      "in 250 epoch, average loss: -88120.75625\n",
      "                , loss1: -5639.384375\n",
      "                , loss2: 623.412548828125\n",
      "=================================\n",
      "in 260 epoch, average loss: -87907.1375\n",
      "                , loss1: -5625.8953125\n",
      "                , loss2: 568.5375\n",
      "=================================\n",
      "in 270 epoch, average loss: -87710.625\n",
      "                , loss1: -5635.790234375\n",
      "                , loss2: 864.25751953125\n",
      "=================================\n",
      "in 280 epoch, average loss: -87801.275\n",
      "                , loss1: -5633.178125\n",
      "                , loss2: 676.24560546875\n",
      "=================================\n",
      "in 290 epoch, average loss: -87762.1625\n",
      "                , loss1: -5628.725\n",
      "                , loss2: 589.1052734375\n",
      "=================================\n",
      "in 300 epoch, average loss: -87735.94375\n",
      "                , loss1: -5640.2875\n",
      "                , loss2: 740.41064453125\n",
      "=================================\n",
      "in 310 epoch, average loss: -87635.43125\n",
      "                , loss1: -5640.43515625\n",
      "                , loss2: 786.8501953125\n",
      "=================================\n",
      "in 320 epoch, average loss: -87690.675\n",
      "                , loss1: -5635.84609375\n",
      "                , loss2: 603.3107421875\n",
      "=================================\n",
      "in 330 epoch, average loss: -87635.45\n",
      "                , loss1: -5637.420703125\n",
      "                , loss2: 626.818701171875\n",
      "=================================\n",
      "in 340 epoch, average loss: -87524.84375\n",
      "                , loss1: -5635.2015625\n",
      "                , loss2: 646.33076171875\n",
      "=================================\n",
      "in 350 epoch, average loss: -87543.75\n",
      "                , loss1: -5635.315625\n",
      "                , loss2: 572.86259765625\n",
      "=================================\n",
      "in 360 epoch, average loss: -87461.4375\n",
      "                , loss1: -5634.871484375\n",
      "                , loss2: 591.8853515625\n",
      "=================================\n",
      "in 370 epoch, average loss: -87400.575\n",
      "                , loss1: -5636.43828125\n",
      "                , loss2: 620.8810546875\n",
      "=================================\n",
      "in 380 epoch, average loss: -87365.8875\n",
      "                , loss1: -5637.768359375\n",
      "                , loss2: 619.948095703125\n",
      "=================================\n",
      "in 390 epoch, average loss: -87305.15\n",
      "                , loss1: -5637.178515625\n",
      "                , loss2: 615.092333984375\n",
      "=================================\n",
      "in 400 epoch, average loss: -87229.9375\n",
      "                , loss1: -5637.5359375\n",
      "                , loss2: 639.513671875\n",
      "=================================\n",
      "in 410 epoch, average loss: -87153.33125\n",
      "                , loss1: -5639.6875\n",
      "                , loss2: 693.26640625\n",
      "=================================\n",
      "in 420 epoch, average loss: -87151.0875\n",
      "                , loss1: -5638.05234375\n",
      "                , loss2: 613.658056640625\n",
      "=================================\n",
      "in 430 epoch, average loss: -87082.4125\n",
      "                , loss1: -5638.01640625\n",
      "                , loss2: 625.381494140625\n",
      "=================================\n",
      "in 440 epoch, average loss: -87000.4875\n",
      "                , loss1: -5639.188671875\n",
      "                , loss2: 669.158203125\n",
      "=================================\n",
      "in 450 epoch, average loss: -86979.20625\n",
      "                , loss1: -5639.4765625\n",
      "                , loss2: 638.52890625\n",
      "=================================\n",
      "in 460 epoch, average loss: -86933.2\n",
      "                , loss1: -5638.68671875\n",
      "                , loss2: 615.866796875\n",
      "=================================\n",
      "in 470 epoch, average loss: -86870.76875\n",
      "                , loss1: -5639.773046875\n",
      "                , loss2: 638.76591796875\n",
      "=================================\n",
      "in 480 epoch, average loss: -86825.875\n",
      "                , loss1: -5639.993359375\n",
      "                , loss2: 630.69384765625\n",
      "=================================\n",
      "in 490 epoch, average loss: -86745.1125\n",
      "                , loss1: -5637.66015625\n",
      "                , loss2: 618.893896484375\n",
      "=================================\n",
      "in 500 epoch, average loss: -86679.08125\n",
      "                , loss1: -5638.518359375\n",
      "                , loss2: 641.822021484375\n",
      "=================================\n",
      "in 510 epoch, average loss: -86583.3625\n",
      "                , loss1: -5639.62421875\n",
      "                , loss2: 698.285400390625\n",
      "=================================\n",
      "in 520 epoch, average loss: -86594.5375\n",
      "                , loss1: -5638.89765625\n",
      "                , loss2: 619.461376953125\n",
      "=================================\n",
      "in 530 epoch, average loss: -86548.2\n",
      "                , loss1: -5638.95546875\n",
      "                , loss2: 610.315234375\n",
      "=================================\n",
      "in 540 epoch, average loss: -86501.75\n",
      "                , loss1: -5640.053125\n",
      "                , loss2: 617.3384765625\n",
      "=================================\n",
      "in 550 epoch, average loss: -86430.3625\n",
      "                , loss1: -5639.10859375\n",
      "                , loss2: 617.73232421875\n",
      "=================================\n",
      "in 560 epoch, average loss: -86372.89375\n",
      "                , loss1: -5638.46640625\n",
      "                , loss2: 608.92294921875\n",
      "=================================\n",
      "in 570 epoch, average loss: -86314.61875\n",
      "                , loss1: -5636.96953125\n",
      "                , loss2: 587.726123046875\n",
      "=================================\n",
      "in 580 epoch, average loss: -86269.68125\n",
      "                , loss1: -5636.856640625\n",
      "                , loss2: 574.54228515625\n",
      "=================================\n",
      "in 590 epoch, average loss: -86198.63125\n",
      "                , loss1: -5637.454296875\n",
      "                , loss2: 598.437451171875\n",
      "=================================\n",
      "in 600 epoch, average loss: -86137.68125\n",
      "                , loss1: -5638.2796875\n",
      "                , loss2: 615.707861328125\n",
      "=================================\n",
      "in 610 epoch, average loss: -86068.94375\n",
      "                , loss1: -5640.609375\n",
      "                , loss2: 663.884130859375\n",
      "=================================\n",
      "in 620 epoch, average loss: -86017.35625\n",
      "                , loss1: -5639.840234375\n",
      "                , loss2: 647.242724609375\n",
      "=================================\n",
      "in 630 epoch, average loss: -85981.19375\n",
      "                , loss1: -5639.469140625\n",
      "                , loss2: 621.3107421875\n",
      "=================================\n",
      "in 640 epoch, average loss: -85937.0625\n",
      "                , loss1: -5640.4703125\n",
      "                , loss2: 624.41904296875\n",
      "=================================\n",
      "in 650 epoch, average loss: -85883.51875\n",
      "                , loss1: -5640.1484375\n",
      "                , loss2: 616.61455078125\n",
      "=================================\n",
      "in 660 epoch, average loss: -85807.9625\n",
      "                , loss1: -5637.04609375\n",
      "                , loss2: 588.237109375\n",
      "=================================\n",
      "in 670 epoch, average loss: -85747.40625\n",
      "                , loss1: -5635.6015625\n",
      "                , loss2: 570.2791015625\n",
      "=================================\n",
      "in 680 epoch, average loss: -85693.35\n",
      "                , loss1: -5635.2421875\n",
      "                , loss2: 562.486376953125\n",
      "=================================\n",
      "in 690 epoch, average loss: -85622.28125\n",
      "                , loss1: -5635.329296875\n",
      "                , loss2: 578.5240234375\n",
      "=================================\n",
      "in 700 epoch, average loss: -85548.125\n",
      "                , loss1: -5639.561328125\n",
      "                , loss2: 661.022412109375\n",
      "=================================\n",
      "in 710 epoch, average loss: -85354.075\n",
      "                , loss1: -5641.351171875\n",
      "                , loss2: 826.01005859375\n",
      "=================================\n",
      "in 720 epoch, average loss: -85391.09375\n",
      "                , loss1: -5640.796484375\n",
      "                , loss2: 724.138037109375\n",
      "=================================\n",
      "in 730 epoch, average loss: -85194.75625\n",
      "                , loss1: -5615.021875\n",
      "                , loss2: 470.84833984375\n",
      "=================================\n",
      "in 740 epoch, average loss: -85125.93125\n",
      "                , loss1: -5612.387890625\n",
      "                , loss2: 443.334423828125\n",
      "=================================\n",
      "in 750 epoch, average loss: -85074.14375\n",
      "                , loss1: -5610.981640625\n",
      "                , loss2: 417.561083984375\n",
      "=================================\n",
      "in 760 epoch, average loss: -85136.1875\n",
      "                , loss1: -5621.414453125\n",
      "                , loss2: 458.2525390625\n",
      "=================================\n",
      "in 770 epoch, average loss: -85101.86875\n",
      "                , loss1: -5631.549609375\n",
      "                , loss2: 590.613623046875\n",
      "=================================\n",
      "in 780 epoch, average loss: -85034.54375\n",
      "                , loss1: -5633.57421875\n",
      "                , loss2: 632.3916015625\n",
      "=================================\n",
      "in 790 epoch, average loss: -84958.525\n",
      "                , loss1: -5640.2\n",
      "                , loss2: 752.768896484375\n",
      "=================================\n",
      "in 800 epoch, average loss: -84972.35625\n",
      "                , loss1: -5637.3484375\n",
      "                , loss2: 639.246435546875\n",
      "=================================\n",
      "in 810 epoch, average loss: -84916.2125\n",
      "                , loss1: -5638.057421875\n",
      "                , loss2: 649.773291015625\n",
      "=================================\n",
      "in 820 epoch, average loss: -84859.7375\n",
      "                , loss1: -5640.52109375\n",
      "                , loss2: 687.207177734375\n",
      "=================================\n",
      "in 830 epoch, average loss: -84824.36875\n",
      "                , loss1: -5637.963671875\n",
      "                , loss2: 627.4431640625\n",
      "=================================\n",
      "in 840 epoch, average loss: -84765.725\n",
      "                , loss1: -5634.5265625\n",
      "                , loss2: 577.6177734375\n",
      "=================================\n",
      "in 850 epoch, average loss: -84704.4625\n",
      "                , loss1: -5640.57578125\n",
      "                , loss2: 674.10419921875\n",
      "=================================\n",
      "in 860 epoch, average loss: -84642.975\n",
      "                , loss1: -5644.66171875\n",
      "                , loss2: 740.9884765625\n",
      "=================================\n",
      "in 870 epoch, average loss: -84622.325\n",
      "                , loss1: -5642.37421875\n",
      "                , loss2: 670.62998046875\n",
      "=================================\n",
      "in 880 epoch, average loss: -84598.39375\n",
      "                , loss1: -5641.329296875\n",
      "                , loss2: 622.355078125\n",
      "=================================\n",
      "in 890 epoch, average loss: -84533.43125\n",
      "                , loss1: -5642.35703125\n",
      "                , loss2: 646.406494140625\n",
      "=================================\n",
      "in 900 epoch, average loss: -84481.4125\n",
      "                , loss1: -5644.125390625\n",
      "                , loss2: 668.6974609375\n",
      "=================================\n",
      "in 910 epoch, average loss: -84424.3\n",
      "                , loss1: -5641.76640625\n",
      "                , loss2: 633.79306640625\n",
      "=================================\n",
      "in 920 epoch, average loss: -84362.0625\n",
      "                , loss1: -5641.939453125\n",
      "                , loss2: 642.216357421875\n",
      "=================================\n",
      "in 930 epoch, average loss: -84288.68125\n",
      "                , loss1: -5640.27109375\n",
      "                , loss2: 634.065966796875\n",
      "=================================\n",
      "in 940 epoch, average loss: -84179.34375\n",
      "                , loss1: -5630.915625\n",
      "                , loss2: 546.2275390625\n",
      "=================================\n",
      "in 950 epoch, average loss: -84140.025\n",
      "                , loss1: -5631.50234375\n",
      "                , loss2: 538.03798828125\n",
      "=================================\n",
      "in 960 epoch, average loss: -84021.2125\n",
      "                , loss1: -5641.06953125\n",
      "                , loss2: 744.32705078125\n",
      "=================================\n",
      "in 970 epoch, average loss: -84012.525\n",
      "                , loss1: -5641.41328125\n",
      "                , loss2: 701.759619140625\n",
      "=================================\n",
      "in 980 epoch, average loss: -83932.5875\n",
      "                , loss1: -5629.565625\n",
      "                , loss2: 547.498291015625\n",
      "=================================\n",
      "in 990 epoch, average loss: -83859.68125\n",
      "                , loss1: -5628.51953125\n",
      "                , loss2: 548.41787109375\n",
      "=================================\n",
      "in 1000 epoch, average loss: -83850.2125\n",
      "                , loss1: -5634.589453125\n",
      "                , loss2: 592.551708984375\n",
      "=================================\n",
      "in 1010 epoch, average loss: -83852.93125\n",
      "                , loss1: -5642.37109375\n",
      "                , loss2: 650.0318359375\n",
      "=================================\n",
      "in 1020 epoch, average loss: -83756.2875\n",
      "                , loss1: -5643.7546875\n",
      "                , loss2: 710.962646484375\n",
      "=================================\n",
      "in 1030 epoch, average loss: -83703.525\n",
      "                , loss1: -5643.66015625\n",
      "                , loss2: 705.879541015625\n",
      "=================================\n",
      "in 1040 epoch, average loss: -83687.7\n",
      "                , loss1: -5643.9328125\n",
      "                , loss2: 669.33662109375\n",
      "=================================\n",
      "in 1050 epoch, average loss: -83637.86875\n",
      "                , loss1: -5644.1578125\n",
      "                , loss2: 666.09599609375\n",
      "=================================\n",
      "in 1060 epoch, average loss: -83590.93125\n",
      "                , loss1: -5643.546484375\n",
      "                , loss2: 647.46884765625\n",
      "=================================\n",
      "in 1070 epoch, average loss: -83550.375\n",
      "                , loss1: -5642.206640625\n",
      "                , loss2: 611.601416015625\n",
      "=================================\n",
      "in 1080 epoch, average loss: -83482.49375\n",
      "                , loss1: -5644.30390625\n",
      "                , loss2: 654.317236328125\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25000\u001b[39m):\n\u001b[1;32m      3\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     32\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(25000):\n",
    "    hgnn_trainer.weight = hgnn_trainer.weight - 0.001\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    # validation\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2121"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([638., 685., 614., 635., 631., 621.], device='cuda:1',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = torch.sum(outs_straight, dim = 0)\n",
    "#bs = torch.sum(outs, dim = 0)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2: tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.0,1.,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0]])\n",
    "H = torch.tensor([[1.,1],[1,1],[0,1],[1,1],[0,1],[1,1]])\n",
    "#H = torch.tensor([[0,1.,0,1,0,1],[1,0,1,0,1,1]])\n",
    "nn = torch.matmul(a, (1 - torch.transpose(a, 0, 1)))\n",
    "#ne_k = torch.matmul(nn, H)\n",
    "ne_k = torch.matmul(nn, H)\n",
    "ne_k = ne_k.mul(H)\n",
    "H_degree = torch.sum(H, dim=0)\n",
    "H_degree = H_degree\n",
    "H_1 = ne_k / H_degree\n",
    "    #bs = torch.where(H_1>=1)\n",
    "    #print(bs)\n",
    "a2 = 1 - H_1\n",
    "a2 = a2.sqrt()\n",
    "print('a2:',a2)\n",
    "a3 = torch.prod(a2, dim=0)\n",
    "print(a3)\n",
    "a3 = a3.sum()\n",
    "loss_1 = -1 * a3\n",
    "\n",
    "a3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
