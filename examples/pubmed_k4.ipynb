{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行前请安装dhg: `pip install git+https://github.com/iMoonLab/DeepHypergraph.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycq/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seed = 600\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import dhg\n",
    "from dhg import Hypergraph\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP\n",
    "from hgp.loss import loss_bs_matrix\n",
    "from hgp.utils import from_pickle_to_hypergraph\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 4\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 2048, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 1536, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.25}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 762, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 1536, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 2048, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 762, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 512, \"out_channels\": 384, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 384, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers54\"] = {\"in_channels\": 128, \"out_channels\": 100, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":3824, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "#l_hyper_prmts[\"linerlayer34\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":3824, \"out_channels\":4, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "        1.计算与顶点``vₙ``处于不同partition的顶点在超边``eₖ``上的数量``ne_k``.  \n",
    "        2.计算与顶点``vₙ``是否处于该超边``eₖ``上.  \n",
    "        3.若在,则说明``vₙ``所在的边为 **cut** , 记录该边的损失.  \n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "\n",
    "    #loss = 50 * loss_1 + loss_2\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 3824)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = from_pickle_to_hypergraph(\"../data/pubmed\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "X = torch.eye(n=G.num_v)\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "optim = optim.Adam(hgnn_trainer.parameters(), lr=1e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.layers.0.theta.weight Parameter containing:\n",
      "tensor([[-0.0044,  0.0065, -0.0097,  ...,  0.0069, -0.0004, -0.0056],\n",
      "        [-0.0120,  0.0150,  0.0120,  ...,  0.0038,  0.0021,  0.0075],\n",
      "        [ 0.0050, -0.0109,  0.0076,  ..., -0.0153, -0.0105, -0.0149],\n",
      "        ...,\n",
      "        [-0.0006,  0.0116,  0.0071,  ..., -0.0029,  0.0108,  0.0047],\n",
      "        [-0.0084, -0.0004, -0.0116,  ..., -0.0037, -0.0057,  0.0100],\n",
      "        [ 0.0005, -0.0104, -0.0114,  ..., -0.0055, -0.0038,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.0.theta.bias Parameter containing:\n",
      "tensor([ 0.0128, -0.0013,  0.0099,  ...,  0.0081,  0.0049,  0.0131],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.1.theta.weight Parameter containing:\n",
      "tensor([[ 0.0050,  0.0102,  0.0084,  ...,  0.0196, -0.0082,  0.0123],\n",
      "        [ 0.0218, -0.0199, -0.0189,  ...,  0.0130, -0.0195, -0.0004],\n",
      "        [-0.0141, -0.0007,  0.0033,  ..., -0.0171,  0.0197, -0.0134],\n",
      "        ...,\n",
      "        [ 0.0029, -0.0080,  0.0028,  ..., -0.0024,  0.0203,  0.0177],\n",
      "        [-0.0146,  0.0136, -0.0148,  ...,  0.0151,  0.0120, -0.0050],\n",
      "        [-0.0032,  0.0116, -0.0166,  ..., -0.0016,  0.0205, -0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.1.theta.bias Parameter containing:\n",
      "tensor([ 0.0202, -0.0024, -0.0039,  ...,  0.0084,  0.0170, -0.0057],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.2.theta.weight Parameter containing:\n",
      "tensor([[-0.0144, -0.0104,  0.0049,  ...,  0.0112, -0.0088,  0.0016],\n",
      "        [-0.0079,  0.0166, -0.0171,  ...,  0.0129,  0.0159, -0.0174],\n",
      "        [ 0.0122,  0.0189,  0.0228,  ...,  0.0064, -0.0112,  0.0110],\n",
      "        ...,\n",
      "        [ 0.0235,  0.0151,  0.0098,  ...,  0.0104, -0.0024, -0.0132],\n",
      "        [ 0.0036, -0.0231,  0.0136,  ...,  0.0135,  0.0096,  0.0167],\n",
      "        [ 0.0027, -0.0244, -0.0212,  ...,  0.0038, -0.0194, -0.0149]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.2.theta.bias Parameter containing:\n",
      "tensor([-0.0021, -0.0010,  0.0238,  ..., -0.0151,  0.0212, -0.0011],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.3.theta.weight Parameter containing:\n",
      "tensor([[ 0.0236,  0.0074, -0.0048,  ..., -0.0056, -0.0258,  0.0311],\n",
      "        [ 0.0132,  0.0148, -0.0090,  ...,  0.0156,  0.0298,  0.0108],\n",
      "        [-0.0230,  0.0205, -0.0219,  ..., -0.0262, -0.0079,  0.0297],\n",
      "        ...,\n",
      "        [ 0.0101,  0.0287, -0.0123,  ..., -0.0162,  0.0120, -0.0001],\n",
      "        [-0.0033, -0.0294, -0.0050,  ..., -0.0122, -0.0214,  0.0215],\n",
      "        [-0.0056, -0.0256,  0.0217,  ..., -0.0232,  0.0279,  0.0026]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.3.theta.bias Parameter containing:\n",
      "tensor([-1.3677e-04, -2.6201e-02,  5.8298e-03, -2.6213e-02,  2.8001e-02,\n",
      "        -2.2967e-02, -2.6940e-03, -9.4145e-03,  3.0613e-02, -3.1117e-02,\n",
      "        -2.9459e-02, -2.8567e-02, -3.0922e-02,  7.1826e-04, -2.9648e-02,\n",
      "        -2.1957e-02,  1.7936e-03, -1.7379e-02, -2.7054e-02,  2.5555e-02,\n",
      "        -9.4937e-03, -2.0238e-02,  1.0727e-02,  4.4157e-03, -1.4012e-02,\n",
      "         2.5149e-02, -1.6001e-02,  7.9588e-03, -1.6651e-02, -3.0414e-02,\n",
      "        -4.4591e-03, -2.8721e-02, -3.0267e-02,  4.1832e-03, -1.1839e-02,\n",
      "        -2.9490e-02, -1.0012e-03,  5.0577e-03,  1.8138e-02,  2.7208e-02,\n",
      "        -4.2333e-03,  2.2393e-02,  5.4702e-03, -1.9304e-02, -1.8836e-02,\n",
      "         1.5713e-02, -8.1882e-03, -1.7386e-02, -1.6656e-02, -3.0546e-03,\n",
      "         2.9524e-02,  2.5018e-02,  3.0539e-02,  5.7973e-03, -1.2123e-02,\n",
      "         2.9909e-02,  1.6310e-03, -1.9913e-02, -5.9535e-03, -2.4652e-02,\n",
      "        -1.5023e-02,  1.1800e-02, -1.3817e-02,  2.3874e-03,  1.9644e-02,\n",
      "        -3.0226e-02,  2.0416e-02,  1.8940e-02,  1.2525e-02,  2.3939e-02,\n",
      "        -4.5365e-03,  1.2028e-02, -1.5453e-02,  1.8527e-03,  1.1084e-02,\n",
      "        -2.7724e-02,  2.6688e-02,  9.9231e-03, -2.6138e-03,  1.1495e-02,\n",
      "         2.9803e-02, -6.4928e-03, -1.6412e-02,  3.9722e-03,  2.4062e-02,\n",
      "         1.9656e-02, -2.1279e-02, -6.3385e-03, -9.9172e-03,  1.8510e-02,\n",
      "        -2.8722e-02,  4.9129e-03,  1.5556e-02,  3.1179e-02,  3.1087e-02,\n",
      "        -2.7649e-02, -1.8872e-03,  2.7068e-02, -1.0420e-02,  2.1997e-02,\n",
      "        -1.8886e-02, -6.1957e-03, -2.7385e-02,  3.5461e-03, -3.3709e-03,\n",
      "        -2.3565e-02,  6.8741e-03,  1.4759e-02,  1.7562e-02,  3.0073e-02,\n",
      "         3.6900e-03,  1.6092e-02, -7.4308e-03, -1.9506e-03,  1.3812e-02,\n",
      "        -2.9354e-02,  8.9055e-03, -1.2990e-02,  1.4834e-02,  1.8441e-02,\n",
      "        -2.0727e-02,  3.0951e-02, -2.2732e-02, -4.7805e-03,  1.8329e-02,\n",
      "        -3.0945e-03, -1.4267e-02, -1.7278e-02, -3.0503e-02,  3.7380e-03,\n",
      "         2.9465e-03,  9.8438e-04, -1.3554e-02,  9.5432e-03,  2.3101e-02,\n",
      "        -1.3138e-02,  1.4645e-02, -2.7593e-02, -1.5445e-03,  1.2466e-02,\n",
      "        -2.1542e-02,  2.2069e-02,  2.3372e-02,  1.7952e-02,  2.8622e-02,\n",
      "         1.9476e-02, -3.0042e-02,  1.0817e-02,  1.7140e-02, -1.3280e-02,\n",
      "         2.0117e-02,  5.0084e-03,  9.1231e-03, -5.2182e-03,  2.2766e-02,\n",
      "         3.0260e-03, -3.0872e-02, -1.9164e-02,  5.1401e-03,  1.2881e-02,\n",
      "        -3.0706e-02, -1.2412e-02,  2.4535e-03,  1.5410e-02, -1.3594e-02,\n",
      "         7.3004e-03, -1.0049e-02,  2.7828e-02, -1.0971e-02, -2.3879e-02,\n",
      "        -2.6534e-02,  1.0733e-02, -1.6256e-03, -7.8639e-03, -8.1745e-03,\n",
      "         1.7261e-02, -2.4727e-02, -1.4057e-02,  1.4056e-02,  2.5260e-02,\n",
      "         4.5271e-03,  3.0529e-02,  1.7021e-02,  2.4217e-02, -2.3049e-02,\n",
      "        -2.4659e-02,  1.9992e-02, -1.2338e-02,  1.8595e-02, -2.2298e-02,\n",
      "        -1.0934e-02, -2.0511e-02,  1.3703e-02, -2.4884e-02,  2.8803e-02,\n",
      "         7.9013e-03,  1.4945e-02,  1.6206e-02, -7.6710e-03, -2.4760e-02,\n",
      "        -2.5590e-02,  8.6585e-03,  8.9867e-03, -2.8065e-02, -1.7757e-02,\n",
      "        -2.4765e-02, -2.0969e-02, -2.5449e-02, -2.7733e-02,  1.1785e-02,\n",
      "        -1.5591e-02, -3.4397e-03,  1.9418e-03,  9.4636e-03,  3.6741e-03,\n",
      "         2.3917e-03, -2.7926e-02,  6.1284e-03, -2.1694e-02,  2.9114e-03,\n",
      "        -2.4696e-02,  2.9456e-02, -2.4786e-02, -3.0379e-02, -6.2952e-03,\n",
      "        -1.2788e-02, -2.2418e-03,  1.3629e-02, -2.3181e-02, -9.1432e-03,\n",
      "         2.8277e-02,  2.6845e-02, -1.5391e-02, -1.3792e-02, -2.9430e-02,\n",
      "        -2.2974e-02,  4.3392e-03,  1.2155e-03,  1.8039e-02,  3.3996e-03,\n",
      "        -2.0612e-02, -2.6643e-02,  1.5649e-02,  1.9844e-02,  5.9667e-03,\n",
      "        -1.9730e-02, -2.8681e-02,  3.9156e-04,  1.4429e-02, -1.5545e-02,\n",
      "         2.1326e-02, -1.1481e-02, -1.1530e-02,  1.6752e-02,  1.7801e-02,\n",
      "        -1.8101e-02,  1.5005e-02,  1.9006e-02,  3.7489e-03, -2.1296e-02,\n",
      "        -5.1961e-03, -2.2518e-02, -6.6198e-03, -2.0543e-02, -1.7472e-02,\n",
      "         2.3059e-02,  3.0613e-02, -3.0918e-02, -7.9168e-03, -1.6343e-02,\n",
      "         5.3370e-03, -2.2168e-02,  4.7367e-03,  1.4492e-02, -6.7626e-03,\n",
      "         5.1407e-03, -1.2780e-02, -8.2548e-03, -9.5718e-04, -1.0529e-02,\n",
      "         2.2942e-02, -2.6109e-02,  3.0320e-03, -8.9192e-03, -5.3437e-04,\n",
      "        -2.3237e-02,  1.5099e-02,  1.3194e-02,  1.9086e-02,  2.0903e-02,\n",
      "        -2.6741e-02, -9.0702e-04,  2.2300e-02,  4.7115e-03,  2.9971e-02,\n",
      "         1.2444e-02,  2.6861e-02,  7.5257e-03, -2.2215e-02, -5.8646e-03,\n",
      "         2.8436e-02, -2.5001e-02,  2.7474e-02, -2.3510e-04,  6.4216e-03,\n",
      "        -2.1614e-02, -6.8841e-03, -1.5929e-02, -1.2186e-02, -2.1445e-02,\n",
      "         2.7027e-03,  2.1484e-02, -3.4940e-03,  1.8539e-02,  1.1669e-03,\n",
      "         2.5558e-03,  2.6727e-02,  2.6129e-02,  4.1540e-03,  2.6139e-02,\n",
      "         5.9993e-03,  2.2284e-02,  3.9975e-03,  1.6091e-02,  2.9277e-02,\n",
      "         2.7133e-02,  1.4423e-02, -2.7167e-02, -1.2519e-02, -6.2722e-04,\n",
      "        -2.0576e-02, -2.6223e-02, -1.7404e-02,  1.1580e-02, -2.5730e-02,\n",
      "        -1.1502e-02,  1.4503e-02,  1.7490e-02,  1.5220e-02, -2.3307e-02,\n",
      "        -8.7479e-03, -4.1641e-04,  2.0756e-02,  1.0585e-02, -1.2230e-02,\n",
      "         8.5718e-03, -1.1859e-02,  6.5947e-04,  2.6394e-02,  3.5608e-03,\n",
      "        -1.0575e-02, -2.5629e-02, -3.0654e-02,  1.2876e-02, -1.8839e-02,\n",
      "        -2.8339e-02,  1.1123e-02,  2.8161e-02,  2.6296e-03,  1.0519e-02,\n",
      "        -2.4859e-02, -1.6863e-02, -1.6581e-02,  2.9203e-02, -2.3960e-02,\n",
      "         2.1092e-02, -1.6941e-02, -2.7815e-03,  2.5888e-02,  7.1033e-04,\n",
      "         2.4743e-02,  1.2765e-02, -1.2562e-02, -8.0216e-03, -1.0961e-02,\n",
      "         1.1601e-02,  2.2442e-02,  1.4298e-02, -2.8054e-02, -2.4886e-02,\n",
      "        -7.3228e-03,  1.4492e-02, -2.3127e-02, -1.4636e-02, -1.6460e-02,\n",
      "         1.9712e-02, -1.5368e-02,  2.4063e-02, -1.9276e-02, -1.3746e-02,\n",
      "        -1.7573e-02, -2.5367e-02,  9.2516e-04,  2.9921e-02,  7.1493e-03,\n",
      "        -1.2955e-03,  2.6207e-02,  2.6588e-03, -8.6218e-03,  2.1009e-02,\n",
      "        -6.9353e-03,  3.8500e-03, -2.0720e-02,  5.9159e-03, -2.8556e-02,\n",
      "         1.6128e-02,  1.3929e-02, -1.3139e-02, -1.0159e-02, -2.1713e-02,\n",
      "         1.2142e-02, -1.6101e-03, -5.1106e-03,  9.3683e-03,  2.9469e-02,\n",
      "         2.7702e-02,  3.0357e-02, -2.1266e-02, -2.6153e-02,  2.6395e-02,\n",
      "         1.7982e-02,  1.3956e-02,  3.9453e-03,  8.0619e-04, -8.0724e-03,\n",
      "        -1.9893e-02, -2.3731e-02,  1.9627e-02, -1.6281e-02,  7.3099e-03,\n",
      "         2.2010e-02,  2.2629e-02,  3.0341e-02,  2.9834e-02,  6.9734e-03,\n",
      "         1.0808e-02, -2.2492e-02,  2.6405e-02,  2.1819e-02,  2.4098e-02,\n",
      "        -1.0087e-02,  7.4502e-03, -1.5738e-02, -6.0902e-03, -1.1103e-02,\n",
      "        -1.7706e-02,  6.8886e-03,  6.8665e-03,  1.5181e-02, -2.5152e-02,\n",
      "         4.3497e-03, -5.3752e-03, -1.6802e-02,  3.0677e-02, -5.6754e-03,\n",
      "        -5.4381e-03, -7.0233e-03, -1.7842e-02,  2.3329e-02, -4.9702e-03,\n",
      "         6.0656e-03,  1.2681e-03, -2.0751e-02,  1.2732e-02, -1.3732e-02,\n",
      "         6.6316e-03,  4.3151e-03,  1.9234e-02,  2.4646e-02, -2.3892e-02,\n",
      "        -2.3330e-02, -6.2029e-03, -2.4535e-02, -2.1924e-02,  1.7204e-02,\n",
      "        -1.0138e-02,  3.0113e-02,  2.1027e-02,  1.3657e-02, -1.1591e-02,\n",
      "        -1.7541e-02, -2.7890e-02, -7.8727e-03, -6.8735e-03, -1.4763e-02,\n",
      "         1.4764e-02,  2.0409e-02, -1.9251e-02,  9.2270e-03, -2.4345e-02,\n",
      "         1.7203e-02,  2.2103e-02, -9.0647e-05, -9.1514e-03,  1.0224e-02,\n",
      "         2.2024e-03,  1.1649e-03, -3.0412e-02,  1.5325e-02,  2.0945e-02,\n",
      "         2.0797e-02,  1.6875e-02,  6.7076e-03, -2.6127e-02, -6.3181e-03,\n",
      "         9.9834e-03, -2.1453e-02, -9.4334e-03,  5.4568e-03,  2.3746e-02,\n",
      "        -2.1712e-02,  5.1928e-03,  2.2618e-02,  2.4238e-02, -4.0893e-03,\n",
      "        -1.8495e-02, -2.4876e-02, -1.4636e-02, -3.0387e-02, -2.8332e-02,\n",
      "         1.6695e-02,  1.5122e-02,  2.2740e-02, -1.9996e-02, -2.8414e-03,\n",
      "         2.8249e-03, -2.6033e-02,  2.8827e-03, -2.3092e-03,  1.2985e-02,\n",
      "         9.7618e-04, -1.7998e-02, -1.3259e-04, -1.4937e-02, -5.2776e-03,\n",
      "        -2.8528e-02,  1.7502e-02,  3.0636e-02,  1.5763e-02, -2.5515e-02,\n",
      "         2.2503e-02,  6.0978e-03, -6.7555e-03, -7.2881e-03, -1.3981e-02,\n",
      "        -1.3123e-02, -4.6801e-03, -3.0043e-02, -1.7057e-02,  1.7995e-03,\n",
      "        -3.1246e-02, -3.6895e-03,  2.6178e-02, -4.6427e-03, -1.7720e-02,\n",
      "         2.9519e-02, -1.3568e-02,  2.8900e-02,  2.0925e-02,  1.4282e-02,\n",
      "         1.7458e-02,  6.3068e-03,  5.8996e-03,  7.1976e-03,  9.3825e-03,\n",
      "         1.0144e-02,  1.5275e-02,  1.2197e-03,  7.5236e-04, -2.6365e-02,\n",
      "        -2.5624e-02, -2.9544e-02,  4.4313e-03, -2.1088e-02, -2.3737e-02,\n",
      "         2.8309e-02,  2.5002e-02,  4.2596e-03,  1.1721e-02,  2.5489e-02,\n",
      "        -3.3650e-03, -8.0968e-03, -1.1204e-02, -8.0176e-03,  4.2848e-04,\n",
      "        -1.8121e-02, -2.3825e-02,  1.8893e-02,  2.4559e-02,  5.6836e-03,\n",
      "        -2.0475e-02, -2.6902e-02, -2.2872e-03, -3.1099e-02,  1.9560e-02,\n",
      "        -7.2544e-03, -1.1509e-02, -1.3435e-02,  3.0435e-02,  1.8422e-02,\n",
      "        -2.2480e-02,  1.6401e-03,  1.9078e-02,  7.5140e-03,  2.7929e-02,\n",
      "         7.3083e-04,  1.1484e-02,  1.1550e-02,  9.0039e-03,  9.3700e-03,\n",
      "        -7.7585e-03,  1.7326e-02,  1.7033e-02, -1.4434e-02,  9.2601e-03,\n",
      "         1.2254e-04, -1.2994e-02, -1.2091e-03, -1.5457e-02, -1.8236e-02,\n",
      "        -1.4890e-02, -7.2938e-03, -1.7478e-02, -2.3082e-02,  2.6498e-02,\n",
      "        -1.7627e-02,  4.9318e-03,  6.6946e-03, -1.7491e-02, -2.1050e-02,\n",
      "         1.8891e-03, -1.2004e-02, -7.8621e-03, -1.1815e-02, -2.4867e-02,\n",
      "         1.6488e-02, -2.3630e-02,  1.9364e-02, -1.5118e-02,  2.9851e-02,\n",
      "        -1.8829e-02, -2.9714e-02, -3.0046e-02,  2.6780e-02, -3.4046e-03,\n",
      "        -1.6558e-03,  2.7254e-02, -1.6515e-02, -1.9245e-02,  8.1448e-03,\n",
      "        -1.7368e-02,  3.2717e-03,  7.6770e-03,  1.1980e-02,  8.1090e-03,\n",
      "        -2.8660e-02,  1.8037e-02, -3.0430e-02,  2.9730e-02, -5.4179e-03,\n",
      "         3.9916e-03,  2.3461e-02,  2.4810e-02,  1.8028e-02, -1.9034e-02,\n",
      "        -2.0701e-02, -1.4506e-03,  2.8232e-02,  1.2769e-02, -2.7428e-02,\n",
      "        -2.6404e-02, -1.6116e-02,  2.8904e-02, -2.7418e-02, -2.6249e-02,\n",
      "        -9.3151e-03, -1.1277e-02,  1.0342e-02,  1.2681e-02,  2.7119e-02,\n",
      "        -2.5482e-02, -5.9920e-03,  1.3246e-02, -1.2487e-02, -1.9947e-03,\n",
      "        -1.8549e-02,  1.6871e-02, -1.7158e-02, -2.6319e-02,  2.1850e-03,\n",
      "         1.7284e-02,  1.7963e-02,  9.0632e-03, -1.5246e-02, -1.6576e-02,\n",
      "         8.7847e-03,  1.1848e-02,  1.2494e-02, -8.3208e-03,  2.2435e-03,\n",
      "         2.5367e-02,  2.5338e-02, -3.6602e-03,  1.7855e-02, -2.6123e-02,\n",
      "        -1.7982e-02,  3.5057e-03,  2.5959e-02, -2.4226e-02, -1.9226e-02,\n",
      "        -8.8258e-03,  2.7884e-02, -2.9228e-02,  2.9772e-02, -2.5617e-02,\n",
      "         1.7581e-02,  6.7801e-03, -2.4774e-03,  4.8124e-03,  1.8204e-02,\n",
      "         4.2582e-04,  1.0233e-02, -1.0381e-02,  2.1872e-02,  2.1653e-02,\n",
      "         9.8188e-03,  7.5631e-03,  4.8512e-03,  2.6205e-02, -1.5148e-02,\n",
      "         7.5919e-04, -1.9120e-03,  1.4166e-02, -2.4176e-02,  2.9715e-02,\n",
      "        -1.7280e-02,  2.0748e-02,  1.4273e-02,  1.7508e-02,  1.1320e-02,\n",
      "        -7.6638e-03,  1.5257e-02, -1.4512e-02,  1.6354e-03,  3.0796e-02,\n",
      "         2.2840e-02,  6.0236e-03,  2.6443e-02, -6.9002e-03,  1.7617e-02,\n",
      "         2.3618e-02, -1.3058e-02, -1.4200e-02,  1.4849e-02,  5.8098e-03,\n",
      "        -2.7260e-02,  2.0284e-02,  1.1077e-02, -1.9681e-02,  3.0330e-02,\n",
      "         7.0335e-03, -9.2471e-03], device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.4.theta.weight Parameter containing:\n",
      "tensor([[-0.0077, -0.0201, -0.0196,  ..., -0.0164, -0.0243, -0.0172],\n",
      "        [ 0.0099,  0.0156, -0.0232,  ...,  0.0359,  0.0117, -0.0198],\n",
      "        [-0.0240, -0.0009,  0.0134,  ..., -0.0049,  0.0337, -0.0026],\n",
      "        ...,\n",
      "        [-0.0017, -0.0115,  0.0154,  ..., -0.0229, -0.0288,  0.0155],\n",
      "        [ 0.0074, -0.0038,  0.0223,  ..., -0.0017, -0.0095,  0.0224],\n",
      "        [-0.0175, -0.0134, -0.0052,  ..., -0.0123, -0.0047, -0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.4.theta.bias Parameter containing:\n",
      "tensor([ 4.0251e-03, -2.8524e-02,  1.0948e-02,  3.0590e-02,  3.3078e-03,\n",
      "         3.5524e-02, -3.0574e-02,  6.5646e-03,  4.0370e-03,  2.2070e-02,\n",
      "        -2.3644e-02, -1.5356e-02, -2.0369e-02,  1.8269e-02,  2.3753e-03,\n",
      "         1.1707e-02,  1.4022e-02,  9.6068e-03,  2.8173e-02, -9.1798e-03,\n",
      "         2.3516e-02,  2.5163e-02, -3.0450e-02,  2.2329e-02,  1.5276e-02,\n",
      "        -3.1706e-02,  6.1174e-03,  1.2746e-02, -2.1563e-02,  1.1851e-02,\n",
      "         3.8280e-03,  6.7988e-03, -2.0726e-02,  2.8552e-02, -1.6188e-02,\n",
      "         5.5925e-03,  4.6299e-03, -1.5333e-02, -3.5786e-02,  3.0371e-02,\n",
      "        -1.5925e-02, -2.4006e-02, -2.7361e-02,  1.9134e-02,  1.1253e-02,\n",
      "         2.3110e-02, -3.1609e-02,  3.1442e-02, -5.5251e-03, -2.4073e-02,\n",
      "        -1.3286e-02, -7.2156e-03,  2.2689e-02, -1.9660e-02,  1.9329e-02,\n",
      "         5.0660e-03,  1.5786e-02,  9.5548e-03,  1.7437e-02,  6.3516e-03,\n",
      "         2.4386e-02, -1.2026e-03, -2.4853e-02, -1.3730e-02, -9.3243e-03,\n",
      "        -2.4324e-02,  2.5565e-02, -5.7630e-03, -9.4722e-03,  2.0192e-02,\n",
      "        -2.9496e-02, -1.4244e-02,  2.5472e-02, -2.3685e-02,  3.3461e-02,\n",
      "         2.8937e-02,  1.2669e-02, -2.7747e-02, -3.5503e-02,  3.5368e-02,\n",
      "         2.9552e-02,  2.7644e-02, -2.5775e-02,  7.7292e-03,  1.2860e-02,\n",
      "         2.5191e-03,  9.4939e-04,  3.2351e-03,  4.5774e-03, -6.6733e-03,\n",
      "        -1.9165e-02, -2.8835e-02, -1.4592e-02,  8.3283e-03,  3.1700e-02,\n",
      "        -6.8182e-03,  1.7041e-02, -1.8234e-02,  3.1249e-02, -6.3469e-03,\n",
      "        -4.8688e-03, -3.2873e-02,  2.1855e-02,  2.1550e-02, -3.4501e-02,\n",
      "         1.7361e-02,  3.5315e-02,  1.7782e-02, -1.2180e-02,  2.1920e-02,\n",
      "        -1.6554e-02,  2.1699e-02,  7.9585e-03, -1.1439e-02,  9.1381e-03,\n",
      "        -2.9141e-02,  6.9392e-03,  7.1389e-03, -2.2079e-02, -3.5683e-03,\n",
      "        -3.2903e-02,  2.8040e-02,  1.5196e-02, -2.6213e-02,  6.0787e-03,\n",
      "        -3.2509e-02,  3.1667e-02,  2.1488e-02,  2.0972e-02, -6.3992e-03,\n",
      "         3.8236e-03, -1.7757e-02, -1.3776e-02, -1.0845e-02,  1.7565e-02,\n",
      "         2.1374e-02,  3.2021e-02,  1.6654e-02,  2.0337e-02, -6.9730e-03,\n",
      "        -3.2443e-02, -3.1619e-02, -2.3723e-02,  1.9212e-03, -5.4461e-05,\n",
      "         3.3411e-02, -9.4903e-03, -2.7517e-02,  2.6686e-02,  3.1116e-02,\n",
      "         3.4679e-02,  2.8853e-02, -1.6298e-02, -9.5022e-03,  1.5102e-03,\n",
      "        -1.0036e-02, -3.4035e-02,  3.5037e-02,  9.1731e-03,  1.0082e-02,\n",
      "        -2.3335e-02, -9.9313e-03, -3.4560e-02, -1.9702e-02,  2.5826e-02,\n",
      "        -3.0918e-02,  5.4246e-03, -2.8749e-02, -2.3844e-03, -2.4687e-02,\n",
      "        -3.4572e-02,  2.2615e-02,  2.4361e-02,  3.2321e-02,  2.5048e-02,\n",
      "        -3.5598e-03,  1.3441e-02, -1.0165e-02,  1.1608e-02,  2.6150e-02,\n",
      "        -2.8819e-02,  2.0290e-03,  1.6211e-02, -2.8851e-02, -5.4881e-03,\n",
      "        -1.1000e-02, -2.4027e-02, -3.0800e-02,  1.2315e-02, -3.3727e-02,\n",
      "        -1.0955e-02,  2.9206e-02,  1.4402e-02,  1.7255e-02,  2.9808e-02,\n",
      "        -3.5628e-02, -2.9242e-02,  1.9663e-02,  1.5370e-02, -3.3798e-02,\n",
      "         1.2171e-02,  2.9403e-02, -9.2854e-03,  1.3821e-02,  2.9311e-02,\n",
      "         1.1692e-02, -2.1000e-02, -3.5037e-02,  1.8211e-02,  2.3858e-02,\n",
      "        -3.1721e-02, -4.1519e-03,  1.5250e-02,  2.4272e-02, -1.1977e-03,\n",
      "        -3.1721e-02, -1.1065e-02,  1.3379e-02, -2.2946e-02,  1.4110e-02,\n",
      "        -2.2063e-02,  4.2623e-03,  5.6691e-03, -3.1163e-02, -1.5638e-02,\n",
      "        -8.5008e-03, -2.3329e-02, -3.7072e-03, -9.7899e-03, -2.3814e-02,\n",
      "        -1.7406e-02,  3.3817e-02, -3.5697e-02,  1.5840e-02, -8.8820e-03,\n",
      "        -1.5486e-02, -3.5700e-02, -4.9869e-03, -1.1323e-02, -2.4319e-02,\n",
      "        -1.7714e-02,  1.3984e-03, -9.8398e-03, -1.4150e-02,  2.9774e-02,\n",
      "        -2.8877e-02,  1.7576e-02, -8.3955e-03, -1.9651e-02,  2.9355e-02,\n",
      "         3.3177e-02,  3.5108e-03, -3.2484e-02,  2.1491e-02,  5.2920e-03,\n",
      "         1.3899e-02,  1.8423e-02,  2.7065e-03, -8.9445e-03, -1.7759e-02,\n",
      "         2.4852e-02, -1.7665e-02,  3.0667e-02,  3.1008e-02, -1.2729e-02,\n",
      "        -9.8091e-03,  1.1512e-02, -1.6936e-02,  2.3411e-02,  3.1332e-02,\n",
      "        -1.1534e-02,  1.7057e-02, -3.5286e-02,  1.7548e-02,  3.8374e-03,\n",
      "        -1.5561e-02,  1.8633e-02, -3.3434e-02, -3.1993e-02, -2.8535e-02,\n",
      "        -6.5207e-03,  5.0786e-04, -3.4269e-02,  1.6312e-03,  2.3097e-02,\n",
      "        -3.2595e-02,  1.0113e-02, -2.8002e-02,  9.5591e-03,  1.0879e-02,\n",
      "        -2.2062e-02,  2.8676e-02, -1.0314e-02, -2.5438e-02,  3.3358e-03,\n",
      "         2.2944e-02,  1.5608e-02,  1.7049e-02, -1.9859e-02,  3.0902e-02,\n",
      "         2.0437e-02, -2.2166e-02,  3.3567e-02,  1.5436e-02, -1.0160e-02,\n",
      "         3.3998e-02,  1.2961e-02, -3.4971e-02,  2.2373e-02, -3.1583e-02,\n",
      "        -3.2343e-02,  1.6349e-04, -1.0348e-03,  1.7632e-02,  4.2417e-03,\n",
      "        -1.4439e-02,  8.3497e-03,  2.7898e-02,  2.8008e-02, -9.3300e-03,\n",
      "         1.2788e-02, -2.8412e-02, -3.4441e-02, -3.3748e-03,  2.1541e-02,\n",
      "         3.0949e-02, -6.0948e-03,  1.7558e-02,  2.2180e-02, -1.6963e-02,\n",
      "         2.4776e-02,  1.8339e-02, -7.3112e-03,  3.5292e-02,  2.1284e-03,\n",
      "        -1.3600e-02, -2.4815e-02, -1.0616e-02, -1.7593e-02, -1.8976e-02,\n",
      "        -6.1456e-03,  1.1964e-02,  1.9461e-02,  2.3246e-02, -2.9498e-02,\n",
      "         3.4194e-02,  8.4713e-03,  1.5068e-02, -2.3127e-02,  2.6850e-02,\n",
      "        -2.8312e-02,  2.9991e-02, -1.2826e-02, -3.0493e-02, -1.8602e-02,\n",
      "         2.5736e-02,  2.1503e-02, -2.3566e-03, -1.8521e-02,  2.7415e-02,\n",
      "         3.3059e-02, -2.5277e-02, -9.6691e-03, -2.9380e-02,  1.2672e-02,\n",
      "         8.6254e-03,  6.8657e-04, -8.3652e-03, -3.9305e-03,  3.5096e-02,\n",
      "        -2.9661e-02, -2.5776e-02,  1.9178e-02,  3.4225e-03,  1.1806e-02,\n",
      "        -1.8443e-02, -9.0051e-03, -4.2148e-03,  1.4820e-02, -3.3833e-02,\n",
      "        -9.9035e-03, -1.3222e-02,  2.0058e-02,  1.5523e-02, -9.1131e-03,\n",
      "         2.6604e-02, -1.3590e-02, -2.9573e-02, -7.3538e-03,  2.5427e-02,\n",
      "        -8.0236e-04,  3.7822e-04, -1.4514e-02, -9.6443e-03,  1.2468e-03,\n",
      "         4.4199e-03, -2.0601e-02, -2.8355e-02,  1.5240e-02,  4.4425e-03,\n",
      "         3.6921e-04,  7.7635e-03, -2.3066e-02, -1.7181e-02, -1.5940e-02,\n",
      "         2.7795e-03, -1.3272e-02, -2.6821e-02, -2.5379e-02,  2.7166e-02,\n",
      "         1.1652e-02,  6.2840e-03,  9.1790e-03,  2.0251e-02,  9.5789e-05,\n",
      "         2.5567e-02,  1.9276e-02, -2.3204e-02,  3.5325e-02,  1.6984e-02,\n",
      "         1.1552e-02,  2.5155e-02,  1.1929e-02,  1.4933e-03, -2.4263e-02,\n",
      "        -3.0752e-02,  4.7206e-03,  1.3508e-02, -7.3534e-03, -3.0073e-02,\n",
      "         3.5949e-03,  2.1663e-02,  2.9223e-02,  2.1131e-02, -2.0891e-02,\n",
      "        -2.7174e-02,  1.0401e-02, -1.4428e-02, -2.4129e-02,  2.6556e-02,\n",
      "         3.5778e-02, -3.3105e-03,  1.8465e-02,  3.3997e-04, -6.3927e-03,\n",
      "        -4.7745e-03,  1.1746e-02, -3.0644e-02,  3.0594e-03,  2.8784e-02,\n",
      "         3.3806e-02,  1.8791e-03,  5.0191e-03, -1.6903e-02, -3.4661e-02,\n",
      "         2.1797e-02,  3.3484e-02,  5.5346e-03, -3.1138e-02,  5.1222e-03,\n",
      "        -3.0883e-03, -1.8621e-02, -2.8495e-02,  3.0234e-02,  1.8365e-02,\n",
      "         2.7227e-02, -3.2427e-02, -1.6815e-03,  2.7303e-02, -8.3273e-03,\n",
      "         7.1263e-04,  2.0206e-03, -2.1527e-02,  2.1886e-02, -1.3582e-02,\n",
      "         3.0913e-02, -2.1048e-02, -1.4925e-02, -6.1607e-03, -1.5969e-02,\n",
      "        -3.0437e-02, -9.6044e-04, -3.4637e-02, -4.9506e-03,  3.3944e-02,\n",
      "        -3.3083e-02,  3.3182e-02, -2.5226e-02,  2.2942e-02, -1.2838e-02,\n",
      "         2.7566e-02,  9.6666e-03,  2.1303e-02, -1.4153e-02,  7.6053e-03,\n",
      "         2.0167e-02, -2.5268e-02, -9.2341e-03,  2.8040e-02,  1.6017e-02,\n",
      "         2.5429e-03,  2.3276e-02,  8.9265e-03,  2.7080e-02,  3.4129e-02,\n",
      "         2.0640e-02, -1.5818e-02,  5.8977e-03, -2.1838e-02,  5.8229e-03,\n",
      "         2.2471e-02,  2.3995e-02], device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.5.theta.weight Parameter containing:\n",
      "tensor([[-0.0233, -0.0389, -0.0018,  ..., -0.0381,  0.0301, -0.0149],\n",
      "        [ 0.0044,  0.0171,  0.0147,  ...,  0.0400,  0.0306,  0.0114],\n",
      "        [ 0.0429, -0.0257,  0.0016,  ..., -0.0029, -0.0199, -0.0208],\n",
      "        ...,\n",
      "        [-0.0415,  0.0191, -0.0254,  ...,  0.0095,  0.0110,  0.0048],\n",
      "        [-0.0228,  0.0328, -0.0201,  ...,  0.0355, -0.0004,  0.0169],\n",
      "        [-0.0387, -0.0063, -0.0346,  ...,  0.0431, -0.0389, -0.0300]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.5.theta.bias Parameter containing:\n",
      "tensor([-4.6774e-03,  2.7216e-02,  8.0898e-04,  1.3377e-02,  2.6249e-02,\n",
      "         1.2042e-02,  4.3259e-02,  4.0019e-02,  2.6534e-03, -3.7510e-02,\n",
      "         3.0274e-02,  3.5129e-02,  5.1847e-03, -4.0874e-02, -1.0252e-02,\n",
      "         8.9289e-03,  3.1376e-03, -4.0333e-02,  1.6700e-02,  1.5619e-02,\n",
      "        -2.0262e-02, -1.6341e-02, -1.4693e-02, -3.1526e-02, -9.5803e-03,\n",
      "        -1.0932e-02, -1.8550e-02,  3.5536e-03,  3.3912e-02,  4.0274e-02,\n",
      "         6.4607e-03,  1.6286e-02,  2.4707e-03,  8.9808e-03,  3.9352e-02,\n",
      "        -2.6131e-02,  1.6377e-02, -5.6757e-03,  1.4329e-02,  3.2956e-02,\n",
      "         9.3781e-03, -5.4565e-03, -2.3666e-02, -2.1758e-02,  2.9002e-03,\n",
      "        -3.5730e-02,  8.9441e-03,  2.5505e-02,  9.2377e-03, -3.0371e-02,\n",
      "         1.4262e-02,  6.5236e-04, -1.1220e-02, -2.7809e-02,  3.9267e-02,\n",
      "         2.9437e-02, -3.3876e-02, -4.2847e-02, -1.2128e-02,  3.1921e-02,\n",
      "        -2.7841e-02, -2.8618e-02, -6.8628e-03, -1.8579e-02,  9.6250e-03,\n",
      "         1.3678e-02,  2.2496e-02,  1.4174e-02,  4.3141e-02,  2.9009e-02,\n",
      "         3.7321e-02,  3.3453e-02,  1.2747e-02, -3.3630e-02,  4.0258e-02,\n",
      "         1.2617e-02,  1.5116e-02,  2.7763e-02, -3.0867e-02,  3.8117e-02,\n",
      "         2.4479e-02,  1.6661e-02,  1.2731e-02, -3.3894e-02,  8.1971e-03,\n",
      "         1.1360e-02, -1.5902e-02,  1.4740e-03, -1.4100e-02,  9.5126e-04,\n",
      "         3.5339e-02,  3.1256e-02,  2.9614e-02, -2.0459e-02,  2.6221e-02,\n",
      "        -3.0280e-02, -2.2090e-02,  3.4729e-02, -1.5602e-04,  1.1199e-02,\n",
      "        -2.5678e-02, -3.2850e-02,  3.2626e-02, -2.3503e-02,  1.2352e-02,\n",
      "        -1.8647e-02, -4.2680e-02,  2.4842e-02, -2.6198e-02,  2.0662e-02,\n",
      "        -3.1193e-02, -1.5936e-02,  2.2053e-02,  2.1411e-02, -3.1744e-02,\n",
      "        -3.6007e-02,  3.7234e-02,  2.2787e-02, -2.6479e-02,  3.6299e-02,\n",
      "         1.1003e-02,  4.0514e-02,  5.4044e-03, -1.8862e-02, -6.5218e-04,\n",
      "        -2.8737e-02, -6.2052e-04,  3.5958e-02, -1.3729e-03,  1.9647e-02,\n",
      "        -4.1520e-03, -9.2677e-04,  1.5557e-02,  9.5057e-04,  3.4676e-02,\n",
      "        -4.0994e-02, -4.0846e-02, -3.5761e-02,  1.4959e-02,  4.0077e-02,\n",
      "         3.6453e-02,  4.1634e-02,  3.3283e-02, -2.9267e-02,  3.9707e-02,\n",
      "        -4.1400e-02,  1.2420e-02, -9.7047e-03, -2.9273e-02,  6.8233e-03,\n",
      "        -9.2138e-04, -1.4926e-02, -4.8574e-03, -1.2645e-02,  1.0422e-02,\n",
      "         1.3107e-02, -5.4857e-03,  3.9814e-02, -2.0541e-02, -3.7038e-02,\n",
      "         3.2621e-02, -4.1301e-02, -1.9876e-02,  7.9791e-03, -9.7496e-03,\n",
      "         6.8973e-03, -4.3084e-02, -4.0732e-02,  3.1873e-02, -2.1708e-02,\n",
      "        -3.1629e-02,  2.5497e-02, -3.3453e-02,  3.4152e-02, -3.1899e-02,\n",
      "         7.1978e-03, -2.6041e-02, -3.2605e-02, -2.8688e-02,  1.5743e-02,\n",
      "        -4.3654e-02, -4.3774e-02, -1.1016e-02,  4.1126e-02, -3.0689e-02,\n",
      "        -1.8264e-02,  1.1879e-02,  1.6522e-02, -2.4733e-02, -3.0206e-02,\n",
      "         2.3471e-02,  2.7834e-04,  8.1600e-03, -1.9423e-02, -3.9589e-02,\n",
      "        -6.5848e-03,  2.5822e-02,  1.6247e-02,  1.3043e-02,  6.6482e-03,\n",
      "         3.6967e-02, -2.8783e-02,  3.3857e-02, -3.7059e-02,  2.2950e-02,\n",
      "        -3.4191e-03, -1.1564e-02,  4.7322e-03, -3.4876e-02, -4.2191e-02,\n",
      "        -1.8372e-03,  6.3806e-03,  2.0614e-02, -1.0359e-04,  2.5863e-03,\n",
      "        -1.8234e-02, -4.3258e-03,  7.4237e-03, -3.3920e-02,  2.8198e-02,\n",
      "        -4.3427e-02, -2.2737e-03, -1.1403e-02, -1.4396e-02, -4.1291e-02,\n",
      "         4.0816e-02, -3.4462e-04,  2.5723e-02, -8.5629e-03, -7.7376e-03,\n",
      "        -1.0483e-03, -1.2239e-02, -1.9044e-02,  1.7811e-02, -2.7674e-02,\n",
      "         1.6848e-03,  2.0723e-02, -2.7768e-02, -1.8223e-02, -1.0251e-02,\n",
      "        -1.0226e-02,  4.1317e-02,  1.1925e-02,  2.0097e-02,  5.2221e-04,\n",
      "        -3.5109e-02,  3.4711e-02, -3.1698e-02, -2.9147e-02, -3.6514e-02,\n",
      "         1.7427e-02, -6.3581e-03,  1.0980e-02, -4.2633e-02, -4.1017e-02,\n",
      "        -2.0090e-02,  2.2559e-02,  4.8210e-03,  1.7641e-02, -2.3565e-02,\n",
      "         4.2566e-02,  1.5345e-02, -3.4174e-02, -3.4027e-02,  3.9359e-02,\n",
      "        -3.0116e-02,  2.0214e-02, -3.6343e-02, -4.0083e-03,  4.3672e-02,\n",
      "         5.9860e-03, -3.0337e-02,  1.3855e-02, -2.4951e-03,  3.7084e-02,\n",
      "         1.0649e-02,  2.3338e-02, -4.3939e-02, -2.4707e-02, -3.2408e-02,\n",
      "         4.3385e-02, -9.5994e-03, -4.0404e-03, -4.0163e-03,  2.7497e-02,\n",
      "        -3.5789e-02,  2.5734e-02,  2.9921e-02,  4.9749e-03, -1.6634e-02,\n",
      "         2.5550e-03, -3.4299e-03,  3.8080e-02,  4.3838e-02, -1.8118e-02,\n",
      "        -1.7701e-02,  1.1754e-03, -1.0609e-04,  3.9447e-02,  3.1493e-02,\n",
      "         1.8983e-02,  1.1050e-02, -4.1881e-02,  3.8102e-02,  1.8321e-02,\n",
      "         4.3754e-02, -1.4755e-02,  2.0761e-02,  2.6143e-02, -1.8903e-02,\n",
      "         9.1838e-03,  2.3660e-02, -3.3083e-02, -3.7741e-02, -2.5916e-02,\n",
      "         8.8092e-03, -7.2070e-03, -1.0616e-02, -3.9096e-03,  3.1571e-02,\n",
      "         4.1556e-02, -1.1976e-03, -2.4377e-02, -3.2035e-02, -3.6853e-02,\n",
      "         4.3034e-02, -4.7320e-03, -1.3042e-03,  4.3328e-02,  1.1912e-05,\n",
      "         2.3377e-02,  3.5703e-02,  1.5396e-02, -2.8048e-02,  1.1639e-03,\n",
      "        -6.8449e-03,  3.9610e-02,  2.9094e-02,  2.7018e-03,  1.0154e-02,\n",
      "         3.0583e-02, -5.1634e-03,  3.2770e-02,  9.5003e-04,  1.1467e-02,\n",
      "         1.2220e-02, -2.8262e-02,  1.3829e-02,  7.8897e-03, -1.6532e-02,\n",
      "         2.2029e-02, -2.7580e-02, -1.3384e-02,  1.3471e-02,  4.1622e-02,\n",
      "        -3.8160e-02,  4.0239e-02, -3.3081e-02, -2.0571e-02,  9.4048e-03,\n",
      "        -1.6085e-02,  4.0347e-02,  3.1344e-02,  2.8593e-02, -2.2565e-02,\n",
      "        -1.8831e-02,  7.9788e-03,  3.2427e-03, -1.3713e-02, -1.6888e-02,\n",
      "         4.2722e-02,  9.8744e-04, -2.5742e-02,  1.3191e-03, -2.2784e-02,\n",
      "        -3.1459e-02, -4.0900e-02,  2.6791e-02, -1.5794e-02,  4.2747e-02,\n",
      "         2.2563e-02, -3.1425e-02,  1.5255e-02,  2.2808e-02,  1.0848e-02,\n",
      "        -2.1884e-02, -5.5133e-03, -3.7988e-02,  4.2571e-02, -2.7745e-02,\n",
      "        -9.9305e-03,  3.9687e-02, -3.2099e-02, -2.0655e-03, -1.2160e-02,\n",
      "        -4.8582e-03, -3.6781e-02, -1.0579e-02, -3.9336e-02,  4.0847e-02,\n",
      "        -1.3032e-02,  3.3701e-02,  1.1516e-02,  1.3766e-02,  8.6935e-04,\n",
      "         3.4503e-02, -9.3913e-03,  4.4156e-02,  2.7933e-02, -3.1851e-02,\n",
      "         1.5332e-02,  2.0220e-02, -6.4465e-03,  1.2047e-02,  2.4032e-02,\n",
      "        -1.5365e-02,  4.1824e-02,  6.5567e-03,  3.7411e-02,  1.5503e-03,\n",
      "         1.2488e-02,  2.4513e-02, -2.8292e-04, -3.3460e-02,  3.7918e-02,\n",
      "         2.2647e-03,  1.1439e-02, -2.0356e-02, -4.2012e-04, -1.6382e-02,\n",
      "         2.7561e-02,  2.3084e-02, -2.9988e-02, -1.6661e-04,  7.3525e-03,\n",
      "         2.2461e-02,  7.3662e-03,  4.3800e-02,  1.9209e-02, -2.2804e-02,\n",
      "         1.9453e-02,  2.1040e-02,  3.3202e-02,  1.0773e-02,  1.8118e-02,\n",
      "         3.6019e-02, -2.1762e-02, -1.4097e-02,  2.5668e-02, -2.7892e-02,\n",
      "         9.1341e-03, -1.5079e-02, -3.0955e-02, -2.5848e-02, -9.0602e-03,\n",
      "         3.8249e-02,  3.2492e-02,  9.5064e-04, -4.3956e-03, -3.4991e-02,\n",
      "        -2.5541e-02,  3.2157e-02, -1.6673e-02, -3.2430e-02,  2.2264e-02,\n",
      "         2.8000e-02,  3.7068e-02, -3.6028e-02,  5.6032e-03,  4.8094e-03,\n",
      "         3.5480e-02, -1.7676e-02, -1.0172e-02,  2.9622e-02,  2.3473e-02,\n",
      "        -2.9410e-02, -1.7401e-03,  2.7375e-02, -7.0211e-03,  1.4820e-02,\n",
      "         3.8177e-02,  1.7505e-02,  1.1910e-02,  9.9058e-03,  2.3588e-02,\n",
      "         1.8754e-02, -9.2269e-03, -3.5329e-02, -4.1728e-03, -4.0261e-03,\n",
      "        -3.4460e-02, -3.2980e-02,  2.5175e-02,  4.9110e-03,  3.3314e-02,\n",
      "         1.9389e-02, -1.4495e-02, -2.6299e-02,  3.9369e-02,  2.4072e-02,\n",
      "        -9.5789e-03, -2.8398e-02, -1.8699e-02,  2.6965e-02, -9.4017e-03,\n",
      "         2.7785e-02, -1.9579e-02,  7.8881e-03,  2.6486e-02, -1.5265e-03,\n",
      "         2.0086e-02, -2.1785e-02], device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.6.theta.weight Parameter containing:\n",
      "tensor([[ 0.0148, -0.0349,  0.0133,  ..., -0.0208, -0.0394, -0.0119],\n",
      "        [-0.0288, -0.0316, -0.0176,  ..., -0.0213, -0.0421, -0.0416],\n",
      "        [-0.0186,  0.0242, -0.0322,  ..., -0.0279,  0.0078, -0.0248],\n",
      "        ...,\n",
      "        [ 0.0005, -0.0280, -0.0034,  ..., -0.0078, -0.0440, -0.0121],\n",
      "        [-0.0299, -0.0123, -0.0266,  ...,  0.0379,  0.0237,  0.0235],\n",
      "        [-0.0226,  0.0400, -0.0343,  ..., -0.0122, -0.0377,  0.0332]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.6.theta.bias Parameter containing:\n",
      "tensor([ 0.0026, -0.0132, -0.0264,  0.0428, -0.0337,  0.0334, -0.0058, -0.0298,\n",
      "        -0.0013,  0.0007, -0.0365, -0.0262,  0.0276,  0.0257,  0.0436,  0.0170,\n",
      "        -0.0413, -0.0279, -0.0054,  0.0343,  0.0276, -0.0410,  0.0228, -0.0166,\n",
      "         0.0255, -0.0288, -0.0213,  0.0093,  0.0259, -0.0070,  0.0121, -0.0109,\n",
      "         0.0064, -0.0198, -0.0208, -0.0198,  0.0093,  0.0115, -0.0190,  0.0387,\n",
      "         0.0200, -0.0331, -0.0432,  0.0311, -0.0247,  0.0252,  0.0043,  0.0230,\n",
      "        -0.0193, -0.0277,  0.0200,  0.0175, -0.0428,  0.0429, -0.0266, -0.0260,\n",
      "         0.0439, -0.0248, -0.0368, -0.0436, -0.0008,  0.0051,  0.0066, -0.0067,\n",
      "        -0.0320,  0.0282,  0.0221,  0.0156,  0.0011, -0.0213,  0.0223,  0.0377,\n",
      "        -0.0348,  0.0375,  0.0112, -0.0318,  0.0019, -0.0022,  0.0074,  0.0298,\n",
      "         0.0374, -0.0287, -0.0007,  0.0422,  0.0405,  0.0396,  0.0175, -0.0264,\n",
      "         0.0410, -0.0012,  0.0141,  0.0421, -0.0279,  0.0341,  0.0058,  0.0186,\n",
      "         0.0033, -0.0437, -0.0040, -0.0440,  0.0272,  0.0405,  0.0170,  0.0347,\n",
      "        -0.0032,  0.0407, -0.0171,  0.0214, -0.0210,  0.0395, -0.0213,  0.0024,\n",
      "         0.0046,  0.0173, -0.0216, -0.0407, -0.0027,  0.0044,  0.0225,  0.0288,\n",
      "        -0.0199, -0.0112,  0.0297, -0.0229,  0.0412, -0.0176, -0.0035, -0.0343,\n",
      "         0.0248,  0.0084,  0.0207,  0.0158,  0.0291,  0.0087,  0.0244,  0.0292,\n",
      "        -0.0037,  0.0137,  0.0072, -0.0306,  0.0010,  0.0212, -0.0437,  0.0313,\n",
      "         0.0134,  0.0252, -0.0054,  0.0309, -0.0113, -0.0373, -0.0191,  0.0005,\n",
      "         0.0423,  0.0099, -0.0226, -0.0384,  0.0063, -0.0122,  0.0415,  0.0183,\n",
      "        -0.0226,  0.0216,  0.0270,  0.0356, -0.0104, -0.0376, -0.0182, -0.0215,\n",
      "         0.0423,  0.0264, -0.0169,  0.0122, -0.0290, -0.0250, -0.0283, -0.0056,\n",
      "        -0.0149,  0.0133, -0.0231, -0.0006, -0.0434,  0.0241, -0.0189, -0.0148,\n",
      "         0.0186,  0.0117, -0.0151,  0.0239,  0.0418,  0.0082, -0.0081,  0.0075,\n",
      "         0.0423, -0.0141, -0.0030,  0.0029, -0.0215, -0.0051,  0.0104, -0.0047,\n",
      "        -0.0002, -0.0336,  0.0031,  0.0045, -0.0023, -0.0420,  0.0315,  0.0383,\n",
      "         0.0080, -0.0203,  0.0379, -0.0423, -0.0187,  0.0162, -0.0260,  0.0273,\n",
      "         0.0252,  0.0204, -0.0076, -0.0353, -0.0212, -0.0143,  0.0336,  0.0316,\n",
      "         0.0397,  0.0295,  0.0123,  0.0184,  0.0368,  0.0087,  0.0017,  0.0224,\n",
      "         0.0412, -0.0111, -0.0203, -0.0007,  0.0069, -0.0353,  0.0322,  0.0284,\n",
      "        -0.0317, -0.0372, -0.0104, -0.0397, -0.0111, -0.0225,  0.0328, -0.0430,\n",
      "         0.0352,  0.0196,  0.0106,  0.0387,  0.0074,  0.0251,  0.0135,  0.0183,\n",
      "         0.0151,  0.0395, -0.0386,  0.0382, -0.0360, -0.0179,  0.0053, -0.0299,\n",
      "         0.0238, -0.0060, -0.0343,  0.0368,  0.0159,  0.0396,  0.0027, -0.0229,\n",
      "        -0.0027, -0.0208,  0.0153, -0.0393, -0.0334, -0.0001,  0.0250,  0.0374,\n",
      "        -0.0324,  0.0156, -0.0039,  0.0136, -0.0406, -0.0390, -0.0043,  0.0128,\n",
      "         0.0309,  0.0307,  0.0408,  0.0003, -0.0265, -0.0372,  0.0237, -0.0388,\n",
      "         0.0050, -0.0334, -0.0346, -0.0249,  0.0345, -0.0215,  0.0410,  0.0033,\n",
      "        -0.0016, -0.0089, -0.0367,  0.0304, -0.0127,  0.0238, -0.0056,  0.0044,\n",
      "         0.0330,  0.0217,  0.0077, -0.0072,  0.0336, -0.0172,  0.0212, -0.0190,\n",
      "         0.0283, -0.0084, -0.0089, -0.0331,  0.0006, -0.0389, -0.0160, -0.0149,\n",
      "        -0.0373,  0.0064,  0.0041,  0.0053,  0.0323,  0.0419,  0.0324,  0.0430,\n",
      "        -0.0297,  0.0281,  0.0430,  0.0354, -0.0397,  0.0075, -0.0310, -0.0050,\n",
      "        -0.0201,  0.0433,  0.0288, -0.0312,  0.0015, -0.0190,  0.0321,  0.0097,\n",
      "        -0.0233, -0.0241, -0.0343, -0.0390, -0.0313, -0.0206, -0.0044,  0.0182,\n",
      "        -0.0009, -0.0361,  0.0089, -0.0210, -0.0366,  0.0128,  0.0196, -0.0044,\n",
      "         0.0014,  0.0207, -0.0293, -0.0099,  0.0396,  0.0078,  0.0136, -0.0091,\n",
      "         0.0170,  0.0362, -0.0262, -0.0132,  0.0244,  0.0115,  0.0433, -0.0057],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.7.theta.weight Parameter containing:\n",
      "tensor([[-0.0149, -0.0458,  0.0154,  ..., -0.0470,  0.0062,  0.0171],\n",
      "        [-0.0461,  0.0053,  0.0500,  ...,  0.0432,  0.0205, -0.0466],\n",
      "        [ 0.0211,  0.0219, -0.0391,  ...,  0.0211, -0.0359, -0.0142],\n",
      "        ...,\n",
      "        [ 0.0454,  0.0256,  0.0279,  ..., -0.0246,  0.0127,  0.0239],\n",
      "        [-0.0102,  0.0503,  0.0025,  ..., -0.0372, -0.0485,  0.0449],\n",
      "        [-0.0461, -0.0159,  0.0323,  ..., -0.0071, -0.0073,  0.0509]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.7.theta.bias Parameter containing:\n",
      "tensor([ 9.9108e-03,  3.2096e-02,  3.5700e-03, -3.6816e-03,  1.4353e-02,\n",
      "         5.0546e-03,  4.7818e-02,  2.0848e-03, -4.8355e-02,  3.0059e-02,\n",
      "        -4.8510e-02,  3.2507e-02, -4.2361e-02,  2.2021e-02, -3.2666e-02,\n",
      "        -3.5628e-02, -1.4385e-02,  3.6904e-02,  2.0670e-02, -1.1711e-02,\n",
      "         2.6434e-02, -2.7745e-02,  3.0347e-02, -1.8046e-02,  2.2389e-02,\n",
      "        -7.7644e-03,  1.4456e-02, -4.0655e-02, -3.4022e-02, -1.3514e-02,\n",
      "        -4.6252e-03,  3.2772e-02,  1.9284e-02, -3.1470e-02, -9.3188e-03,\n",
      "         1.7158e-03,  1.6336e-02,  4.2300e-02,  3.9373e-02, -2.1302e-02,\n",
      "         1.3791e-03,  2.1748e-02, -2.2438e-02, -2.3931e-02,  1.1478e-02,\n",
      "         7.8427e-03,  3.6504e-02, -2.8650e-02, -3.6501e-02,  4.7360e-03,\n",
      "         5.1790e-03, -6.9328e-03,  1.5603e-03,  4.5714e-02, -3.6543e-02,\n",
      "        -3.9820e-02, -4.3199e-02, -9.8873e-03,  4.3393e-02,  1.7309e-02,\n",
      "         7.3627e-03, -7.0737e-03, -1.1377e-02, -7.0625e-03,  4.1585e-02,\n",
      "        -6.1596e-03, -1.8694e-02,  8.0992e-03,  3.4093e-02, -2.2207e-02,\n",
      "         3.7334e-02, -3.5308e-02,  6.2536e-03,  2.5581e-02, -3.5157e-02,\n",
      "         4.4556e-02, -4.3342e-02, -1.6927e-02, -3.6390e-02,  1.3728e-02,\n",
      "        -3.6743e-02, -2.3523e-02,  4.8039e-02,  3.3384e-02,  4.3494e-02,\n",
      "         1.1156e-03,  1.1852e-02, -1.5290e-02, -1.2951e-02,  4.9251e-02,\n",
      "         2.5978e-02,  3.8056e-02,  4.4319e-03,  9.6263e-03, -1.4225e-02,\n",
      "         4.7489e-02, -2.3144e-02, -2.6351e-03,  1.2674e-02, -2.9730e-02,\n",
      "         3.7599e-02,  4.8455e-03,  4.9798e-03,  4.7587e-02, -8.1535e-04,\n",
      "         5.8455e-03,  4.1628e-02,  1.1388e-02,  2.3003e-02,  2.6400e-02,\n",
      "        -2.4744e-02, -1.7613e-04, -2.3935e-02,  5.0701e-02,  3.5210e-02,\n",
      "         3.6720e-02,  8.7180e-03,  2.7898e-02,  4.0389e-03, -1.6919e-02,\n",
      "         2.6659e-02,  3.7374e-02, -1.9582e-04,  3.5212e-03, -1.2521e-02,\n",
      "         4.3372e-02, -3.4988e-02,  1.1682e-02, -1.8530e-02,  4.7846e-02,\n",
      "         1.2287e-02, -4.5852e-02,  4.3387e-02, -1.6848e-02,  4.2187e-02,\n",
      "         2.1531e-02,  2.7971e-02, -5.8551e-03, -4.7333e-02, -1.5193e-02,\n",
      "         4.3246e-02, -4.2883e-02,  4.2355e-02,  3.5021e-02,  4.9244e-02,\n",
      "        -1.1400e-02, -4.5551e-02,  2.1890e-03,  4.9771e-02, -4.8820e-02,\n",
      "         1.3204e-02,  3.4796e-02, -2.0470e-02,  4.7166e-02,  2.0800e-02,\n",
      "         2.3987e-02,  4.2910e-02, -4.5305e-02,  4.5388e-03,  2.4599e-02,\n",
      "        -7.7681e-03, -2.4757e-02,  2.7841e-02,  1.8376e-02, -2.6770e-02,\n",
      "        -4.0497e-02, -2.8262e-02, -1.6320e-02,  3.7089e-02,  3.6640e-02,\n",
      "        -3.7407e-02, -7.0857e-03,  7.5768e-03,  1.4855e-02,  1.7068e-03,\n",
      "        -1.9055e-02,  3.3977e-02,  3.7118e-02,  1.3726e-02, -3.3346e-02,\n",
      "         4.4838e-02,  1.1541e-02,  3.9591e-02, -4.0072e-02,  3.4044e-02,\n",
      "         2.8120e-02,  5.7067e-03, -8.2154e-03,  2.9786e-02, -5.0270e-02,\n",
      "         1.1125e-02, -2.6792e-02, -4.2511e-05,  2.8557e-02,  1.2932e-03,\n",
      "        -3.4477e-02,  2.0329e-02,  8.9264e-03, -8.8056e-03,  4.8628e-02,\n",
      "        -1.0631e-02, -2.3393e-02,  4.8344e-02,  4.1308e-02, -4.7334e-03,\n",
      "        -4.0123e-02,  4.6280e-02,  1.5736e-02,  2.0126e-02, -9.6053e-03,\n",
      "        -3.9506e-02,  2.9205e-02, -4.4503e-02, -3.4079e-02, -2.0234e-02,\n",
      "         3.0307e-02,  1.3754e-02, -1.7835e-02, -8.3964e-03,  2.2586e-02,\n",
      "        -7.5642e-03,  2.2856e-02,  1.1688e-02,  2.6132e-02,  3.0608e-03,\n",
      "        -9.5069e-03, -3.3003e-02,  3.2412e-02, -2.9377e-02, -5.0407e-02,\n",
      "         6.3016e-03,  3.5436e-02,  2.4945e-02, -5.9248e-03, -2.8355e-02,\n",
      "         2.4182e-02, -4.8968e-02,  7.5024e-04, -1.5080e-02,  2.1193e-03,\n",
      "        -2.5997e-02, -4.3587e-02,  4.1013e-02, -1.0414e-02,  4.9736e-02,\n",
      "        -4.4186e-02, -3.9236e-02, -3.3915e-02,  2.8961e-03, -4.6542e-02,\n",
      "        -4.9763e-02,  4.8537e-02, -3.9947e-02, -4.4549e-02, -1.2030e-02,\n",
      "         1.2755e-02], device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.8.theta.weight Parameter containing:\n",
      "tensor([[ 0.0558,  0.0195,  0.0142,  ...,  0.0117, -0.0546, -0.0288],\n",
      "        [ 0.0363,  0.0174, -0.0479,  ..., -0.0355, -0.0231, -0.0009],\n",
      "        [ 0.0266,  0.0258,  0.0365,  ..., -0.0466, -0.0349,  0.0016],\n",
      "        ...,\n",
      "        [ 0.0464, -0.0199,  0.0383,  ...,  0.0039, -0.0240, -0.0441],\n",
      "        [ 0.0136,  0.0366, -0.0124,  ..., -0.0473,  0.0207, -0.0037],\n",
      "        [-0.0149,  0.0242, -0.0575,  ..., -0.0317,  0.0391,  0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.8.theta.bias Parameter containing:\n",
      "tensor([-0.0237,  0.0488,  0.0420,  0.0049,  0.0086,  0.0115, -0.0180,  0.0187,\n",
      "        -0.0403,  0.0256,  0.0193, -0.0207, -0.0242, -0.0170, -0.0479, -0.0037,\n",
      "         0.0446,  0.0386, -0.0251,  0.0452,  0.0219, -0.0414, -0.0514,  0.0487,\n",
      "        -0.0296, -0.0207, -0.0489, -0.0618, -0.0586,  0.0041,  0.0279, -0.0397,\n",
      "         0.0282, -0.0040, -0.0236,  0.0099, -0.0203, -0.0443,  0.0402, -0.0343,\n",
      "         0.0080,  0.0320,  0.0495,  0.0427, -0.0576,  0.0192, -0.0156,  0.0468,\n",
      "        -0.0368, -0.0430,  0.0420,  0.0109,  0.0569, -0.0349,  0.0094,  0.0602,\n",
      "        -0.0184, -0.0227,  0.0246, -0.0257, -0.0329,  0.0137, -0.0043,  0.0416,\n",
      "        -0.0268, -0.0201, -0.0264,  0.0206, -0.0449,  0.0353,  0.0063, -0.0156,\n",
      "         0.0286,  0.0240, -0.0597,  0.0252, -0.0541, -0.0393, -0.0158, -0.0353,\n",
      "        -0.0397, -0.0057, -0.0338,  0.0538,  0.0518, -0.0375, -0.0493,  0.0576,\n",
      "         0.0353, -0.0098,  0.0440,  0.0373, -0.0127,  0.0472,  0.0532,  0.0541,\n",
      "         0.0228,  0.0042,  0.0257,  0.0047,  0.0525,  0.0318, -0.0099, -0.0023,\n",
      "         0.0233,  0.0052, -0.0471, -0.0510, -0.0232,  0.0490,  0.0353, -0.0269,\n",
      "        -0.0377, -0.0376, -0.0437, -0.0259,  0.0495,  0.0106,  0.0543, -0.0061,\n",
      "        -0.0293, -0.0159, -0.0550,  0.0516,  0.0541, -0.0481,  0.0150, -0.0404],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.weight Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', requires_grad=True)\n",
      "layers.1.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n",
      "layers.4.weight Parameter containing:\n",
      "tensor([[-0.0168, -0.0469, -0.0281,  ..., -0.0238, -0.0329,  0.0345],\n",
      "        [-0.0428, -0.0008, -0.0511,  ..., -0.0266, -0.0633, -0.0063],\n",
      "        [ 0.0084, -0.0530, -0.0418,  ...,  0.0152,  0.0374, -0.0343],\n",
      "        ...,\n",
      "        [ 0.0844,  0.0368,  0.0313,  ...,  0.0086, -0.0568,  0.0598],\n",
      "        [-0.0032, -0.0174,  0.0251,  ..., -0.0193, -0.0085, -0.0862],\n",
      "        [ 0.0853,  0.0699,  0.0055,  ..., -0.0582,  0.0237,  0.0836]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.4.bias Parameter containing:\n",
      "tensor([ 0.0188, -0.0267,  0.0693,  ..., -0.0679, -0.0135, -0.0826],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.7.weight Parameter containing:\n",
      "tensor([[ 0.0148, -0.0055,  0.0080,  ..., -0.0031,  0.0009,  0.0071],\n",
      "        [-0.0033,  0.0088,  0.0013,  ...,  0.0117, -0.0003,  0.0024],\n",
      "        [-0.0030, -0.0148, -0.0040,  ..., -0.0028, -0.0024,  0.0094],\n",
      "        [ 0.0030,  0.0084,  0.0028,  ..., -0.0063,  0.0088,  0.0097]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.7.bias Parameter containing:\n",
      "tensor([-0.0039, -0.0136,  0.0151, -0.0122], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hgnn_trainer.layers\n",
    "for n,p in hgnn_trainer.named_parameters():\n",
    "    print(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnn_trainer.weight = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -6309.91796875\n",
      "                , loss1: -624.5353515625\n",
      "                , loss2: 4.1342529296875\n",
      "=================================\n",
      "in 10 epoch, average loss: -62766.75625\n",
      "                , loss1: -6247.46640625\n",
      "                , loss2: 51.506964111328124\n",
      "=================================\n",
      "in 20 epoch, average loss: -62144.9125\n",
      "                , loss1: -6246.6828125\n",
      "                , loss2: 40.812896728515625\n",
      "=================================\n",
      "in 30 epoch, average loss: -61521.8125\n",
      "                , loss1: -6246.53984375\n",
      "                , loss2: 37.84578857421875\n",
      "=================================\n",
      "in 40 epoch, average loss: -60901.6875\n",
      "                , loss1: -6246.480859375\n",
      "                , loss2: 32.72926635742188\n",
      "=================================\n",
      "in 50 epoch, average loss: -60282.51875\n",
      "                , loss1: -6246.200390625\n",
      "                , loss2: 24.53887176513672\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2800\u001b[39m):\n\u001b[1;32m      3\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode | 设置为训练模式\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 23\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     25\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/Hyper-Graph-Partition-bs/examples/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1657\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[1;32m   1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate\u001b[38;5;241m=\u001b[39mv2e_drop_rate)\n\u001b[0;32m-> 1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me2v_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1595\u001b[0m, in \u001b[0;36mHypergraph.e2v\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21me2v\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, aggr: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, e2v_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1586\u001b[0m ):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v_update(X)\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1466\u001b[0m, in \u001b[0;36mHypergraph.e2v_aggregation\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1466\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_v_neg_1, X)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(2800):\n",
    "    hgnn_trainer.weight = hgnn_trainer.weight - 0.01\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    # validation\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([959., 954., 959., 952.], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = torch.sum(outs_straight, dim = 0)\n",
    "#bs = torch.sum(outs, dim = 0)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2: tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.0,1.,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0]])\n",
    "H = torch.tensor([[1.,1],[1,1],[0,1],[1,1],[0,1],[1,1]])\n",
    "#H = torch.tensor([[0,1.,0,1,0,1],[1,0,1,0,1,1]])\n",
    "nn = torch.matmul(a, (1 - torch.transpose(a, 0, 1)))\n",
    "#ne_k = torch.matmul(nn, H)\n",
    "ne_k = torch.matmul(nn, H)\n",
    "ne_k = ne_k.mul(H)\n",
    "H_degree = torch.sum(H, dim=0)\n",
    "H_degree = H_degree\n",
    "H_1 = ne_k / H_degree\n",
    "    #bs = torch.where(H_1>=1)\n",
    "    #print(bs)\n",
    "a2 = 1 - H_1\n",
    "a2 = a2.sqrt()\n",
    "print('a2:',a2)\n",
    "a3 = torch.prod(a2, dim=0)\n",
    "print(a3)\n",
    "a3 = a3.sum()\n",
    "loss_1 = -1 * a3\n",
    "\n",
    "a3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
